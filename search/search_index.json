{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"NOMAD Documentation","text":"<p>NOMAD is a free, and open-source data management platform for materials science, whose goal is to make scientific research data FAIR (findable, accessible, interoperable and reusable).</p> <p>NOMAD provides tools for data management, sharing, and publishing. The platform lets you structure, explore, and analyze your data and the data of others.</p> <p>NOMAD solves the challenge of using heterogeneous and unfindable data.</p> <p>NOMAD is useful for scientists that work with data, for research groups that need to collaborate on data, and for communities that need to build an archive of FAIR research data.</p> Project and community <p>NOMAD is an open source project that warmly welcomes community projects, contributions, suggestions, fixes and constructive feedback. NOMAD is developed by FAIRmat, an open NFDI consortium of over 30 partners building a shared data structure of for materials science together.</p> <ul> <li>Get support</li> <li>Join our online forum</li> <li>Contribute</li> <li>View our roadmap</li> <li>Code guidelines</li> </ul> <p>Thinking about using NOMAD for your next project? Get in touch!</p>"},{"location":"index.html#tutorial","title":"Tutorial","text":"<p>A series of tutorials will guide you through the main functionality of NOMAD.</p> <ul> <li>Upload and publish your own data</li> <li>Use the search interface to identify interesting data</li> <li>Use the API to search and access processed data for analysis</li> <li>Create and use custom schemas in NOMAD</li> <li> <p>Developing a NOMAD plugin</p> </li> <li> <p>Example data and exercises</p> </li> <li>More videos and tutorials on YouTube</li> </ul>"},{"location":"index.html#how-to-guides","title":"How-to guides","text":"<p>How-to guides provide step-by-step instructions for a wide range of tasks, with the overarching topics:</p> <ul> <li>Manage and find data</li> <li>Programmatic data access</li> <li>NOMAD Oasis \u2014 self-hosting</li> <li>Plugins</li> <li>Customization</li> <li>Development</li> </ul> <p>Open the how-to guides</p>"},{"location":"index.html#explanation","title":"Explanation","text":"<p>The explanation section provides background knowledge on what are schemas and structured data, how does processing work, the NOMAD architecture, and more.</p>"},{"location":"index.html#reference","title":"Reference","text":"<p>The reference includes all CLI commands and arguments, all configuration options, the possible schema annotations and their arguments, and a glossary of used terms.</p>"},{"location":"aitoolkit.html","title":"Aitoolkit","text":"<p>go here ..</p>"},{"location":"todo.html","title":"Coming soon ...","text":"<p>We still have to write this.</p>"},{"location":"writing_guide.html","title":"Documentation Development and Writing Guide","text":"<p>This is a guide for best practices when contributing to the NOMAD documentation.</p>"},{"location":"writing_guide.html#no-broken-links","title":"No broken links!","text":"<p>Before merging make sure that the mkdocs logs do not report any broken links. This applies even if these links are not relevant to your changes. If you do not know how to address the broken links, create an issue in the Github repo and tag someone who you think could be of assistance.</p>"},{"location":"writing_guide.html#images-and-data","title":"Images and Data","text":"<p>All assets specific to an individual markdown file should be stored within an immediate sub-directory of the file, labeled accordingly. Please use <code>images/</code> and <code>data/</code> for the image and data files, respectively.</p>"},{"location":"writing_guide.html#sections-hierarchy","title":"Sections Hierarchy","text":"<p>single \"#\" sections should only be used at the beginning of the md file</p>"},{"location":"writing_guide.html#standardized-internal-and-external-link-naming","title":"Standardized Internal and External Link Naming","text":"<p>Do not use <code>HERE</code> as a name for links. For internal links use the path hierarchy to the referenced page or section, separated by &gt;'s. For example: <code>[Tutorial &gt; Exploring Data &gt; Search Interface &amp; Filters](&lt;path-to-referenced-section&gt;)</code>. Long paths can be abbreviated to the first and last parts: by using <code>[Tutorial &gt; ... &gt; Search Interface &amp; Filters](&lt;path-to-referenced-section&gt;)</code> If the referenced section belongs to the current page, drop the global path, i.e., <code>[Search Interface &amp; Filters](&lt;path-to-referenced-section&gt;)</code>. External links to NOMAD plugins or other NOMAD-related documentation should follow the same syntax, with the name of the plugin as the root. For other external links provide some sort of descriptive name and use your discretion.</p>"},{"location":"writing_guide.html#external-links","title":"External Links","text":"<p>Use  for external links to open a new browser window.</p>"},{"location":"writing_guide.html#admonitions","title":"Admonitions","text":"<p>Here is a list of currently used admonitions within the docs:</p> <ul> <li> <p>Attention</p> </li> <li> <p>Note</p> </li> <li> <p>Tip</p> </li> <li> <p>Important</p> </li> </ul> <ul> <li> <p>Info</p> </li> <li> <p>Task</p> </li> <li> <p>Example</p> </li> </ul>"},{"location":"writing_guide.html#adding-image-sliders","title":"Adding image sliders","text":"<p>Image sliders can be added using the following syntax:</p> <p><pre><code>&lt;div class=\"image-slider\" id=\"slider#*\"&gt;\n    &lt;div class=\"nav-arrow left\" id=\"prev#\"&gt;\u2190&lt;/div&gt;\n    &lt;img src=\"\" alt=\"\" class=\"active\"&gt;\n    &lt;img src=\"\" alt=\"\"&gt;\n    &lt;img src=\"\" alt=\"\"&gt;\n    &lt;div class=\"nav-arrow right\" id=\"next#\"&gt;\u2192&lt;/div&gt;\n&lt;/div&gt;\n</code></pre> To minimize flickering effect during transitions, make all the sliding images of the same size. </p> <p>If you use more than one slider on the same page, make sure to give them different id. The same applies for the navigation arrows.</p>"},{"location":"examples/overview.html","title":"NOMAD Domain-specific Examples","text":"<p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> One last thing <p>If you can't find what you're looking for in our guides, contact our friendly team for personalized help and assistance. Don't worry, we're here to help and learn what we're doing wrong!</p>"},{"location":"examples/overview.html#computational-data","title":"Computational data","text":"<p>Historically a repository for Density Functional Theory calculations, NOMAD now supports a wide range of computational methodologies including advanced many-body calculations and classical molecular dynamics simulations, as well as complex simulation workflows.</p> <ul> <li>Quick Start: Uploading computational data</li> <li>Standard and custom computational workflows</li> <li>Guide to computational MetaInfo</li> <li>Guide to computational schema plugins</li> <li>Guide to computational parser plugins</li> <li>H5MD: Uploading custom molecular dynamics data<ul> <li>How-to</li> <li>Explanation</li> <li>Reference</li> </ul> </li> </ul>"},{"location":"examples/overview.html#experimental-data","title":"Experimental data","text":"<p>Thanks to key activities of the FAIRmat project, NOMAD also supports a set of parsing capabilities for standardizing data from materials characterization experiments.</p> <ul> <li>Electron microscopy </li> <li>Photoemission spectroscopy </li> <li>X-ray photoemission spectroscopy </li> <li>Optical spectroscopy </li> <li>Atom probe tomography </li> <li>Scanning tunneling spectroscopy </li> <li>pynxtools </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html","title":"H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD","text":""},{"location":"examples/computational_data/h5md_expl.html#overview","title":"Overview","text":"<p>Most computational data in NOMAD is harvested with code-specific parsers that recognize the output files from a particular software and retrieve the appropriate (meta)data accordingly. However, this approach is not possible for many modern molecular simulation engines that use fully-flexible scriptable input and non-fixed output files. \"HDF5 for molecular data\" (H5MD) is a data schema for storage of molecular simulation data, based on the HDF5 file format. This page describes an extension of the H5MD schema, denoted H5MD-NOMAD, which adds specificity to several of the H5MD guidelines while also retaining reasonable flexibility. This enables simulation data stored according to the H5MD-NOMAD schema to be stored in the NOMAD.</p> <p>Due to the nature of extending upon the original H5MD schema, portions of this doc page was duplicated, extended, or summarized from the H5MD webpage.</p>"},{"location":"examples/computational_data/h5md_expl.html#introduction-to-the-h5md-storage-format","title":"Introduction to the H5MD storage format","text":"<p>H5MD was originally proposed by P. de Buyl, P. H. Colberg and F. H\u00f6fling in H5MD: A structured, efficient, and portable file format for molecular data, Comp. Phys. Comm. 185, 1546\u20131553 (2014) [arXiv:1308.6382]. The schema is maintained, along with associated tools, in a GitHub repository: H5MD GitHub.</p> <p>This section provides the basic nomenclature of the H5MD schema relevant for understanding H5MD-NOMAD, and was duplicated or summarized from the H5MD webpage.</p>"},{"location":"examples/computational_data/h5md_expl.html#file-format","title":"File format","text":"<p>H5MD structures are stored in the HDF5 file format version 0 or later. It is recommended to use the HDF5 file format version 2, which includes the implicit tracking of the creation and modification times of the file and of each of its objects.</p>"},{"location":"examples/computational_data/h5md_expl.html#notation-and-naming","title":"Notation and naming","text":"<p>HDF5 files are organized into groups and datasets, summarized as objects, which form a tree structure with the datasets as leaves. Attributes can be attached to each object. The H5MD specification adopts this naming and uses the following notation to depict the tree or its subtrees:</p> <code>\\-- item</code> An object within a group, that is either a dataset or a group. If it is a group itself, the objects within the group are indented by five spaces with respect to the group name. <code>+-- attribute</code> An attribute, that relates either to a group or a dataset. <code>\\-- data: &lt;type&gt;[dim1][dim2]</code> A dataset with array dimensions <code>dim1</code> by <code>dim2</code> and of type <code>&lt;type&gt;</code>. The type is taken from <code>Enumeration</code>, <code>Integer</code>, <code>Float</code> or <code>String</code> and follows the HDF5 Datatype classes. If the type is not mandated by H5MD, <code>&lt;type&gt;</code> is indicated. A scalar dataspace is indicated by <code>[]</code>. <code>(identifier)</code> An optional item. <code>&lt;identifier&gt;</code> An optional item with unspecified name. <p>H5MD defines a structure called H5MD element (or element whenever there is no confusion). An element is either a time-dependent group or a single dataset (see time-dependent data below), depending on the situation.</p>"},{"location":"examples/computational_data/h5md_expl.html#time-dependent-data","title":"Time-dependent data","text":"<p>Time-dependent data consists of a series of samples (or frames) referring to multiple time steps. Such data are found inside a single dataset and are accessed via dataset slicing. In order to link the samples to the time axis of the simulation, H5MD defines a time-dependent H5MD element as a group that contains, in addition to the actual data, information on the corresponding integer time step and on the physical time. The structure of such a group is:</p> <pre><code>&lt;element&gt;\n \\-- step\n \\-- (time)\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <code>value</code> A dataset that holds the data of the time series. It uses a simple dataspace whose rank is given by 1 plus the tensor rank of the data stored. Its shape is the shape of a single data item prepended by a <code>[variable]</code> dimension that allows the accumulation of samples during the course of time. For instance, the data shape of scalars has the form <code>[variable]</code>, <code>D</code>-dimensional vectors use <code>[variable][D]</code>, etc. The first dimension of <code>value</code> must match the unique dimension of <code>step</code> and <code>time</code>. <p>If several H5MD elements are sampled at equal times, <code>step</code> and <code>time</code> of one element may be hard links to the <code>step</code> and <code>time</code> datasets of a different element. If two elements are sampled at different times (for instance, if one needs the positions more frequently than the velocities), <code>step</code> and <code>time</code> are unique to each of them.</p> <p>The storage of step and time information follows one of the two modes below, depending on the dataset layout of <code>step</code>.</p>"},{"location":"examples/computational_data/h5md_expl.html#explicit-step-and-time-storage","title":"Explicit step and time storage","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[variable]\n \\-- (time: type[variable])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <code>step</code> A dataset with dimensions <code>[variable]</code> that contains the time steps at which the corresponding data were sampled. It is of <code>Integer</code> type to allow exact temporal matching of data from one H5MD element to another. The values of the dataset are in monotonically increasing order. <code>time</code> An optional dataset that is the same as the <code>step</code> dataset, except it is <code>Float</code> or <code>Integer</code>-valued and contains the simulation time in physical units. The values of the dataset are in monotonically increasing order."},{"location":"examples/computational_data/h5md_expl.html#fixed-step-and-time-storage-currently-not-supported-in-h5md-nomad","title":"Fixed step and time storage (currently not supported in H5MD-NOMAD)","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[]\n     +-- (offset: type[])\n \\-- (time: type[])\n     +-- (offset: type[])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <code>step</code> <p>A scalar dataset of <code>Integer</code> type that contains the increment of the time step between two successive rows of data in <code>value</code>.</p> <code>offset</code> A scalar attribute of type <code>Integer</code> corresponding to the first sampled value of <code>step</code>. <code>time</code> An optional scalar dataset that is the same as the <code>step</code> dataset, except that it is <code>Float</code> or <code>Integer</code>-valued and contains the increment in simulation time, in physical units. <p><code>offset</code>     : A scalar attribute of the same type as <code>time</code> corresponding to the first     sampled value of <code>time</code>.</p> <p>For this storage mode, the explicit value \\(s(i)\\) of the step corresponding to the \\(i\\)-th row of the dataset <code>value</code> is \\(s(i) = i\\times\\mathrm{step} + \\mathrm{offset}\\) where \\(\\mathrm{offset}\\) is set to zero if absent. The corresponding formula for the time \\(t(i)\\) is identical: \\(t(i) = i\\times\\mathrm{time} + \\mathrm{offset}\\). The index \\(i\\) is zero-based.</p>"},{"location":"examples/computational_data/h5md_expl.html#time-independent-data","title":"Time-independent data","text":"<p>H5MD defines a time-independent H5MD element as a dataset. As for the <code>value</code> dataset in the case of time-dependent data, data type and array shape are implied by the stored data, where the <code>[variable]</code> dimension is omitted.</p>"},{"location":"examples/computational_data/h5md_expl.html#storage-order-of-arrays","title":"Storage order of arrays","text":"<p>All arrays are stored in C-order as enforced by the HDF5 file format. A C or C++ program may thus declare <code>r[N][D]</code> for the array of particle coordinates while the Fortran program will declare a <code>r(D,N)</code> array (appropriate index ordering for a system of <code>N</code> particles in <code>D</code> spatial dimensions), and the HDF5 file will be the same.</p>"},{"location":"examples/computational_data/h5md_expl.html#storage-of-particles-and-tuples-lists","title":"Storage of particles and tuples lists","text":""},{"location":"examples/computational_data/h5md_expl.html#storage-of-a-list-of-particles","title":"Storage of a list of particles","text":"<p>A list of particles is an H5MD element:</p> <pre><code>&lt;list_name&gt;: Integer[N]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>list_name</code> is a dataset of <code>Integer</code> type and dimensions <code>[N]</code>, N being the number of particle indices stored in the list. <code>particles_group</code> is an attribute containing an HDF5 Object Reference as defined by the HDF5 file format. <code>particles_group</code> must refer to one of the groups in <code>/particles</code>.</p> <p>If a fill value is defined for <code>list_name</code>, the particles indices in <code>list_name</code> set to this value are ignored.</p> <p>If the corresponding <code>particles_group</code> does not possess the <code>id</code> element, the values in <code>list_name</code> correspond to the indexing of the elements in <code>particles_group</code>. Else, the values in <code>list_name</code> must be put in correspondence with the equal values in the <code>id</code> element.</p>"},{"location":"examples/computational_data/h5md_expl.html#storage-of-tuples","title":"Storage of tuples","text":"<p>A list of tuples is an H5MD element:</p> <pre><code>&lt;tuples_list_name&gt;: Integer[N,T]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>N</code> is the length of the list and <code>T</code> is the size of the tuples.  Both <code>N</code> and <code>T</code> may indicate variable dimensions. <code>particles_group</code> is an attribute containing an HDF5 Object Reference, obeying the same rules as for the lists of particles.</p> <p>The interpretation of the values stored within the tuples is done as for a list of particles.</p> <p>If a fill value is defined, tuples with at least one entry set to this value are ignored.</p>"},{"location":"examples/computational_data/h5md_expl.html#time-dependence-time-dependent-particle-lists-currently-not-supported-in-h5md-nomad","title":"Time-dependence (time-dependent particle lists currently not supported in H5MD-NOMAD)","text":"<p>As the lists of particles and tuples above are H5MD elements, they can be stored either as time-dependent groups or time-independent datasets.</p> <p>As an example, a time-dependent list of pairs is stored as:</p> <pre><code>&lt;pair_list_name&gt;\n   +-- particles_group: Object reference\n   \\-- value: Integer[variable,N,2]\n   \\-- step: Integer[variable]\n</code></pre> <p>The dimension denoted by <code>N</code> may be variable.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-root-level","title":"The root level","text":"<p>The root level of H5MD-NOMAD structure is organized as follows (identical to the original H5MD specification):</p> <pre><code>H5MD-NOMAD root\n \\-- h5md\n \\-- (particles)\n \\-- (observables)\n \\-- (connectivity)\n \\-- (parameters)\n</code></pre> <code>h5md</code> A group that contains metadata and information on the H5MD structure itself. It is the only mandatory group at the root level of H5MD. <code>particles</code> An optional group that contains information on each particle in the system, e.g., a snapshot of the positions or the full trajectory in phase space. <code>observables</code> An optional group that contains other quantities of interest, e.g., physical observables that are derived from the system state at given points in time. <code>connectivity</code> An optional group that contains information about the connectivity between particles. <code>parameters</code> An optional group that contains application-specific (meta)data such as control parameters or simulation scripts."},{"location":"examples/computational_data/h5md_expl.html#the-h5md-group","title":"The H5MD Group","text":"<p>A set of global metadata describing the H5MD structure is stored in the <code>h5md</code> group as attributes. The contents of the group are:</p> <pre><code>h5md\n +-- version: Integer[2]\n \\-- author\n |    +-- name: String[]\n |    +-- (email: String[])\n \\-- creator\n |    +-- name: String[]\n |    +-- version: String[]\n \\-- program\n      +-- name: String[]\n      +-- version: String[]\n</code></pre> <code>version</code> An attribute, of <code>Integer</code> datatype and of simple dataspace of rank 1 and size 2, that contains the major version number and the minor version number of the H5MD specification the H5MD structure conforms to. <p>The version x.y.z of the H5MD specification follows semantic versioning: A change of the major version number x indicates backward-incompatible changes to the file structure. A change of the minor version number y indicates backwards-compatible changes to the file structure. A change of the patch version number z indicates changes that have no effect on the file structure and serves to allow for clarifications or minor text editing of the specification.</p> <p>As the z component has no impact on the content of an H5MD file, the <code>version</code> attribute contains only x and y.</p> <code>author</code> A group that contains metadata on the person responsible for the simulation (or the experiment) as follows: <ul> <li> <code>name</code> An attribute, of fixed-length string datatype and of scalar dataspace, that holds the author's real name. </li> <li> <code>email</code> An optional attribute, of fixed-length string datatype and of scalar dataspace, that holds the author's email address of the form <code>email@domain.tld</code>. </li> </ul> <code>creator</code> A group that contains metadata on the program that created the H5MD structure as follows: <ul> <li> <code>name</code> An attribute, of fixed-length string datatype and of scalar dataspace, that stores the name of the program. </li> <li> <code>version</code> An attribute, of fixed-length string datatype and of scalar dataspace, that yields the version of the program. </li> </ul> <code>program</code> A group that contains metadata on the code/package that created the simulation data contained within this H5MD structure: <ul> <li> <code>name</code> An attribute, of fixed-length string datatype and of scalar dataspace, that stores the name of the program. </li> <li> <code>version</code> An attribute, of fixed-length string datatype and of scalar dataspace, that yields the version of the program. </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#modules-currently-unused-in-h5md-nomad","title":"Modules (currently unused in H5MD-NOMAD)","text":"<p>The original H5MD specification allowed the definition of modules under the h5md group. Such modules are currently ignored when uploading to NOMAD, although they of course will remain present in the raw uploaded hdf5 file.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-particles-group","title":"The particles group","text":"<p>Particle attributes, i.e., information about each particle in the system, are stored within the <code>particles</code> group. According to the original H5MD schema, the <code>particles</code> group is a container for subgroups that represent different subsets of the system under consideration. For simplicity of parsing, H5MD-NOMAD currently requires one such group, labeled <code>all</code>, to contain all the particles and corresponding attributes to be stored in the NOMAD archive. Additional particle groups will be ignored.</p> <p>For each dataset, the ordering of indices (whenever relevant) is as follows: frame index, particle index, dimension index. Thus, the contents of the <code>particles</code> group for a trajectory with <code>N_frames</code> frames and <code>N_part</code> particles in a <code>D</code>-dimensional space can be represented:</p> <pre><code>particles\n \\-- all\n |    \\-- box\n |    \\-- (&lt;time-dependent_vector_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part][D]\n |    \\-- (&lt;time-dependent_scalar_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part]\n |    \\-- (&lt;time-independent_vector_attribute&gt;): &lt;type&gt;[N_part][D]\n |    \\-- (&lt;time-independent_scalar_attribute&gt;): &lt;type&gt;[N_part]\n |    \\-- ...\n \\-- &lt;group2&gt;\n      \\-- ...\n</code></pre>"},{"location":"examples/computational_data/h5md_expl.html#standardized-h5md-elements-for-particles-group","title":"Standardized H5MD elements for particles group","text":"<code>position</code> (required for parsing other particle attributes) An element that describes the particle positions as coordinate vectors of <code>Float</code> or <code>Integer</code> type. <code>velocity</code> An element that contains the velocities for each particle as a vector of <code>Float</code> or <code>Integer</code> type. <code>force</code> An element that contains the total forces (i.e., the accelerations multiplied by the particle mass) for each particle as a vector of <code>Float</code> or <code>Integer</code> type. <code>mass</code> An element that holds the mass for each particle as a scalar of <code>Float</code> type. <ul> <li> <code>image</code> (currently unused in H5MD-NOMAD) </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#todo-can-we-make-these-admonitions-indented-somehow-or-more-obviously-connected-with-the-members-of-this-list","title":"TODO can we make these admonitions indented somehow or more obviously connected with the members of this list?","text":"Details <p>An element that represents periodic images of the box as coordinate vectors of <code>Float</code> or <code>Integer</code> type and allows one to compute for each particle its absolute position in space. If <code>image</code> is present, <code>position</code> must be present as well. For time-dependent data, the <code>step</code> and <code>time</code> datasets of <code>image</code> must equal those of <code>position</code>, which must be accomplished by hard-linking the respective datasets.</p> <code>species</code> (currently unused in H5MD-NOMAD) Details <p>An element that describes the species for each particle, i.e., its atomic or chemical identity, as a scalar of <code>Enumeration</code> or <code>Integer</code> data type. Particles of the same species are assumed to be identical with respect to their properties and unbonded interactions.</p> <code>id</code> (currently unused in H5MD-NOMAD) Details <p>An element that holds a scalar identifier for each particle of <code>Integer</code> type, which is unique within the given particle subgroup. The <code>id</code> serves to identify particles over the course of the simulation in the case when the order of the particles changes, or when new particles are inserted and removed. If <code>id</code> is absent, the identity of the particles is given by their index in the <code>value</code> datasets of the elements within the same subgroup.</p> <code>charge</code> An element that contains the charge associated to each particle as a scalar, of <code>Integer</code> or <code>Float</code> type."},{"location":"examples/computational_data/h5md_expl.html#standardized-h5md-nomad-elements-for-particles-group","title":"Standardized H5MD-NOMAD elements for particles group","text":"<code>species_label</code>  An element that holds a label (fixed-length string datatype) for each particle. This label denotes the fundamental species type of the particle (e.g., the chemical element label for atoms), regardless of its given interactions within the model. Both time-independent and time-dependent <code>species_label</code> elements are supported. <code>model_label</code> An element that holds a label (fixed-length string datatype) for each particle. This label denotes the type of particle with respect to the given interactions within the model (e.g., force field) Currently only time-independent species labels are supported."},{"location":"examples/computational_data/h5md_expl.html#non-standard-elements-in-particles-group","title":"Non-standard elements in particles group","text":"<p>All non-standard elements within the particles group are currently ignored by the NOMAD H5MD parser. In principle, one can store additional custom attributes as configuration-specific observables (see The observables group).</p>"},{"location":"examples/computational_data/h5md_expl.html#the-simulation-box-subgroup","title":"The simulation box subgroup","text":"<p>Information about the simulation box is stored in a subgroup named <code>box</code>, within the relevant particles group (<code>all</code> in our case). Both time-independent and time-dependent box information are supported (i.e. via the <code>edges</code> element). Because the <code>box</code> group is specific to a particle group of particles, time-dependent boxes must contain <code>step</code> and <code>time</code> datasets that exactly match those of the corresponding <code>position</code> group. In principal, this should be accomplished by hard-linking the respective datasets. In practice, H5MD-NOMAD currently assumes that this is the case (i.e., the box group <code>step</code> and <code>time</code> information is unused), and simply checks that <code>edges.value</code> has the same leading dimension as <code>position</code>.</p> <p>The structure of the <code>box</code> group is as follows:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- (edges)\n</code></pre> <code>dimension</code> An attribute that stores the spatial dimension <code>D</code> of the simulation box and is of <code>Integer</code> datatype and scalar dataspace. <code>boundary</code>  An attribute, of boolean datatype (changed from string to boolean in H5MD-NOMAD) and of simple dataspace of rank 1 and size <code>D</code>, that specifies the boundary condition of the box along each dimension, i.e., <code>True</code> implies periodic boundaries are applied in the corresponding dimension. If all values in <code>boundary</code> are <code>False</code>, <code>edges</code> may be omitted. <code>edges</code> A <code>D</code>-dimensional vector or a <code>D</code> \u00d7 <code>D</code> matrix, depending on the geometry of the box, of <code>Float</code> or <code>Integer</code> type. Only cuboid and triclinic boxes are allowed. If <code>edges</code> is a vector, it specifies the space diagonal of a cuboid-shaped box. If <code>edges</code> is a matrix, the box is of triclinic shape with the edge vectors given by the rows of the matrix. For a time-dependent box, a cuboid geometry is encoded by a dataset <code>value</code> (within the H5MD element) of rank 2 (1 dimension for the time and 1 for the vector) and a triclinic geometry by a dataset <code>value</code> of rank 3 (1 dimension for the time and 2 for the matrix). For a time-independent box, a cuboid geometry is encoded by a dataset <code>edges</code> of rank 1 and a triclinic geometry by a dataset of rank 2. <p>For instance, a cuboid box that changes in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges\n                \\-- step: Integer[variable]\n                \\-- time: Float[variable]\n                \\-- value: &lt;type&gt;[variable][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>. A triclinic box that is fixed in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges: &lt;type&gt;[D][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-observables-group","title":"The observables group","text":"<p>The initial H5MD proposed a simple and flexible schema for the general storage of observable info, defined roughly as \"macroscopic observables\" or \"averages of a property over many particles\", as H5MD elements:</p> <pre><code>observables\n \\-- &lt;observable1&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames]\n \\-- &lt;observable2&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames][D]\n \\-- &lt;group1&gt;\n |    \\-- &lt;observable3&gt;\n |         \\-- step: Integer[N_frames]\n |         \\-- time: Float[N_frames]\n |         \\-- value: &lt;type&gt;[N_frames][D][D]\n \\-- &lt;observable4&gt;: &lt;type&gt;[]\n \\-- ...\n</code></pre> <p></p> <p>As depicted above, observables representing only a subset of the particles may be stored in appropriate subgroups similar to the <code>particles</code> tree. H5MD-NOMAD does support the organization of observables into subgroups (as discussed in more detail below). However, grouping by particle groups is not fully supported in the sense that there is currently no metadata storing the corresponding indices of the relevant particles subgroup. Additionally, since only the <code>all</code> particles group is parsed, information about the named subgroup will not be stored anywhere in the archive. Thus, we recommend for now that only observables relevant to the <code>all</code> particles subgroup are stored within this section.</p>"},{"location":"examples/computational_data/h5md_expl.html#h5md-nomad-observables","title":"H5MD-NOMAD observables","text":"<p>H5MD-NOMAD extends H5MD observable storage by 1. specifying standard observable types with associated metadata and 2. providing standardized specifications for some common observables. In contrast to the schema above, a more restrictive structure is required:</p> <pre><code>observables\n \\-- &lt;observable_type_1&gt;\n |    \\-- &lt;observable_1_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n \\-- &lt;observable_type_2&gt;\n |    \\-- &lt;observable_2_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- &lt;observable_2_label_2&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- ...\n \\-- ...\n</code></pre> <p>Here, each <code>observable_type</code> corresponds to a particular group of observables, e.g., to be plotted together in a single plot. The given name for this group could be generic, e.g., <code>radial distribution function</code>, or more specific, e.g., <code>molecular radial distribution function for solvents</code>. The latter may be useful in case multiple groupings of a single type of observable are needed. Each <code>observable_label</code> then corresponds to a specific name for an individual instance of this observable type. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>observable_label</code> might be set to <code>A-B</code>.</p> <p>Finally, H5MD-NOMAD has added the observable <code>type</code> as an attribute of each observable: The following observable types are supported:</p> <p></p> <code>configurational</code> <p>An observable that is computed for each individual configuration, with the following general structure:</p> <p>observables  --   |    --   |    |    +-- type: \"configurational\"  |    |    -- step: Integer[N_frames]  |    |    -- time: Float[N_frames]  |    |    -- value: [N_frames][M]  |    -- ...  -- ...  where <code>M</code> is the dimension of the observable. This section may also be used to store per-particle quantities/attributes that are not currently supported as standardized H5MD-NOMAD elements for particles group, in which case <code>value</code> will have dimensions <code>[N_frames][N_part][M]</code>. <p></p> <code>ensemble_average</code> <p>An observable that is computed by averaging over multiple configurations, with the following generic structure:</p> <p>observables  --   |    --   |    |    +-- type: \"ensemble_average\"  |    |    -- (n_variables): Integer  |    |    -- (variables_name): String[n_variables][]  |    |    -- (n_bins): Integer[]  |    |    -- bins: Float[n_bins][]  |    |    -- value: [n_bins][]  |    |    -- (frame_start): Integer  |    |    -- (frame_end): Integer  |    |    -- (n_smooth): Integer  |    |    -- (type): String[]  |    |    -- (error_type): String[]  |    |    -- (errors): Float[n_bins]  |    |    -- (error_labels): String[]  |    |    -- (frame_end): Integer  |    |    -- (): []  |    -- ...  -- ... <ul> <li> <code>n_variables</code> dimensionality of the observable. Can also be inferred from leading dimension of <code>bins</code>. </li> <li> <code>variables_name</code> name/description of the independent variables along which the observable is defined. </li> <li> <code>n_bins</code> number of bins along each dimension of the observable. Either single Integer for 1-D observables, or a list of Integers for multi-dimensional observable. Can also be inferred from dimensions of <code>bins</code>. </li> <li> <code>bins</code> value of the bins used for calculating the observable along each dimension of the observable. </li> <li> <code>value</code> value of the calculated ensemble average at each bin. </li> <li> <code>frame_start</code> trajectory frame index at which the averaging begins. This index must correspond to the list of steps and times in <code>particles.all.position</code>. </li> <li> <code>frame_end</code> trajectory frame index at which the averaging ends. This index must correspond to the list of steps and times in <code>particles.all.position</code>. </li> <li> <code>n_smooth</code> number of bins over which the running average was computed for <code>value</code>. </li> <li> <code>type</code> Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level. </li> </ul> <ul> <li> <code>error_type</code> describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>. </li> <li> <code>errors</code> value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>. </li> <li> <code>error_labels</code> describes the error along individual dimensions for multi-D errors. </li> <li> <code>&lt;custom_dataset&gt;</code> additional metadata may be given as necessary. </li> </ul> <p></p> <code>time_correlation</code> <p>An observable that is computed by calculating correlations between configurations in time, with the following general structure:</p> <p>observables  --   |    --   |    |    +-- type: \"time_correlation\"  |    |    -- (direction): String[]  |    |    -- (n_times): Integer[]  |    |    -- times: Float[n_times][]  |    |    -- value: [n_bins][]  |    |    -- (type): String[]  |    |    -- (error_type): String[]  |    |    -- (errors): Float[n_bins]  |    |    -- (error_labels): String[]  |    |    -- (): []  |    -- ...  -- ... <ul> <li> <code>label</code> describes the particles involved in determining the property. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>label</code> might be set to <code>A-B</code> </li> <li> <code>direction</code> allowed values of <code>x</code>, <code>y</code>, <code>z</code>, <code>xy</code>, <code>yz</code>, <code>xz</code>, <code>xyz</code>. The direction/s used for calculating the correlation function. </li> <li> <code>n_times</code> number of times windows for the calculation of the correlation function. Can also be inferred from dimensions of <code>times</code>. </li> <li> <code>times</code> time values used for calculating the correlation function (i.e., \u0394t values). </li> <li> <code>value</code> value of the calculated correlation function at each time. </li> <li> <code>type</code> Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level. </li> </ul> <ul> <li> <code>error_type</code> describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>. </li> <li> <code>errors</code> value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>. </li> <li> <code>error_labels</code> describes the error along individual dimensions for multi-D errors. </li> <li> <code>&lt;custom_dataset&gt;</code> additional metadata may be given as necessary. </li> </ul> <p>A list of standardized observables can be found in Reference - H5MD-NOMAD &gt; Standardized observables in H5MD-NOMAD.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-connectivity-group","title":"The connectivity group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of \"connectivity\" information, e.g., to be used in conjunction with a molecular mechanics force field. The connectivity information is stored as tuples in the group <code>/connectivity</code>. The tuples are pairs, triples, etc. as needed and may be either time-independent or time-dependent. As with other elements, connectivity elements can be defined for particular particle groups. However, H5MD-NOMAD focuses on the storage of connectivity elements for the entire system (i.e., the <code>all</code> particles group).</p>"},{"location":"examples/computational_data/h5md_expl.html#standardized-h5md-nomad-connectivity","title":"Standardized H5MD-NOMAD connectivity","text":"<p>The general structure of the <code>connectivity</code> group is as follows:</p> <pre><code>connectivity\n \\-- (bonds): Integer[N_part][2]\n \\-- (angles): Integer[N_part][3]\n \\-- (dihedrals): Integer[N_part][4]\n \\-- (impropers): Integer[N_part][4]\n \\-- (&lt;custom_interaction&gt;): Integer[N_part][m]\n \\-- (particles_group)\n      \\-- ...\n</code></pre> <p><code>N_part</code> corresponds to the number of particles stored in the <code>particles/all</code> group.</p> <ul> <li> <code>bonds</code> a list of 2-tuples specifying the indices of particles containing a \"bond interaction\". </li> <li> <code>angles</code> a list of 3-tuples specifying the indices of particles containing an \"angle interaction\". </li> <li> <code>dihedrals</code> a list of 4-tuples specifying the indices of particles containing a \"dihedral interaction\". </li> <li> <code>impropers</code> a list of 4-tuples specifying the indices of particles containing an \"improper dihedral interaction\". </li> <li> <code>&lt;custom_interaction&gt;</code> a list of m-tuples specifying the indices of particles containing an arbitrary interaction. <code>m</code> denotes the number of particles involved in the interaction. </li> <li> <code>particles_group</code> See below. </li> </ul> <p> Currently only time-independent connectivity elements are supported.</p>"},{"location":"examples/computational_data/h5md_expl.html#the-particles_group-subgroup","title":"The particles_group subgroup","text":"<p>Despite not fully utilizing the organization of arbitrary groups of particles within the <code>particles</code> group, H5MD-NOMAD allows for the user to provide an arbitrary hierarchy of particle groupings, also referred to as a \"topology\", within the <code>connectivity</code> subgroup called <code>particles_group</code>. This information will be used by NOMAD to facilitate visualizations of the system, through the \"topology bar\" in the overview page. The general structure of the topology group is as follows:</p> <pre><code>connectivity\n \\-- particles_group\n      \\-- &lt;group_1&gt;\n      |    \\-- (type): String[]\n      |    \\-- (formula): String[]\n      |    \\-- indices: Integer[]\n      |    \\-- (is_molecule): Bool\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n      |    \\-- (particles_group):\n      |        \\-- ...\n      \\-- &lt;group_2&gt;\n          \\-- ...\n</code></pre> <p>The initial <code>particles_group</code> subgroup, directly under <code>connectivity</code>, is a container for the entire topology. <code>particles_group</code> contains a series of subgroups with arbitrary names, which denote the first level of organization within the topology. The name of each subgroup will become the group label within the NOMAD metadata. Each of these subgroups then contain a series of datasets:</p> <ul> <li> <code>type</code> describes the type of particle group. There exists a list of standardized types: <code>molecule_group</code>, <code>molecule</code>, <code>monomer_group</code>, <code>monomer</code>. However, arbitrary types can be given. We suggest that you 1. use the standardized types when appropriate (note that protein residues should be generically typed as <code>monomer</code>) and 2. use the general format <code>&lt;type&gt;_group</code> for groups of a distinct type (see further description of suggested hierarchy below). </li> <li> <code>formula</code> a \"chemical-like\" formula that describes the particle group with respect to its underlying components. The format for the formula is <code>&lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...</code>, where <code>&lt;child_x&gt;</code> is the name/label of the underlying component, and <code>n_child_x</code> is the number of such components found within this particle group. Example: A particles group containing 100 water molecules named <code>water</code> has the formula <code>water(100)</code>, whereas each underlying water molecule has the standard chemical formula <code>H2O</code>. </li> <li> <code>indices</code> a list of integer indices corresponding to all particles belonging to this group. Indices should correspond to the list of particles stored in the <code>particles/all</code> group. </li> <li> <code>is_molecule</code> indicator of individual molecules (typically with respect to the bond connections defined by a force field). </li> <li> <code>custom_dataset</code> arbitrary additional metadata for this particle group may be given. </li> </ul> <p>Each subgroup may also contain a (nested) <code>particles_group</code> subgroup, in order to subdivide the group of particles into an organizational hierarchy. As with the overall <code>particles_group</code> container, the groups contained within <code>particles_group</code> must not partition the particles within this group (i.e., overlapping or non-complete groupings are allowed). However, particle groups must contain particles already contained within the parent <code>particles_group</code> (i.e., subgroups must be a subset of the grouping at the previous level of the hierarchy).</p> <p>Note that typically the <code>particles_group</code> hierarchy ends at the level of individual particles (i.e., individual particles are not stored, since this information is already contained within the <code>particles</code> group).</p>"},{"location":"examples/computational_data/h5md_expl.html#the-parameters-group","title":"The parameters group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of general \"parameter\" information within the <code>parameters</code> group, with the following structure:</p> <pre><code>parameters\n +-- &lt;user_attribute1&gt;\n \\-- &lt;user_data1&gt;\n \\-- &lt;user_group1&gt;\n |    \\-- &lt;user_data2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>In contrast, the H5MD-NOMAD schema calls for very specific structures to be used when storing parameter information. While the previous groups have attempted to stay away from enforcing NOMAD-specific data structures on the user, instead opting for more intuitive and generally-convenient structures, the <code>parameters</code> group utilizes already-existing metadata and structures within NOMAD to efficiently import simulation parameters in a way that is searchable and comparable to simulations performed by other users.</p> <p>In this way, the H5MD-NOMAD <code>parameters</code> group has the following structure:</p> <pre><code>parameters\n \\-- &lt;parameter_subgroup_1&gt;\n |    \\-- ...\n \\-- &lt;parameter_subgroup_2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>The subgroups <code>force_calculations</code> and <code>workflow</code> are supported. The following describes the detailed data structures for these subgroups, using the NOMAD MetaInfo definitions for each underlying <code>Quantity</code>. Please note that:</p> <ol> <li> <p>Quantities with <code>type=MEnum()</code> are restricted to the provided allowed values.</p> </li> <li> <p>The unit given in the MetaInfo definition does not have to be used within the H5MD-NOMAD file, however, the dimensionality of the unit should match.</p> </li> </ol>"},{"location":"examples/computational_data/h5md_expl.html#force-calculations","title":"Force calculations","text":"<p>The <code>force_calculations</code> group contains the parameters for force calculations according to the force field during a molecular dynamics run.</p> <p></p> <p>The following json template illustrates the structure of the <code>force_calculations</code> group, with example values for clarity:</p> <pre><code>{\n    \"vdw_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n    \"coulomb_type\": \"particle_mesh_ewald\",\n    \"coulomb_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n    \"neighbor_searching\": {\n        \"neighbor_update_frequency\": 1,\n        \"neighbor_update_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"}\n        }\n    }\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>vdw_cutoff</code> :</p> <pre><code>Quantity(\n        type=np.float64,\n        shape=[],\n        unit='m',\n        description='''\n        Cutoff for calculating VDW forces.\n        ''')\n</code></pre> </li> <li> <p><code>coulomb_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('cutoff', 'ewald', 'multilevel_summation', 'particle_mesh_ewald',\n            'particle_particle_particle_mesh', 'reaction_field'),\n    shape=[],\n    description='''\n    Method used for calculating long-ranged Coulomb forces.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"Cutoff\"`          | Simple cutoff scheme. |\n\n    | `\"Ewald\"` | Standard Ewald summation as described in any solid-state physics text. |\n\n    | `\"Multi-Level Summation\"` |  D. Hardy, J.E. Stone, and K. Schulten,\n    [Parallel. Comput. **35**, 164](https://doi.org/10.1016/j.parco.2008.12.005)|\n\n    | `\"Particle-Mesh-Ewald\"`        | T. Darden, D. York, and L. Pedersen,\n    [J. Chem. Phys. **98**, 10089 (1993)](https://doi.org/10.1063/1.464397) |\n\n    | `\"Particle-Particle Particle-Mesh\"` | See e.g. Hockney and Eastwood, Computer Simulation Using Particles,\n    Adam Hilger, NY (1989). |\n\n    | `\"Reaction-Field\"` | J.A. Barker and R.O. Watts,\n    [Mol. Phys. **26**, 789 (1973)](https://doi.org/10.1080/00268977300102101)|\n    ''')\n</code></pre> </li> <li> <p><code>coulomb_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    Cutoff for calculating short-ranged Coulomb forces.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_searching</code> : Section containing the parameters for neighbor searching/lists during a molecular dynamics run.</p> </li> <li> <p><code>neighbor_update_frequency</code> :</p> <pre><code>Quantity(\n    type=int,\n    shape=[],\n    description='''\n    Number of timesteps between updating the neighbor list.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_update_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    The distance cutoff for determining the neighbor list.\n    ''')\n</code></pre> </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#the-molecular-dynamics-workflow","title":"The molecular dynamics workflow","text":"<p>The <code>workflow</code> group contains the parameters for any type of workflow. Here we describe the specific case of the well-defined <code>molecular_dynamics</code> workflow. Custom workflows are described in detail in Workflows in NOMAD.</p> <p></p> <p>The following json template illustrates the structure of the <code>molecular_dynamics</code> subsection of the <code>workflow</code> group, with example values for clarity:</p> <pre><code>{\n    \"molecular_dynamics\": {\n        \"thermodynamic_ensemble\": \"NPT\",\n        \"integrator_type\": \"langevin_leap_frog\",\n        \"integration_timestep\": {\"value\": 2e-15, \"unit\": \"ps\"},\n        \"n_steps\": 20000000,\n        \"coordinate_save_frequency\": 10000,\n        \"velocity_save_frequency\": null,\n        \"force_save_frequency\": null,\n        \"thermodynamics_save_frequency\": null,\n        \"thermostat_parameters\": {\n            \"thermostat_type\": \"langevin_leap_frog\",\n            \"reference_temperature\": {\"value\": 300.0, \"unit\": \"kelvin\"},\n            \"coupling_constant\": {\"value\": 1.0, \"unit\": \"ps\"}},\n        \"barostat_parameters\": {\n            \"barostat_type\": \"berendsen\",\n            \"coupling_type\": \"isotropic\",\n            \"reference_pressure\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], \"unit\": \"bar\"},\n            \"coupling_constant\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]},\n            \"compressibility\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]}\n            }\n    }\n}\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>thermodynamic_ensemble</code> :</p> <pre><code>Quantity(\n    type=MEnum('NVE', 'NVT', 'NPT', 'NPH'),\n    shape=[],\n    description='''\n    The type of thermodynamic ensemble that was simulated.\n\n    Allowed values are:\n\n    | Thermodynamic Ensemble          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"NVE\"`           | Constant number of particles, volume, and energy |\n\n    | `\"NVT\"`           | Constant number of particles, volume, and temperature |\n\n    | `\"NPT\"`           | Constant number of particles, pressure, and temperature |\n\n    | `\"NPH\"`           | Constant number of particles, pressure, and enthalpy |\n    ''')\n</code></pre> </li> <li> <p><code>integrator_type</code> :         Quantity(             type=MEnum(                 'brownian', 'conjugant_gradient', 'langevin_goga',                 'langevin_schneider', 'leap_frog', 'rRESPA_multitimescale', 'velocity_verlet'             ),             shape=[],             description='''             Name of the integrator.</p> <pre><code>    Allowed values are:\n\n    | Integrator Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"leap_frog\"`          | R.W. Hockney, S.P. Goel, and J. Eastwood,\n    [J. Comp. Phys. **14**, 148 (1974)](https://doi.org/10.1016/0021-9991(74)90010-2) |\n\n    | `\"velocity_verlet\"` | W.C. Swope, H.C. Andersen, P.H. Berens, and K.R. Wilson,\n    [J. Chem. Phys. **76**, 637 (1982)](https://doi.org/10.1063/1.442716) |\n\n    | `\"rRESPA_multitimescale\"` | M. Tuckerman, B. J. Berne, and G. J. Martyna\n    [J. Chem. Phys. **97**, 1990 (1992)](https://doi.org/10.1063/1.463137) |\n    ''')\n</code></pre> </li> <li> <p><code>integration_timestep</code> :         Quantity(             type=np.float64,             shape=[],             unit='s',             description='''             The timestep at which the numerical integration is performed.             ''')</p> </li> <li> <p><code>n_steps</code> :         Quantity(             type=int,             shape=[],             description='''             Number of timesteps performed.             ''')</p> </li> <li> <p><code>coordinate_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the coordinates.             ''')</p> </li> <li> <p><code>velocity_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the velocities.             ''')</p> </li> <li> <p><code>force_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the forces.             ''')</p> </li> <li> <p><code>thermodynamics_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the thermodynamic quantities.             ''')</p> </li> <li> <code>thermostat_parameters</code> Section containing the parameters pertaining to the thermostat for a molecular dynamics run. </li> <li> <p><code>thermostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('andersen', 'berendsen', 'brownian', 'langevin_goga', 'langevin_schneider', 'nose_hoover', 'velocity_rescaling',\n            'velocity_rescaling_langevin'),\n    shape=[],\n    description='''\n    The name of the thermostat used for temperature control. If skipped or an empty string is used, it\n    means no thermostat was applied.\n\n    Allowed values are:\n\n    | Thermostat Name        | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"andersen\"`           | H.C. Andersen, [J. Chem. Phys.\n    **72**, 2384 (1980)](https://doi.org/10.1063/1.439486) |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"brownian\"`           | Brownian Dynamics |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"velocity_rescaling\"` | G. Bussi, D. Donadio, and M. Parrinello,\n    [J. Chem. Phys. **126**, 014101 (2007)](https://doi.org/10.1063/1.2408420) |\n\n    | `\"velocity_rescaling_langevin\"` | G. Bussi and M. Parrinello,\n    [Phys. Rev. E **75**, 056707 (2007)](https://doi.org/10.1103/PhysRevE.75.056707) |\n    ''')\n</code></pre> </li> <li> <p><code>reference_temperature</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kelvin',\n    description='''\n    The target temperature for the simulation.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='s',\n    description='''\n    The time constant for temperature coupling. Need to describe what this means for the various\n    thermostat options...\n    ''')\n</code></pre> </li> <li> <p><code>effective_mass</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kilogram',\n    description='''\n    The effective or fictitious mass of the temperature resevoir.\n    ''')\n</code></pre> </li> <li> <code>barostat_parameters</code> Section containing the parameters pertaining to the barostat for a molecular dynamics run. </li> <li> <p><code>barostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('berendsen', 'martyna_tuckerman_tobias_klein', 'nose_hoover', 'parrinello_rahman', 'stochastic_cell_rescaling'),\n    shape=[],\n    description='''\n    The name of the barostat used for temperature control. If skipped or an empty string is used, it\n    means no barostat was applied.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"martyna_tuckerman_tobias_klein\"` | G.J. Martyna, M.E. Tuckerman, D.J. Tobias, and M.L. Klein,\n    [Mol. Phys. **87**, 1117 (1996)](https://doi.org/10.1080/00268979600100761);\n    M.E. Tuckerman, J. Alejandre, R. L\u00f3pez-Rend\u00f3n, A.L. Jochim, and G.J. Martyna,\n    [J. Phys. A. **59**, 5629 (2006)](https://doi.org/10.1088/0305-4470/39/19/S18)|\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"parrinello_rahman\"`        | M. Parrinello and A. Rahman,\n    [J. Appl. Phys. **52**, 7182 (1981)](https://doi.org/10.1063/1.328693);\n    S. Nos\u00e9 and M.L. Klein, [Mol. Phys. **50**, 1055 (1983) |\n\n    | `\"stochastic_cell_rescaling\"` | M. Bernetti and G. Bussi,\n    [J. Chem. Phys. **153**, 114107 (2020)](https://doi.org/10.1063/1.2408420) |\n    ''')\n</code></pre> </li> <li> <p><code>coupling_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('isotropic', 'semi_isotropic', 'anisotropic'),\n    shape=[],\n    description='''\n    Describes the symmetry of pressure coupling. Specifics can be inferred from the `coupling constant`\n\n    | Type          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `isotropic`          | Identical coupling in all directions. |\n\n    | `semi_isotropic` | Identical coupling in 2 directions. |\n\n    | `anisotropic`        | General case. |\n    ''')\n</code></pre> </li> <li> <p><code>reference_pressure</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='pascal',\n    description='''\n    The target pressure for the simulation, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='s',\n    description='''\n    The time constants for pressure coupling, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. 0 values along the off-diagonal\n    indicate no-coupling between these directions.\n    ''')\n</code></pre> </li> <li> <p><code>compressibility</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='1 / pascal',\n    description='''\n    An estimate of the system's compressibility, used for box rescaling, stored in a 3x3 matrix indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. If None, it may indicate that these values\n    are incorporated into the coupling_constant, or simply that the software used uses a fixed value that is not available in\n    the input/output files.\n    ''')\n</code></pre> </li> </ul>"},{"location":"examples/computational_data/h5md_expl.html#units","title":"Units","text":"<p>In the original H5MD schema, units were given as string attributes of datasets, e.g., <code>60 m s-2</code>. H5MD-NOMAD amends the treatment of units in 2 ways:</p> <ol> <li> <p>If needed, the leading prefactor is stored as a separate attribute of <code>float</code> datatype called <code>unit_factor</code>.</p> </li> <li> <p>The string that describes the unit should be compatible with the <code>UnitRegistry</code> class of the <code>pint</code> python module.</p> </li> </ol> <p>Generic representation of unit storage in H5MD-NOMAD:</p> <pre><code>&lt;group&gt;\n    \\-- &lt;dataset&gt;\n        +-- (unit: String[])\n        +-- (unit_factor: Float)\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html","title":"How to work with the H5MD-NOMAD schema","text":"<p>Attention</p> <p>The H5MD-NOMAD functionalities are still in the beta testing phase. Please contact us with any issues or questions.</p>"},{"location":"examples/computational_data/h5md_howto.html#writing-an-hdf5-file-according-to-h5md-nomad-with-python","title":"Writing an HDF5 file according to H5MD-NOMAD with python","text":"<p>You can write to an HDF5 file via a python interface, using the h5py package. This section walks you through the creation of each section of the H5MD-NOMAD schema, using practical examples to help you get started.</p>"},{"location":"examples/computational_data/h5md_howto.html#imports","title":"Imports","text":"<pre><code>import numpy as np\nimport json\n\nimport h5py\nimport parmed as chem\nimport MDAnalysis as mda\nfrom pint import UnitRegistry\nureg = UnitRegistry()\n</code></pre> h5py module for reading and writing HDF5 files. UnitRegistry object from the pint package that provides assistance for working with units. We suggest using this package for easiest compatibility with NOMAD. If you have <code>nomad-lab</code> installed, you can alternatively import <code>ureg</code> with <code>from nomad.units import ureg</code>. MDAnalysis a library to analyze trajectories from molecular dynamics simulations stored in various formats. ParmEd a tool for aiding in investigations of biomolecular systems using popular molecular simulation packages."},{"location":"examples/computational_data/h5md_howto.html#example-data","title":"Example Data","text":"<p>For concreteness, we consider a fictitious set of \"vanilla\" molecular dynamics simulations, run with the OpenMM software. The following definitions set the dimensionality, periodicity, and the units for this simulation.</p> <pre><code>dimension = 3\nperiodicity = [True, True, True]\n\ntime_unit = 1.0 * ureg.picosecond\nlength_unit = 1.0 * ureg.angstrom\nenergy_unit = 1000. * ureg.joule\nmass_unit = 1.0 * ureg.amu\ncharge_unit = 1.0 * ureg.e\ntemperature_unit = 1.0 * ureg.K\ncustom_unit = 1.0 * ureg.newton / length_unit**2\nacceleration_unit = 1.0 * length_unit / time_unit**2\n</code></pre> <p>In this example, we will assume that the relevant simulation data is compatible with MDAnalysis, such that a universe containing the trajectory and topology information can be created.</p> <p>Note</p> <p>Knowledge of the MDAnalysis package is not necessary for understanding this example. The dimensions of the supplied quantities will be made clear in each case.</p> <p>Create a universe by supplying a <code>pdb</code> structure file and corresponding <code>dcd</code> trajectory file (MDAnalysis supports many different file formats): <pre><code>universe = mda.Universe('initial_structure.pdb', 'trajectory.dcd')\n\nn_frames = len(universe.trajectory)\nn_atoms = universe.trajectory[0].n_atoms\n</code></pre> Some topologies can be loaded directly into MDAnalysis. However, for simulations from OpenMM, one can read the topology using <code>parmed</code> and then import it to MDanalysis: <pre><code>pdb = app.PDBFile('initial_structure.pdb')\nforcefield = app.ForceField('force_field.xml')\nsystem = forcefield.createSystem(pdb.topology)\nstruct = chem.openmm.load_topology(pdb.topology, system)\nuniverse_toponly = mda.Universe(struct)\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#h5md-group","title":"H5MD Group","text":"<p>Create an HDF5 file called <code>test_h5md-nomad.h5</code> and create the group <code>h5md</code> under <code>root</code>: <pre><code>h5_write = h5py.File('test_h5md-nomad.h5', 'w')\nh5md = h5_write.create_group('h5md')\n</code></pre></p> <p>Add the h5md version (1.0.x in this case) as an attribute of the <code>h5md</code> group: <pre><code>h5md.attrs['version'] = [1, 0]\n</code></pre></p> <p>Create the <code>author</code> group and add the associated metadata: <pre><code>author = h5md.create_group('author')\nauthor.attrs['name'] = 'author name'\nauthor.attrs['email'] = 'author-name@example-domain.edu'\n</code></pre></p> <p>Create the <code>program</code> group and add the associated metadata: <pre><code>program = h5md.create_group('program')\nprogram.attrs['name'] = 'OpenMM'\nprogram.attrs['version'] = '7.7.0'\n</code></pre></p> <p>Create the <code>creator</code> group and add the associated metadata: <pre><code>program = h5md.create_group('creator')\nprogram.attrs['name'] = h5py.__name__\nprogram.attrs['version'] = str(h5py.__version__)\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#particles-group","title":"Particles Group","text":"<p>Create the <code>particles</code> group and the underlying <code>all</code> group to hold the relevant particle data: <pre><code>particles = h5_write.create_group('particles')\nparticles_group_all = particles.create_group('all')\n</code></pre></p> <p>Get the steps, times, positions, and lattice vectors (i.e., box dimensions) from the MDA universe: <pre><code># quantities extracted from MDAnalysis\nsteps = []\ntimes = []\npositions = []\nlattice_vectors = []\nfor i_frame, frame in enumerate(universe.trajectory):\n    times.append(frame.time)\n    steps.append(frame.frame)\n    positions.append(frame.positions)\n    lattice_vectors.append(frame.triclinic_dimensions)\n</code></pre></p> <p>Set the positions and corresponding metadata: <pre><code>position_group_all = particles_group_all.create_group('position')\nposition_group_all['step'] = steps  # shape = (n_frames)\nposition_group_all['time'] = times  # shape = (n_frames)\nposition_group_all['time'].attrs['unit'] = str(time_unit.units)\nposition_group_all['time'].attrs['unit_factor'] = time_unit.magnitude\nposition_group_all['value'] = positions  # shape = (n_frames, n_atoms, dimension)\nposition_group_all['value'].attrs['unit'] = str(length_unit.units)\nposition_group_all['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p> <p>Set the particle-specific metadata: <pre><code>particles_group_all['species_label'] = universe_toponly.atoms.types  # shape = (n_atoms)\nparticles_group_all['force_field_label'] = universe_toponly.atoms.names  # shape = (n_atoms)\nparticles_group_all['mass'] = universe_toponly.atoms.masses  # shape = (n_atoms)\nparticles_group_all['mass'].attrs['unit'] = str(mass_unit.units)\nparticles_group_all['mass'].attrs['unit_factor'] = mass_unit.magnitude\nparticles_group_all['charge'] = universe_toponly.atoms.charges  # shape = (n_atoms)\nparticles_group_all['charge'].attrs['unit'] = str(charge_unit.units)\nparticles_group_all['charge'].attrs['unit_factor'] = charge_unit.magnitude\n</code></pre></p> <p>Create the <code>box</code> group under <code>particles.all</code> and write corresponding data: <pre><code>box_group = particles_group_all.create_group('box')\nbox_group.attrs['dimension'] = dimension\nbox_group.attrs['boundary'] = periodicity\nedges = box_group.create_group('edges')\nedges['step'] = steps\nedges['time'] = times\nedges['time'].attrs['unit'] = str(time_unit.units)\nedges['time'].attrs['unit_factor'] = time_unit.magnitude\nedges['value'] = lattice_vectors\nedges['value'].attrs['unit'] = str(length_unit.units)\nedges['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#connectivity-group","title":"Connectivity Group","text":"<p>Create the <code>connectivity</code> group under <code>root</code> and add the tuples of bonds, angles, and dihedrals: <pre><code>connectivity = h5_write.create_group('connectivity')\nconnectivity['bonds'] = universe_toponly.bonds._bix  # shape = (n_bonds, 2)\nconnectivity['angles'] = universe_toponly.angles._bix  # shape = (n_angles, 3)\nconnectivity['dihedrals'] = universe_toponly.dihedrals._bix  # shape = (n_dihedrals, 4)\nconnectivity['impropers'] = universe_toponly.impropers._bix  # shape = (n_impropers, 4)\n</code></pre> Here <code>n_bonds</code>, <code>n_angles</code>, <code>n_dihedrals</code>, and <code>n_impropers</code> represent the corresponding number of instances of each interaction within the force field.</p> <p>You can read more about the creation of the hierarchical <code>particles_group</code> in Creating a topology.</p>"},{"location":"examples/computational_data/h5md_howto.html#observables-group","title":"Observables Group","text":"<p>For this section, we will consider sets of fabricated observable data for clarity. First, create the <code>observables</code> group under root: <pre><code>observables = h5_write.create_group('observables')\n</code></pre></p> <p>There are 3 types of support observables: <pre><code>types = ['configurational', 'ensemble_average', 'correlation_function']\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#configurational-observables","title":"Configurational Observables","text":"<p>Fabricated data: <pre><code>temperatures = 300. * np.ones(n_frames)\npotential_energies = 1.0 * np.ones(n_frames)\nkinetic_energies = 2.0 * np.ones(n_frames)\n</code></pre></p> <p>Create a <code>temperature</code> group and populate the associated metadata:</p> <pre><code>temperature = observables.create_group('temperature')\ntemperature.attrs['type'] = types[0]\ntemperature['step'] = steps\ntemperature['time'] = times\ntemperature['time'].attrs['unit'] = str(time_unit.units)\ntemperature['time'].attrs['unit_factor'] = time_unit.magnitude\ntemperature['value'] = temperatures\ntemperature['value'].attrs['unit'] = str(temperature_unit.units)\ntemperature['value'].attrs['unit_factor'] = temperature_unit.magnitude\n</code></pre> <p>Create an <code>energy</code> group to hold various types of energies. Add : <pre><code>energies = observables.create_group('energy')\n\npotential_energy = energies.create_group('potential')\npotential_energy.attrs['type'] = types[0]\npotential_energy['step'] = steps\npotential_energy['time'] = times\npotential_energy['time'].attrs['unit'] = str(time_unit.units)\npotential_energy['time'].attrs['unit_factor'] = time_unit.magnitude\npotential_energy['value'] = potential_energies\npotential_energy['value'].attrs['unit'] = str(energy_unit.units)\npotential_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\n\nkinetic_energy = energies.create_group('kinetic')\nkinetic_energy.attrs['type'] = types[0]\nkinetic_energy['step'] = steps\nkinetic_energy['time'] = times\nkinetic_energy['time'].attrs['unit'] = str(time_unit.units)\nkinetic_energy['time'].attrs['unit_factor'] = time_unit.magnitude\nkinetic_energy['value'] = kinetic_energies\nkinetic_energy['value'].attrs['unit'] = str(energy_unit.units)\nkinetic_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#ensemble-average-observables","title":"Ensemble Average Observables","text":"<p>Fabricated data - the following represents radial distribution function (rdf) data calculated between molecule types <code>X</code> and <code>Y</code>, stored in <code>rdf_MOLX-MOLY.xvg</code>: <pre><code>      0.24 0.000152428\n     0.245 0.00457094\n      0.25  0.0573499\n     0.255   0.284764\n      0.26   0.842825\n     0.265    1.64705\n      0.27    2.37243\n     0.275    2.77916\n      0.28    2.80622\n     0.285    2.60082\n      0.29    2.27182\n      ...\n</code></pre></p> <p>Store the rdf data in a dictionary along with some relevant metadata:</p> <pre><code>rdf_XX = np.loadtxt('rdf_MOLX-MOLX.xvg')\nrdf_XY = np.loadtxt('rdf_MOLX-MOLY.xvg')\nrdf_YY = np.loadtxt('rdf_MOLY-MOLY.xvg')\nrdfs = {\n    'MOLX-MOLX': {\n        'n_bins': len(rdf_XX[:, 0]),\n        'bins': rdf_XX[:, 0],\n        'value': rdf_XX[:, 1],\n        'type': 'molecular',\n        'frame_start': 0,\n        'frame_end': n_frames-1\n    },\n    'MOLX-MOLY': {\n        'n_bins': len(rdf_XY[:, 0]),\n        'bins': rdf_XY[:, 0],\n        'value': rdf_XY[:, 1],\n        'type': 'molecular',\n        'frame_start': 0,\n        'frame_end': n_frames-1\n    },\n    'MOLY-MOLY': {\n        'n_bins': len(rdf_YY[:, 0]),\n        'bins': rdf_YY[:, 0],\n        'value': rdf_YY[:, 1],\n        'type': 'molecular',\n        'frame_start': 0,\n        'frame_end': n_frames-1\n    }\n}\n</code></pre> <p>Now create the <code>radial_distribution_functions</code> group under <code>observables</code> and store each imported rdf: <pre><code>radial_distribution_functions = observables.create_group('radial_distribution_functions')\nfor key in rdfs.keys():\n    rdf = radial_distribution_functions.create_group(key)\n    rdf.attrs['type'] = types[1]\n    rdf['type'] = rdfs[key]['type']\n    rdf['n_bins'] = rdfs[key]['n_bins']\n    rdf['bins'] = rdfs[key]['bins']\n    rdf['bins'].attrs['unit'] = str(length_unit.units)\n    rdf['bins'].attrs['unit_factor'] = length_unit.magnitude\n    rdf['value'] = rdfs[key]['value']\n    rdf['frame_start'] = rdfs[key]['frame_start']\n    rdf['frame_end'] = rdfs[key]['frame_end']\n</code></pre></p> <p>We can also store scalar ensemble average observables. Let's consider some fabricated diffusion constant data: <pre><code>Ds = {\n    'MOLX': {\n        'value': 1.0,\n        'error_type': 'Pearson_correlation_coefficient',\n        'errors': 0.98\n    },\n    'MOLY': {\n        'value': 2.0,\n        'error_type': 'Pearson_correlation_coefficient',\n        'errors': 0.95\n    }\n}\n</code></pre></p> <p>Create the <code>diffusion constants</code> group under <code>observables</code> and store the correspond (meta)data:</p> <pre><code>diffusion_constants = observables.create_group('diffusion_constants')\nfor key in Ds.keys():\n    diffusion_constant = diffusion_constants.create_group(key)\n    diffusion_constant.attrs['type'] = types[1]\n    diffusion_constant['value'] = Ds[key]['value']\n    diffusion_constant['value'].attrs['unit'] = str(diff_unit.units)\n    diffusion_constant['value'].attrs['unit_factor'] = diff_unit.magnitude\n    diffusion_constant['error_type'] = Ds[key]['error_type']\n    diffusion_constant['errors'] = Ds[key]['errors']\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html#time-correlation-observables","title":"Time Correlation Observables","text":"<p>Fabricated data - the following represents mean squared displacement (msd) data calculated for molecule type <code>X</code>, stored in <code>msd_MOLX.xvg</code>: <pre><code>         0           0\n         2   0.0688769\n         4    0.135904\n         6    0.203573\n         8    0.271162\n        10    0.339284\n        12    0.410115\n        14    0.477376\n        16    0.545184\n        18     0.61283\n        ...\n</code></pre></p> <p>Store the msd data in a dictionary along with some relevant metadata:</p> <pre><code>msd_X = np.loadtxt('msd_MOLX.xvg')\nmsd_Y = np.loadtxt('msd_MOLY.xvg')\nmsds = {\n    'MOLX': {\n        'n_times': len(msd_X[:, 0]),\n        'times': msd_X[:, 0],\n        'value': msd_X[:, 1],\n        'type': 'molecular',\n        'direction': 'xyz',\n        'error_type': 'standard_deviation',\n        'errors': np.zeros(len(msd_X[:, 0])),\n    },\n    'MOLY': {\n        'n_times': len(msd_Y[:, 0]),\n        'times': msd_Y[:, 0],\n        'value': msd_Y[:, 1],\n        'type': 'molecular',\n        'direction': 'xyz',\n        'error_type': 'standard_deviation',\n        'errors': np.zeros(len(msd_Y[:, 0])),\n    }\n}\n</code></pre> <p>Now create the <code>mean_squared_displacements</code> group under <code>observables</code> and store each imported rdf:</p> <pre><code>mean_squared_displacements = observables.create_group('mean_squared_displacements')\nmsd_unit = length_unit * length_unit\ndiff_unit = msd_unit / time_unit\nfor key in msds.keys():\n    msd = mean_squared_displacements.create_group(key)\n    msd.attrs['type'] = types[2]\n    msd['type'] = msds[key]['type']\n    msd['direction'] = msds[key]['direction']\n    msd['error_type'] = msds[key]['error_type']\n    msd['n_times'] = msds[key]['n_times']\n    msd['times'] = msds[key]['times']\n    msd['times'].attrs['unit'] = str(time_unit.units)\n    msd['times'].attrs['unit_factor'] = time_unit.magnitude\n    msd['value'] = msds[key]['value']\n    msd['value'].attrs['unit'] = str(msd_unit.units)\n    msd['value'].attrs['unit_factor'] = msd_unit.magnitude\n    msd['errors'] = msds[key]['errors']\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html#parameter-group","title":"Parameter Group","text":"<p>Using the json templates for force calculations and molecular dynamics workflows, the (meta)data can be written to the H5MD-NOMAD file using the following code:</p> <p>First, import the data extracted from the JSON templates: <pre><code>with open('force_calculations_metainfo.json') as json_file:\n    force_calculation_parameters = json.load(json_file)\n\nwith open('workflow_metainfo.json') as json_file:\n    workflow_parameters = json.load(json_file)\n</code></pre></p> <p>Then, create the appropriate container groups: <pre><code>parameters = h5_write.create_group('parameters')\nforce_calculations = parameters.create_group('force_calculations')\nworkflow = parameters.create_group('workflow')\n</code></pre></p> <p>Now, recursively write the (meta)data: <pre><code>def get_parameters_recursive(parameter_group, parameter_dict):\n    # Store the parameters from parameter dict into an hdf5 file\n    for key, val in parameter_dict.items():\n        if type(val) == dict:\n            param = val.get('value')\n            if param is not None:\n                parameter_group[key] = param\n                unit = val.get('unit')\n                if unit is not None:\n                    parameter_group[key].attrs['unit'] = unit\n            else:  # This is a subsection\n                subsection = parameter_group.create_group(key)\n                subsection = get_parameters_recursive(subsection, val)\n        else:\n            if val is not None:\n                parameter_group[key] = val\n\n    return parameter_group\n\n\nforce_calculations = get_parameters_recursive(force_calculations, force_calculation_parameters)\nworkflow = get_parameters_recursive(workflow, workflow_parameters)\n</code></pre></p> <p>It's as simple as that! Now, we can upload our H5MD-NOMAD file directly to NOMAD and all the written (meta)data will be stored according to the standard NOMAD schema.</p>"},{"location":"examples/computational_data/h5md_howto.html#accessing-an-h5md-nomad-file","title":"Accessing an H5MD-NOMAD file","text":"<p>The following functions are useful for accessing data from your H5MD-NOMAD file: <pre><code>def apply_unit(quantity, unit, unit_factor):\n    from pint import UnitRegistry\n    ureg = UnitRegistry()\n\n    if quantity is None:\n        return\n    if unit:\n        unit = ureg(unit)\n        unit *= unit_factor\n        quantity *= unit\n\n    return quantity\n\ndef decode_hdf5_bytes(dataset):\n    if dataset is None:\n        return\n    elif type(dataset).__name__ == 'ndarray':\n        if dataset == []:\n            return dataset\n        dataset = np.array([val.decode(\"utf-8\") for val in dataset]) if type(dataset[0]) == bytes else dataset\n    else:\n        dataset = dataset.decode(\"utf-8\") if type(dataset) == bytes else dataset\n    return dataset\n\ndef hdf5_attr_getter(source, path, attribute, default=None):\n    '''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\n    section_segments = path.split('.')\n    for section in section_segments:\n        try:\n            value = source.get(section)\n            source = value[-1] if isinstance(value, list) else value\n        except Exception:\n            return\n    value = source.attrs.get(attribute)\n    source = value[-1] if isinstance(value, list) else value\n    source = decode_hdf5_bytes(source) if source is not None else default\n    return source\n\ndef hdf5_getter(source, path, default=None):\n    '''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\n    section_segments = path.split('.')\n    for section in section_segments:\n        try:\n            value = source.get(section)\n            unit = hdf5_attr_getter(source, section, 'unit')\n            unit_factor = hdf5_attr_getter(source, section, 'unit_factor', default=1.0)\n            source = value[-1] if isinstance(value, list) else value\n        except Exception:\n            return\n\n    if source is None:\n        source = default\n    elif type(source) == h5py.Dataset:\n        source = source[()]\n        source = apply_unit(source, unit, unit_factor)\n\n    return decode_hdf5_bytes(source)\n</code></pre></p> <p>Open your H5MD-NOMAD file with <code>h5py</code>: <pre><code>import h5py\n\nh5_read = h5py.File('test_h5md-nomad.h5', 'r')\n</code></pre></p> <p>Access a particular data set: <pre><code>potential_energies = h5_read['observables']['energy']['potential']['value']\nprint(potential_energies[()])\n</code></pre> result: <pre><code>array([1., 1., 1., 1., 1.])\n</code></pre></p> <p>Get the unit information for this quantity: <pre><code>unit = potential_energies.attrs['unit']\nunit_factor = potential_energies.attrs['unit_factor']\n\nprint(unit)\nprint(unit_factor)\n</code></pre></p> <p>results: <pre><code>joule\n1000.0\n</code></pre></p> <p>Alternatively, the above functions will return the dataset as python arrays, i.e., already applying <code>[()]</code> to the HDF5 element, and also apply the appropriate units where applicable: <pre><code>potential_energies = hdf5_getter(h5_read, 'observables.energy.potential.value')\nprint(potential_energies)\n</code></pre></p> <p>result: <pre><code>Magnitude\n[1000.0 1000.0 1000.0 1000.0 1000.0]\nUnits   joule\n</code></pre></p>"},{"location":"examples/computational_data/h5md_howto.html#creating-a-topology-particles_group","title":"Creating a topology (<code>particles_group</code>)","text":"<p>This page demonstrates how to create a \"standard\" topology in H5MD-NOMAD. The demonstrated organization of molecules and monomers is identical to what other NOMAD parsers do to create a topology from native simulation files (e.g., outputs from GROMACS or LAMMPS). However, the user is free to deviate from this standard to create arbitrary organizations of particles, as described in Connectivity.</p>"},{"location":"examples/computational_data/h5md_howto.html#standard-topology-structure-for-bonded-force-fields","title":"Standard topology structure for bonded force fields","text":"<p><pre><code>topology\n\u251c\u2500\u2500 molecule_group_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502       \u251c\u2500\u2500 monomer_1\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_1\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 monomer_2\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_2\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 molecule_group_2\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> Here, the first level of organization is the \"molecule group\". Molecule groups contain molecules of the same type. In other words, <code>molecule_group_1</code> and <code>molecule_group_2</code> represent distinct molecule types. At the next level of the hierarchy, each molecule within this group is stored (i.e., <code>molecule_1</code>, <code>molecule_2</code>, etc.). In the above example, <code>molecule_group_1</code> represents a polymer (or protein). Thus, below the molecule level, there is a \"monomer group level\". Similar to the molecule group, the monomer group organizes all monomers (of the parent molecule) that are of the same type. Thus, for <code>molecule_1</code> of <code>molecule_group_1</code>, <code>monomer_group_1</code> and <code>monomer_group_2</code> represent distinct types of monomers existing within the polymer. Then, below <code>monomer_group_1</code>, each monomer within this group is stored. Finally, beneath these individual monomers, only the metadata for that monomer is stored (i.e., no further organization levels). Note however, that metadata can be (and is) stored at each level of the hierarchy, but is left out of the illustration for clarity. Notice also that <code>molecule_group_2</code> is not a polymer. Thus, each molecule within this group stores only the corresponding metadata, and no further levels of organization.</p>"},{"location":"examples/computational_data/h5md_howto.html#creating-the-standard-hierarchy-from-an-mdanalysis-universe","title":"Creating the standard hierarchy from an MDAnalysis universe","text":"<p>We start from the perspective of the Writing an HDF5 file according to H5MD-NOMAD with python section, with identical imports and assuming that an MDAnalysis <code>universe</code> is already instantiated from the raw simulation files. As in the previous example, the <code>universe</code> containing the topology information is called <code>universe_topology</code>.</p> <p>The following functions will be useful for creating the topology:</p> <pre><code>def get_composition(children_names):\n    '''\n    Given a list of children, return a compositional formula as a function of\n    these children. The format is &lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...\n    '''\n    children_count_tup = np.unique(children_names, return_counts=True)\n    formula = ''.join([f'{name}({count})' for name, count in zip(*children_count_tup)])\n    return formula\n\n\ndef get_molecules_from_bond_list(n_particles: int, bond_list: List[int], particle_types: List[str] = None, particles_typeid=None):\n    '''\n    Returns a dictionary with molecule info from the list of bonds\n    '''\n\n    import networkx\n\n    system_graph = networkx.empty_graph(n_particles)\n    system_graph.add_edges_from([(i[0], i[1]) for i in bond_list])\n    molecules = [system_graph.subgraph(c).copy() for c in networkx.connected_components(system_graph)]\n    mol_dict = []\n    for i_mol, mol in enumerate(molecules):\n        mol_dict.append({})\n        mol_dict[i_mol]['indices'] = np.array(mol.nodes())\n        mol_dict[i_mol]['bonds'] = np.array(mol.edges())\n        mol_dict[i_mol]['type'] = 'molecule'\n        mol_dict[i_mol]['is_molecule'] = True\n        if particles_typeid is None and len(particle_types) == n_particles:\n            mol_dict[i_mol]['names'] = [particle_types[int(x)] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\n        if particle_types is not None and particles_typeid is not None:\n            mol_dict[i_mol]['names'] = [particle_types[particles_typeid[int(x)]] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\n        mol_dict[i_mol]['formula'] = get_composition(mol_dict[i_mol]['names'])\n\n    return mol_dict\n\n\ndef is_same_molecule(mol_1: dict, mol_2: dict):\n    '''\n    Checks whether the 2 input molecule dictionaries represent the same\n    molecule type, i.e., same particle types and corresponding bond connections.\n    '''\n\n    if sorted(mol_1['names']) == sorted(mol_2['names']):\n        mol_1_shift = np.min(mol_1['indices'])\n        mol_2_shift = np.min(mol_2['indices'])\n        mol_1_bonds_shift = mol_1['bonds'] - mol_1_shift\n        mol_2_bonds_shift = mol_2['bonds'] - mol_2_shift\n\n        bond_list_1 = [sorted((mol_1['names'][i], mol_1['names'][j])) for i, j in mol_1_bonds_shift]\n        bond_list_2 = [sorted((mol_2['names'][i], mol_2['names'][j])) for i, j in mol_2_bonds_shift]\n\n        bond_list_names_1, bond_list_counts_1 = np.unique(bond_list_1, axis=0, return_counts=True)\n        bond_list_names_2, bond_list_counts_2 = np.unique(bond_list_2, axis=0, return_counts=True)\n\n        bond_list_dict_1 = {bond[0] + '-' + bond[1]: bond_list_counts_1[i_bond] for i_bond, bond in enumerate(bond_list_names_1)}\n        bond_list_dict_2 = {bond[0] + '-' + bond[1]: bond_list_counts_2[i_bond] for i_bond, bond in enumerate(bond_list_names_2)}\n        if bond_list_dict_1 == bond_list_dict_2:\n            return True\n\n        return False\n\n    return False\n</code></pre> <p>Then, we can create the topology structure from the MDAnalysis universe:</p> <pre><code>bond_list = universe_toponly.bonds._bix\nmolecules = get_molecules_from_bond_list(n_atoms, bond_list, particle_types=universe_toponly.atoms.types, particles_typeid=None)\n\n# create the topology\nmol_groups = []\nmol_groups.append({})\nmol_groups[0]['molecules'] = []\nmol_groups[0]['molecules'].append(molecules[0])\nmol_groups[0]['type'] = 'molecule_group'\nmol_groups[0]['is_molecule'] = False\nfor mol in molecules[1:]:\n    flag_mol_group_exists = False\n    for i_mol_group in range(len(mol_groups)):\n        if is_same_molecule(mol, mol_groups[i_mol_group]['molecules'][0]):\n            mol_groups[i_mol_group]['molecules'].append(mol)\n            flag_mol_group_exists = True\n            break\n    if not flag_mol_group_exists:\n        mol_groups.append({})\n        mol_groups[-1]['molecules'] = []\n        mol_groups[-1]['molecules'].append(mol)\n        mol_groups[-1]['type'] = 'molecule_group'\n        mol_groups[-1]['is_molecule'] = False\n\n\nfor i_mol_group, mol_group in enumerate(mol_groups):\n    mol_groups[i_mol_group]['formula'] = molecule_labels[i_mol_group] + '(' + str(len(mol_group['molecules'])) + ')'\n    mol_groups[i_mol_group]['label'] = 'group_' + str(molecule_labels[i_mol_group])\n    mol_group_indices = []\n    for i_molecule, molecule in enumerate(mol_group['molecules']):\n        molecule['label'] = molecule_labels[i_mol_group]\n        mol_indices = molecule['indices']\n        mol_group_indices.append(mol_indices)\n        mol_resids = np.unique(universe_toponly.atoms.resindices[mol_indices])\n        if mol_resids.shape[0] == 1:\n            continue\n\n        res_dict = []\n        for i_resid, resid in enumerate(mol_resids):\n            res_dict.append({})\n            res_dict[i_resid]['indices'] = np.where( universe_toponly.atoms.resindices[mol_indices] == resid)[0]\n            res_dict[i_resid]['label'] = universe_toponly.atoms.resnames[res_dict[i_resid]['indices'][0]]\n            res_dict[i_resid]['formula'] = get_composition(universe_toponly.atoms.names[res_dict[i_resid]['indices']])\n            res_dict[i_resid]['is_molecule'] = False\n            res_dict[i_resid]['type'] = 'monomer'\n\n        res_groups = []\n        res_groups.append({})\n        res_groups[0]['residues'] = []\n        res_groups[0]['residues'].append(res_dict[0])\n        res_groups[0]['label'] = 'group_' + res_dict[0]['label']\n        res_groups[0]['type'] = 'monomer_group'\n        res_groups[0]['is_molecule'] = False\n        for res in res_dict[1:]:\n            flag_res_group_exists = False\n            for i_res_group in range(len(res_groups)):\n                if res['label'] == res_groups[i_res_group]['label']:\n                    res_groups[i_res_group]['residues'].append(res)\n                    flag_res_group_exists = True\n                    break\n            if not flag_res_group_exists:\n                res_groups.append({})\n                res_groups[-1]['residues'] = []\n                res_groups[-1]['residues'].append(res)\n                res_groups[-1]['label'] = 'group_' + res['label']\n                res_groups[-1]['formula'] = get_composition(universe_toponly.atoms.names[res['indices']])\n                res_groups[-1]['type'] = 'monomer_group'\n                res_groups[-1]['is_molecule'] = False\n\n        molecule['formula'] = ''\n        for res_group in res_groups:\n            res_group['formula'] = res_group['residues'][0]['label'] + '(' + str(len(res_group['residues'])) + ')'\n            molecule['formula'] += res_group['formula']\n            res_group_indices = []\n            for res in res_group['residues']:\n                res_group_indices.append(res['indices'])\n            res_group['indices'] = np.concatenate(res_group_indices)\n\n        mol_group['indices'] = np.concatenate(mol_group_indices)\n\n        molecule['residue_groups'] = res_groups\n</code></pre>"},{"location":"examples/computational_data/h5md_howto.html#writing-the-topology-to-an-h5md-nomad-file","title":"Writing the topology to an H5MD-NOMAD file","text":"<p>Here we assume an H5MD-NOMAD file has already been created, as demonstrated on the Writing an HDF5 file according to H5MD-NOMAD with python section, and that the <code>connectivity</code> group was created under the root level.</p> <p>Now, create the <code>particles_group</code> group under <code>connectivity</code> within our HDF5-NOMAD file: <pre><code>topology_keys = ['type', 'formula', 'particles_group', 'label', 'is_molecule', 'indices']\ncustom_keys = ['molecules', 'residue_groups', 'residues']\n\ntopology = connectivity.create_group('particles_group')\n\nfor i_mol_group, mol_group in enumerate(mol_groups):\n    hdf5_mol_group = topology.create_group('group_' + molecule_labels[i_mol_group])\n    for mol_group_key in mol_group.keys():\n        if mol_group_key not in topology_keys + custom_keys:\n            continue\n        if mol_group_key != 'molecules':\n            hdf5_mol_group[mol_group_key] = mol_group[mol_group_key]\n        else:\n            hdf5_molecules = hdf5_mol_group.create_group('particles_group')\n            for i_molecule, molecule in enumerate(mol_group[mol_group_key]):\n                hdf5_mol = hdf5_molecules.create_group('molecule_' + str(i_molecule))\n                for mol_key in molecule.keys():\n                    if mol_key not in topology_keys + custom_keys:\n                        continue\n                    if mol_key != 'residue_groups':\n                        hdf5_mol[mol_key] = molecule[mol_key]\n                    else:\n                        hdf5_residue_groups = hdf5_mol.create_group('particles_group')\n                        for i_res_group, res_group in enumerate(molecule[mol_key]):\n                            hdf5_res_group = hdf5_residue_groups.create_group('residue_group_' + str(i_res_group))\n                            for res_group_key in res_group.keys():\n                                if res_group_key not in topology_keys + custom_keys:\n                                    continue\n                                if res_group_key != 'residues':\n                                    hdf5_res_group[res_group_key] = res_group[res_group_key]\n                                else:\n                                    hdf5_residues = hdf5_res_group.create_group('particles_group')\n                                    for i_res, res in enumerate(res_group[res_group_key]):\n                                        hdf5_res = hdf5_residues.create_group('residue_' + str(i_res))\n                                        for res_key in res.keys():\n                                            if res_key not in topology_keys:\n                                                continue\n                                            if res[res_key] is not None:\n                                                hdf5_res[res_key] = res[res_key]\n</code></pre></p>"},{"location":"examples/computational_data/h5md_ref.html","title":"H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD","text":""},{"location":"examples/computational_data/h5md_ref.html#notable-changes-from-h5md-to-h5md-nomad","title":"Notable changes from H5MD to H5MD-NOMAD","text":"<p>In order to effectively parse and normalize the molecular simulation data, the H5MD-NOMAD schema extends the original H5MD framework while also enforces various restrictions to the schema. This section contains a list of such additions and restrictions. Here we distinguish between \"unused\" features, i.e., metadata that will be ignored by NOMAD and \"unsupported\" features, i.e., structures that will likely cause an error if used within an H5MD-NOMAD file for upload to NOMAD.</p>"},{"location":"examples/computational_data/h5md_ref.html#new-or-amended-features","title":"New or amended features","text":"<ul> <li> <p>additional standardized particles group elements</p> </li> <li> <p>boundary attribute changed to boolean datatype</p> </li> <li> <p>treatment of units</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#unused-features","title":"Unused features","text":"<ul> <li> <p>modules in h5md metadata</p> </li> <li> <p>arbitrary particle groups not parsed, group labeled <code>all</code> required</p> </li> <li> <p>image, species, and id elements of particles group</p> </li> <li> <p>non-standard elements in particles group</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#unsupported-features","title":"Unsupported features","text":"<ul> <li> <p>fixed step and time storage</p> </li> <li> <p>time-dependent particle lists</p> </li> <li> <p>time-dependent model labels for particles</p> </li> <li> <p>only partial support for grouping of observables by particle subgroups</p> </li> <li> <p>time-dependent connectivity elements</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#standardized-observables-in-h5md-nomad","title":"Standardized observables in H5MD-NOMAD","text":""},{"location":"examples/computational_data/h5md_ref.html#configurational","title":"configurational","text":"<ul> <li> <p><code>energy quantities</code> :</p> </li> <li> <p><code>radius_of_gyration</code> :</p> </li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#ensemble-average","title":"ensemble average","text":"<ul> <li><code>radial_distribution_function</code> :</li> </ul>"},{"location":"examples/computational_data/h5md_ref.html#time-correlation","title":"time correlation","text":"<ul> <li><code>mean_squared_displacement</code> :</li> </ul>"},{"location":"examples/computational_data/metainfo.html","title":"Guide to Computational MetaInfo","text":""},{"location":"examples/computational_data/metainfo.html#overview-of-metadata-organization-for-computation","title":"Overview of metadata organization for computation","text":"<p>NOMAD stores all processed data in a well defined, structured, and machine readable format, known as the <code>archive</code>. The schema that defines the organization of (meta)data within the archive is known as the MetaInfo. See Explanation &gt; Data structure for general information about data structures and schemas in NOMAD.</p> <p>The following diagram is an overarching visualization of the most important archive sections for computational data:</p> <pre><code>archive\n\u251c\u2500\u2500 run\n\u2502  \u00a0 \u251c\u2500\u2500 method\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atom_parameters\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 dft\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 forcefield\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 system\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atoms\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 positions\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 lattice_vectors\n\u2502  \u00a0 \u2502      \u2502     \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 calculation\n\u2502  \u00a0        \u251c\u2500\u2500 energy\n\u2502  \u00a0        \u251c\u2500\u2500 forces\n\u2502  \u00a0        \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 workflow2\n \u00a0\u00a0  \u251c\u2500\u2500 method\n \u00a0\u00a0  \u251c\u2500\u2500 inputs\n \u00a0\u00a0  \u251c\u2500\u2500 tasks\n \u00a0\u00a0  \u251c\u2500\u2500 outputs\n \u00a0\u00a0  \u2514\u2500\u2500 results\n</code></pre> <p>The most important section of the archive for computational data is the <code>run</code> section, which is divided into three main subsections: <code>method</code>, <code>system</code>, and <code>calculation</code>. <code>method</code> stores information about the computational model used to perform the calculation. <code>system</code> stores attributes of the atoms involved in the calculation, e.g., atom types, positions, lattice vectors, etc. <code>calculation</code> stores the output of the calculation, e.g., energy, forces, etc.</p> <p>The <code>workflow</code> section of the archive then stores information about the series of tasks performed to accumulate the (meta)data in the run section. The relevant input parameters for the workflow are stored in <code>method</code>, while the <code>results</code> section stores output from the workflow beyond observables of single configurations. For example, any ensemble-averaged quantity from a molecular dynamics simulation would be stored under <code>workflow/results</code>. Then, the <code>inputs</code>, <code>outputs</code>, and <code>tasks</code> sections define the specifics of the workflow. For some standard workflows, e.g., geometry optimization and molecular dynamics, the NOMAD normalizers For non-standard workflows, the parser (or more appropriately the corresponding normalizer) must populate these sections accordingly. See Standard and Custom Computational Workflows in NOMAD for more information about the structure of the workflow section, as well as instructions on how to upload custom workflows to link individual Entries in NOMAD.</p> <p>Attention</p> <p>We are currently performing a complete refactoring of the computational MetaInfo schema. Details and updates about this task, and how it may benefit your future usage of NOMAD, will be added below.</p>"},{"location":"examples/computational_data/parser_plugins.html","title":"Guide to computational parser plugins","text":"<p>NOMAD uses parsers to convert raw data (for example, output from computational software, instruments, or electronic lab notebooks) into NOMAD's common Archive format. This page provides a guide to the existing standard computational parsers in NOMAD.</p>"},{"location":"examples/computational_data/parser_plugins.html#parser-organization","title":"Parser organization","text":"<p>Note</p> <p>The majority of NOMAD's computational parsers do not yet exist as plugins. Instead, they are linked to the nomad-lab software as submodules. We will be migrating these parsers to proper plugins in the near future, and may reorganize the projects described below. We will update this page accordingly.</p> <p>The NOMAD computational parsers can be found within your local NOMAD distribution under <code>&lt;path_to_nomad-lab&gt;/dependencies/parsers/&lt;parserproject&gt;</code>, where <code>&lt;parser_project&gt;</code> corresponds to the following organization:</p> <ul> <li>atomistic - Parsers for output from classical molecular simulations, e.g., from Gromacs, Lammps, etc.</li> <li>database - Parsers for various databases, e.g., OpenKim.</li> <li>eelsdb - Parser for the EELS database (https://eelsdb.eu/; to be integrated in the database project).</li> <li>electronic - Parsers for output from electronic structure calculations, e.g., from Vasp, Fhiaims, etc. </li> <li>nexus - Parsers for combining various instrument output formats and electronic lab notebooks.</li> <li>workflow - Parsers for output from task managers and workflow schedulers.</li> </ul> <p>You can also examine the source code of the parsers by following the above links to the corresponding GitHub repository for each project. Within each project folder you will find a <code>test/</code> directory, containing the parser tests, and also a directory containing the parsers' source code, <code>&lt;parserproject&gt;parser</code> or <code>&lt;parserproject&gt;parsers</code>, depending on if one or more parsers are contained within the project, respectively. In the case of multiple parsers, the files for individual parsers are contained within a corresponding subdirectory: <code>&lt;parserproject&gt;parsers/&lt;parsername&gt;</code> For example, the Quantum Espresso parser files are found in <code>dependencies/parsers/electronic/electronicparsers/quantumespresso/</code>.</p>"},{"location":"examples/computational_data/parser_plugins.html#developing-your-own-parser-plugin","title":"Developing your own parser plugin","text":""},{"location":"examples/computational_data/parser_plugins.html#prerequisites","title":"Prerequisites","text":"<p>The general docs contain information about the nuts and bolts of developing a plugin:</p> <ul> <li> <p>How to write a plugin: Some basic information about different types of plugins, plugin anatomy, and creating a plugin project.</p> </li> <li> <p>How to write a parser: The basics of how NOMAD parsers work, how to configure the files that your parser will match, and how to utilize existing parser classes.</p> </li> </ul> <p>Attention</p> <p>This page is under construction as we convert NOMAD's standard computational parsers to parser plugins. Along the way, we will add content below to guide you in the development of your own computational parser plugins.</p>"},{"location":"examples/computational_data/schema_plugins.html","title":"Guide to computational schema plugins","text":"<p>NOMAD uses Schemas to define the data structures and organization of Processed Data. Schemas can be defined in yaml or python formats. How to write a schema describes the basics of writing a schema, in the yaml format. Computational schemas in NOMAD have historically been written in python. There are several existing computational schema plugin projects for reference:</p> <ul> <li>nomad-schema-plugin-run: contains schemas for standard processed computational data, stored in the <code>run</code> section within the NOMAD archive.</li> </ul> <ul> <li> <p>nomad-schema-plugin-simulation-data: contains schemas for standard processed computational data, stored in the <code>data</code> section within the NOMAD archive.</p> </li> <li> <p>nomad-schema-plugin-simulation-workflow: contains schemas for standard computational workflows defined in NOMAD.</p> </li> <li> <p>nomad-normalizer-plugin-simulation-workflow: contains schemas for standard computational \"normalized\" data.</p> </li> <li> <p>nomad-schema-plugin-example: contains an example for NOMAD schema plugins. It should be forked to create actual plugins.</p> </li> </ul> <p>Guide to Computational MetaInfo describes how these schemas are used to organize standard computational data within an Entry in the NOMAD repository.</p> <p>Attention</p> <p>This page is under construction. We will be adding content below to guide you in the development of your own computational schema plugins.</p>"},{"location":"examples/computational_data/uploading.html","title":"Quick Start: Uploading computational data in NOMAD","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p> <p>This page provides an overview of NOMAD's usage with computational data. If you are completely new to NOMAD, we recommend to first read through the Navigating to NOMAD, Uploading and publishing data, and Exploring data tutorials.</p> <p>Uploading data in NOMAD can be done in several ways:</p> <ul> <li>By dragging-and-dropping your files into the <code>PUBLISH &gt; Uploads</code> page: suitable for users who have a relatively small amount of data.</li> <li>By using the Python-based NOMAD API: suitable for users who have larger datasets and need to automatize the upload.</li> <li>By using the shell command <code>curl</code> for sending files to the upload: suitable for users who have larger datasets and need to automatize the upload.</li> </ul> <p>You can upload the files one by one or you can zip them in <code>.zip</code> or <code>.tar.gz</code> formats to upload a larger amount of files at once.</p>"},{"location":"examples/computational_data/uploading.html#drag-and-drop-uploads","title":"Drag-and-drop uploads","text":"<p>On the top-left menu, click on <code>PUBLISH &gt; Uploads</code>.</p> <p></p> <p>You can then click on <code>CREATE A NEW UPLOAD</code> or try one of the example uploads by clicking in <code>ADD EXAMPLE UPLOADS</code> and selecting one of the multiple options, including data from an ELN, various instruments, or computational software. For a clear demonstration of the entire process, we will use the following example data:</p>  [Download Example Data](data/Si_gw.zip){:target=\"_blank\" .md-button .nomad-button}  <p>This particular example represents a computational workflow to investigate some properties of Si~2~, however the details are not important for our demonstration here.</p> <p>After downloading the example <code>.zip</code> file, you can drag-and-drop it or click on the <code>CLICK OR DROP FILES</code> button to browse through your local directories.</p> <p></p> <p>After the files are uploaded, a processing is triggered. This generally includes an automatic identification of the uploaded files that are supported in NOMAD, and then a corresponding processing to harvest all the relevant (meta)data. The precise details of the processing depend on each use-case. For example, you can find out more about the processing of computational data in Processing of computational data.</p> <p>You will receive an email when the upload processing is finished.</p>"},{"location":"examples/computational_data/uploading.html#sections-of-the-uploads-page","title":"Sections of the Uploads page","text":"<p>At the top of the uploads page, you can modify certain general metadata fields.</p> <p></p> <p>The name of the upload can be modify by clicking on the pen icon . The other icons correspond to:</p> <ul> <li> Manage members: allows users to invite collaborators by defining co-authors and reviewers roles.</li> <li> Download files: downloads all files present in the upload.</li> <li> Reload: reloads the uploads page.</li> <li> Reprocess: triggers again the processing of the uploaded data.</li> <li> API: generates a JSON response to use by the NOMAD API.</li> </ul> <ul> <li> Delete the upload: deletes completely the upload.</li> </ul> <p>The remainder of the uploads page is divided in 4 sections.</p>"},{"location":"examples/computational_data/uploading.html#prepare-and-upload-your-files","title":"Prepare and upload your files","text":"<p>This section shows the files and folder structure in the upload. You can add a <code>README.md</code> in the root directory and its content will be shown above this section.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#process-data","title":"Process data","text":"<p>This section shows the processed data and the generated entries in NOMAD.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#edit-author-metadata","title":"Edit author metadata","text":"<p>This section allows users to edit certain metadata fields from all entries recognized in the upload. This includes comments, where you can add as much extra information as you want, references, where you can add a URL to your upload (e.g., an article DOI), and datasets, where you can create or add the uploaded data into a more general dataset (see Organizing data in datasets).</p>  ![Edit author metadata](images/edit_author_metadata.png){.screenshot style=\"max-width:300px !important;\"}"},{"location":"examples/computational_data/uploading.html#publish","title":"Publish","text":"<p>This section lets the user to publish the data with or without an embargo.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#publishing","title":"Publishing","text":"<p>After uploading and a successful parsing, congratulations! Now you can publish your data and let other users browse through it and re-use it for other purposes.</p> <p></p> <p>You can define a specific <code>Embargo period</code> of up to 36 months, after which the data will be made publicly available under the CC BY 4.0 license.</p> <p>After publishing by clicking on <code>PUBLISH</code>, the uploaded files cannot be altered. However, you can still edit the metadata fields.</p>"},{"location":"examples/computational_data/uploading.html#organizing-data-in-datasets","title":"Organizing data in datasets","text":"<p>You can organize your uploads and individual entries by grouping them into common datasets.</p> <p>In the uploads page, click on <code>EDIT AUTHOR METADATA OF ALL ENTRIES</code>.</p> <p>Under <code>Datasets</code> you can either <code>Create a new dataset</code> or <code>Search for an existing dataset</code>. After selecting the dataset, click on <code>SUBMIT</code>.</p> <p>Now, the defined dataset will be defined under <code>PUBLISH &gt; Datasets</code>.</p> <p></p> <p>The icon  allows you to assign a DOI to a specific dataset. Once a DOI has been assign to a dataset, no more data can be added to it. This can then be added into your publication so that it can be used as a reference, e.g., see the Data availability statement in M. Kuban et al., Similarity of materials and data-quality assessment by fingerprinting, MRS Bulletin 47, 991-999 (2022).</p>"},{"location":"examples/computational_data/uploading.html#processing-of-computational-data","title":"Processing of computational data","text":"<p>See From files to data and Processing for full explanations about data processing in NOMAD.</p> <p>When data is uploaded to NOMAD, the software interprets the files and determines which of them is a mainfile. Any other files in the upload can be viewed as auxiliary files. In the same upload, there might be multiple mainfiles and auxiliary files organized in a folder tree structure.</p> <p>The mainfiles are the main output file of a calculation. The presence of a mainfile in the upload is key for NOMAD to recognize a calculation. In NOMAD, we support an array computational codes for first principles calculations, molecular dynamics simulations, and lattice modeling, as well as workflow and database managers. For each code, NOMAD recognizes a single file as the mainfile. For example, the VASP mainfile is by default the <code>vasprun.xml</code>, although if the <code>vasprun.xml</code> is not present in the upload NOMAD searches the <code>OUTCAR</code> file and assigns it as the mainfile (see VASP POTCAR stripping).</p> <p>The rest of files which are not the mainfile are auxiliary files. These can have several purposes and be supported and recognized by NOMAD in the parser. For example, the <code>band*.out</code> or <code>GW_band*</code> files in FHI-aims are auxiliary files that allows the NOMAD FHI-aims parser to recognize band structures in DFT and GW, respectively.</p> <p>You can see the full list of supported codes, mainfiles, and auxiliary files in the general NOMAD documentation under Supported parsers.</p> <p>We recommend that the user keeps the folder structure and files generated by the simulation code, but without reaching the uploads limits. Please, also check our recommendations on Best Practices: preparing the data and folder structure.</p>"},{"location":"examples/computational_data/uploading.html#structured-data-with-the-nomad-metainfo","title":"Structured data with the NOMAD metainfo","text":"<p>Once the mainfile has been recognized, a new entry in NOMAD is created and a specific parser is called. The auxliary files are searched by and accessed within the parser.</p> <p>For this new entry, NOMAD generates a NOMAD archive. It will contain all the (meta)information extracted from the unstructured raw data files but in a structured, well defined, and machine readable format. This metadata provides context to the raw data, i.e., what were the input methodological parameters, on which material the calculation was performed, etc. We define the NOMAD Metainfo as all the set of sections, subsections, and quantities used to structure the raw data into a structured schema. Further information about the NOMAD Metainfo is available in the general NOMAD documentation page in Learn &gt; Structured data.</p> <p></p>"},{"location":"examples/computational_data/uploading.html#nomad-sections-for-computational-data","title":"NOMAD sections for computational data","text":"<p>Under the <code>Entry</code> / <code>archive</code> section, there are several sections and quantities being populated by the parsers. For computational data, only the following sections are populated:</p> <ul> <li><code>metadata</code>: contains general and non-code specific metadata. This is mainly information about authors, creation of the entry time, identifiers (id), etc.</li> <li><code>run</code>: contains the parsed and normalized raw data into the structured NOMAD schema. This is all the possible raw data which can be translated into a structured way.</li> <li><code>workflow2</code>: contains metadata about the specific workflow performed within the entry. This is mainly a set of well-defined workflows, e.g., <code>GeometryOptimization</code>, and their parameters.</li> <li><code>results</code>: contains the normalized and search indexed metadata. This is mainly relevant for searching, filtering, and visualizing data in NOMAD.</li> </ul> <code>workflow</code> and <code>workflow2</code> sections: development and refactoring <p>You have probably noticed the name <code>workflow2</code> but also the existence of a section called <code>workflow</code> under <code>archive</code>. This is because <code>workflow</code> is an old version of the workflow section, while <code>workflow2</code> is the new version. Sometimes, certain sections suffer a rebranding or refactoring, in most cases to add new features or to polish them after we receive years of feedback. In this case, the <code>workflow</code> section will remain until all older entries containing such section are reprocessed to transfer this information into <code>workflow2</code>.</p>"},{"location":"examples/computational_data/uploading.html#parsing","title":"Parsing","text":"<p>A parser is a Python module which reads the code-specific mainfile and auxiliary files and populates the <code>run</code> and <code>workflow2</code> sections of the <code>archive</code>, along with all relevant subsections and quantities.</p> <p>Parsers are added to NOMAD as plugins and are divided in a set of Github sub-projects under the main NOMAD repository.</p>"},{"location":"examples/computational_data/uploading.html#normalizing","title":"Normalizing","text":"<p>After the parsing populates the <code>run</code> and <code>workflow2</code> sections, an extra layer of Python modules is executed on top of the processed NOMAD metadata. This has two main purposes: 1. normalize or homogenize certain metadata parsed from different codes, and 2. populate the <code>results</code> section. For example, this is the case of normalizing the density of states (DOS) to its size intensive value, independently of the code used to calculate the DOS. The set of normalizers relevant for computational data are listed in <code>/nomad/config/models.py</code> and are executed in the specific order defined there. Their roles are explained more in detail in Processing.</p>"},{"location":"examples/computational_data/uploading.html#search-indexing-and-storing","title":"Search indexing (and storing)","text":"<p>The last step is to store the structured metadata and pass some of it to the search index. The metadata which is passed to the search index is defined in the <code>results</code> section. These metadata can then be searched by filtering in the Entries page of NOMAD or by writing a Python script which searches using the NOMAD API.</p>"},{"location":"examples/computational_data/uploading.html#entries-overview-page","title":"Entries OVERVIEW page","text":"<p>Once the parsers and normalizers finish, the Uploads page will show if the processing of the entry was a <code>SUCCESS</code> or a <code>FAILURE</code>. The entry information can be browsed by clicking on the  icon.</p> <p>You will land on the <code>OVERVIEW</code> page of the entry. On the top menu you can further select the <code>FILES</code> page, the <code>DATA</code> page, and the <code>LOGS</code> page.</p> <p></p> <p>The overview page contains a summary of the parsed metadata, e.g., tabular information about the material and methodology of the calculation (in the example, a G0W0 calculation done with the code exciting for bulk Si<sub>2</sub>), and visualizations of the system and some relevant properties. We note that all metadata are read directly from <code>results</code>.</p>"},{"location":"examples/computational_data/uploading.html#logs-page","title":"LOGS page","text":"<p>In the <code>LOGS</code> page, you can find information about the processing. You can read error, warning, and critical messages which can provide insight if the processing of an entry was a <code>FAILURE</code>.</p> <p></p> <p>We recommend you to Get support or contact our team in case you find <code>FAILURE</code> situations. These might be due to bugs which we are rapidly fixing, and whose origin might be varied: from a new version of a code which is not yet supported to wrong handling of potential errors in the parser script. It may also be a problem with the organization of the data in the folders. In order to minimize these situations, we suggest that you read Best Practices: preparing the data and folder structure.</p>"},{"location":"examples/computational_data/uploading.html#data-page","title":"DATA page","text":"<p>The <code>DATA</code> page contains all the structured NOMAD metainfo populated by the parser and normalizers. This is the most important page in the entry, as it contains all the relevant metadata which will allow users to find that specific simulation.</p> <p></p> <p>Furthermore, you can click on the  icon to download the NOMAD <code>archive</code> in a JSON format.</p>"},{"location":"examples/computational_data/uploading.html#best-practices-preparing-the-data-and-folder-structure","title":"Best Practices: preparing the data and folder structure","text":"<p>Attention</p> <p>Under construction.</p>"},{"location":"examples/computational_data/uploading.html#vasp-potcar-stripping","title":"VASP POTCAR stripping","text":"<p>For VASP data, NOMAD complies with the licensing of the <code>POTCAR</code> files. In agreement with Georg Kresse, NOMAD extracts the most important information of the <code>POTCAR</code> file and stores them in a stripped version called <code>POTCAR.stripped</code>. The <code>POTCAR</code> files are then automatically removed from the upload, so that you can safely publish your data.</p>"},{"location":"examples/computational_data/workflows.html","title":"Standard and Custom Computational Workflows in NOMAD","text":"<p>The following examples contain the basic knowledge on understanding and learning to use NOMAD workflows, and its relation with DFT and beyond-DFT (GW, BSE, DMFT, etc.) methodologies. You will use a fictitious example of a simulation workflow with the following files and folder structure: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u2514\u2500\u2500 pressure2\n \u00a0\u00a0 \u251c\u2500\u2500 temperature1\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 temperature2\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n \u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n \u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n</code></pre></p> <p>which can be downloaded here:</p>  [Download example_files.zip](data/example_files.zip){ .md-button .nomad-button }  <p>Each of the mainfiles represent an electronic-structure calculation (either DFT{:target=\"blank\"}, TB, or DMFT) which in turn is then parsed into a singular _entry in NOMAD. When dragged into the NOMAD Upload page{:target=\"blank\"}, these files should generate 8 entries in total. This folder structure presents a typical workflow calculation which can be represented as a provenance graph: <pre><code>graph LR;\n    A2((Inputs)) --&gt; B2[DFT];\n    A1((Inputs)) --&gt; B1[DFT];\n    subgraph pressure P2\n    B2[DFT] --&gt; C2[TB];\n    C2[TB] --&gt; D21[DMFT at T1];\n    C2[TB] --&gt; D22[DMFT at T2];\n    end\n    D21[DMFT at T1] --&gt; E21([Output calculation P2, T1])\n    D22[DMFT at T2] --&gt; E22([Output calculation P2, T2])\n    subgraph pressure P1\n    B1[DFT] --&gt; C1[TB];\n    C1[TB] --&gt; D11[DMFT at T1];\n    C1[TB] --&gt; D12[DMFT at T2];\n    end\n    D11[DMFT at T1] --&gt; E11([Output calculation P1, T1])\n    D12[DMFT at T2] --&gt; E12([Output calculation P1, T2])</code></pre> Here, \"Input\" refers to the all _input information given to perform the calculation (e.g., atom positions, model parameters, experimental initial conditions, etc.). \"DFT\", \"TB\" and \"DMFT\" refer to individual tasks of the workflow, which each correspond to a SinglePoint entry in NOMAD. \"Output calculation\" refers to the output data of each of the final DMFT tasks.</p> <p>The goal of this part is to set up the following workflows:</p> <ol> <li>A <code>SinglePoint</code> workflow for one of the calculations (e.g., the DFT one) in the <code>pressure1</code> subfolder.</li> <li>An overarching workflow entry for each pressure P<sub>i=1,2</sub>, grouping all <code>SinglePoint</code> \"DFT\", \"TB\", \"DMFT at T<sub>1</sub>\", and \"DMFT at T<sub>2</sub>\" tasks.</li> <li>A top level workflow entry, grouping together all pressure calculations.</li> </ol> <p>The files for all these cases can be downloaded here:</p>  [Download worfklowyaml_files.zip](data/workflowyaml_files.zip){ .md-button .nomad-button }  <p>You can try writing these files yourself first, and then compare them with the tested files.</p>"},{"location":"examples/computational_data/workflows.html#starting-example-singlepoint-workflow","title":"Starting example: SinglePoint workflow","text":"<p>NOMAD is able to recognize certain workflows in an automatic way, such as the <code>SinglePoint</code> case mentioned above. However, to showcase how to the use workflows in NOMAD, you will learn how to \"manually\" construct the SinglePoint workflow, represented by the following provenance graph: <pre><code>graph LR;\n    A((Inputs)) --&gt; B[DFT];\n    B[DFT] --&gt; C([Output calculation]);</code></pre> To define a workflow manually in NOMAD, you must add a YAML file to the upload folder that contains the relevant input, output, and task information. This file should be named <code>&lt;filename&gt;.archive.yaml</code>. In this case, you should include the file <code>single_point.archive.yaml</code> with the following content:</p> <pre><code>workflow2:\n  name: SinglePoint\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n  outputs:\n    - name: Output calculation\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at Pressure P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>Note several things about the content of this file:</p> <ol> <li><code>name</code> keys are optional.</li> <li>The root path of the upload can be referenced with <code>../upload/archive/mainfile/</code>. Starting from there, the original directory tree structure of the upload is maintained.</li> <li><code>inputs</code> reference the section containing inputs of the whole workflow. In this case this is the section <code>run[0].system[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>outputs</code> reference the section containing outputs of the whole workflow. In this case this is the section <code>run[0].calculation[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>tasks</code> reference the section containing tasks of each step in the workflow. These must also contain <code>inputs</code> and <code>outputs</code> properly referencing the corresponding sections; this will then link inputs/outputs/tasks in the NOMAD Archive. In this case this is a <code>TaskReference</code> to the section <code>workflow2</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>section</code> reference to the uploaded mainfile specific section. The left side of the <code>#</code> symbol contains the path to the mainfile, while the right contains the path to the section.</li> </ol> <p>This will produce an extra entry with the following Overview content:</p> <p></p> <p>Note that you are referencing sections which are lists. Thus, in each case you should be careful to reference the correct section for inputs and outputs (example: a <code>GeometryOptimization</code> workflow calculation will have the \"Input structure\" as <code>run[0].system[0]</code>, while the \"Output calculation\" would also contain <code>run[0].system[-1]</code>, and all intermediate steps must input/output the corresponding section system).</p> <p>NOMAD workflow filename</p> <p>The NOMAD workflow YAML file name, i.e., <code>&lt;filename&gt;</code> in the explanation above, can be any custom name defined by the user, but the file must keep the extension <code>.archive.yaml</code> at the end. This is done in order for NOMAD to recognize this file as a custom schema. Custom schemas are widely used in experimental parsing, and you can learn more about them in the FAIRmat tutorial 8.</p> <p>You can extend the workflow meta-information by adding the metholodogical input parameters. These are stored in NOMAD in the section path <code>run[0].method[-1]</code>. The new <code>single_point.archive.yaml</code> will be:</p> <pre><code>workflow2:\n  name: SinglePoint\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n    - name: Input methodology parameters\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n  outputs:\n    - name: Output calculation\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at Pressure P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n        - name: Input methodology parameters\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n      outputs:\n        - name: Output calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>which in turn produces a similar workflow than before, but with an extra input node:</p> <p></p>"},{"location":"examples/computational_data/workflows.html#pressure-workflows","title":"Pressure workflows","text":"<p>Now that you know the basics of the workflow YAML schema, let's try to define an overarching workflow for each of the pressures. For this section, you will learn how to create the workflow YAML schema for the P<sub>1</sub> case; the extension for P<sub>2</sub> is then a matter of changing names and paths in the YAML files. For simplicity, you can skip referencing to methodologies.</p> <p>Thus, the <code>inputs</code> can be defined as: <pre><code>workflow2:\n  name: DFT+TB+DMFT at P1\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n</code></pre> and there are two <code>outputs</code>, one for each of the DMFT calculations at distinct temperatures: <pre><code>  outputs:\n    - name: Output DMFT at P1, T1 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P1, T2 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Now, <code>tasks</code> are defined for each of the methodologies performed (each corresponding to an underlying SinglePoint workflow). To define a valid workflow, each task must contain an input that corresponds to one of the outputs of the previous task. Moreover, the first task should take as input the overall input of the workflow, and the final task should also have as an output the overall workflow output. Then: <pre><code>  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DFT at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/tb_p1.wout#/workflow2'\n      name: TB at P1\n      inputs:\n        - name: Input DFT at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n      outputs:\n        - name: Output TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\n      name: DMFT at P1 and T1\n      inputs:\n        - name: Input TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n      outputs:\n        - name: Output DMFT at P1, T1 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\n      name: DMFT at P1 and T2\n      inputs:\n        - name: Input TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n      outputs:\n        - name: Output DMFT at P1, T2 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Note here:</p> <ul> <li>The <code>inputs</code> for each subsequent step are the <code>outputs</code> of the previous step.</li> <li>The final two <code>outputs</code> coincide with the <code>workflow2</code> <code>outputs</code>.</li> </ul> <p>This workflow (<code>pressure1.archive.yaml</code>) file will then produce an entry with the following Overview page:</p> <p></p> <p>Similarly, for P<sub>2</sub> you can upload a new <code>pressure2.archive.yaml</code> file with the same content, except when substituting 'pressure1' and 'p1' by their counterparts. This will produce a similar graph than the one showed before but for \"P2\".</p>"},{"location":"examples/computational_data/workflows.html#the-top-level-workflow","title":"The top-level workflow","text":"<p>After adding the workflow YAML files, Your upload folder directory now looks like: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure1.archive.yaml\n\u251c\u2500\u2500 pressure2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure2.archive.yaml\n\u2514\u2500\u2500 single_point.archive.yaml\n</code></pre> In order to define the general workflow that groups all pressure calculations, YOU can reference directly the previous <code>pressureX.archive.yaml</code> files as tasks. Still, <code>inputs</code> and <code>outputs</code> must be referenced to their corresponding mainfile and section paths.</p> <p>Create a new <code>fullworkflow.archive.yaml</code> file with the <code>inputs</code>: <pre><code>workflow2:\n  name: Full calculation at different pressures for SrVO3\n  inputs:\n    - name: Input structure at P1\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n    - name: Input structure at P2\n      section: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n</code></pre> And <code>outputs</code>: <pre><code>  outputs:\n    - name: Output DMFT at P1, T1 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P1, T2 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P2, T1 calculation\n      section: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n    - name: Output DMFT at P2, T2 calculation\n      section: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Finally, <code>tasks</code> references the previous YAML schemas as follows: <pre><code>  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1.archive.yaml#/workflow2'\n      name: DFT+TB+DMFT at P1\n      inputs:\n        - name: Input structure at P1\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DMFT at P1, T1 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n        - name: Output DMFT at P1, T2 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure2.archive.yaml#/workflow2'\n      name: DFT+TB+DMFT at P2\n      inputs:\n        - name: Input structure at P2\n          section: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DMFT at P2, T1 calculation\n          section: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n        - name: Output DMFT at P2, T2 calculation\n          section: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre></p> <p>This will produce the following entry and its Overview page:</p> <p></p>"},{"location":"examples/computational_data/workflows.html#automatic-workflows","title":"Automatic workflows","text":"<p>There are some cases where the NOMAD infrastructure is able to recognize certain workflows automatically when processing the uploaded files. The simplest example is any <code>SinglePoint</code> calculation, as explained above. Other examples include <code>GeometryOptimization</code>, <code>Phonons</code>, <code>GW</code>, and <code>MolecularDynamics</code>. Automated workflow detection may require your folder structure to fulfill certain conditions.</p> <p>Here are some general guidelines for preparing your upload folder in order to make it easier for the automatic workflow recognition to work:</p> <ul> <li>Always organize your files in an top-down structure, i.e., the initial tasks should be upper in the directory tree, while the later tasks lower on it.</li> <li>Avoid having to go up and down between folders if some properties are derived between these files. These situations are very complicated to predict for the current NOMAD infrastructure.</li> <li>Avoid duplication of files in subfolders. If initially you do a calculation A from which a later calculation B is derived and you want to store B in a subfolder, there is no need to copy the A files inside the subfolder B.</li> </ul> <p>The folder structure used throughout this part is a good example of a clean upload which is friendly and easy to work with when defining NOMAD workflows.</p>"},{"location":"examples/experiment_data/apm.html","title":"Domain-specific examples for atom probe tomography and field ion microscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p>"},{"location":"examples/experiment_data/apm.html#contextualization-for-the-technique-and-the-scientific-domain","title":"Contextualization for the technique and the scientific domain","text":"<p>A variety of file formats are used in the research field of atom probe tomography to document atom probe measurements and computer simulations. The pynxtools-apm plugin of the pynxtools parsing library solves the challenge of how these formats can be parsed and normalized into a common representation that increases interoperability and adds semantic expressiveness.</p> <ul> <li>pynxtools-apm</li> </ul> <p>The plugin uses the ifes_apt_tc_data_modeling Python library that is developed together with the International Field Emission Society.</p>"},{"location":"examples/experiment_data/apm.html#work-with-standardized-atom-probe-data-in-nomad-and-north","title":"Work with standardized atom probe data in NOMAD and NORTH","text":"<p>Once standardized, NXapm-compliant data in NOMAD can be explored with domain-specific software tools using a convenient JupyterHub-based service offered by the NOMAD Remote Tools Hub (NORTH).</p> <p>For this you should go to <code>Analyze</code> (menu bar) and select <code>NOMAD Remote Tools Hub</code>. If you are using the public Oasis the hub is available here. This service and toolset of NOMAD offers a set of preconfigured containers. We have configured the <code>apmtools</code> container to provide you an example of the capabilities that come with such customizable containers interfaced to an Oasis. Mind that if you are using a local Oasis the hub may not have been activated and that specific containers need to be pull from the container registry as they are not part of a default Oasis development installation.</p> <p>Currently, <code>apmtools</code> is a specific docker container that offers a graphical user interface through the web browser that serves your NOMAD GUI. The container includes three data analysis tools: - aptyzer by Alexander Reichmann et al. - paraprobe-toolbox by Markus K\u00fchbach et al. - apav by Jesse Smith et al.</p> <p>The container is configured such that the data in your uploads are available via the <code>/config/uploads</code> sub-directory from within the container. Thereby, you can move data in between the container. The connection between the container image and NOMAD allows you to enrich your upload with specific post-processing results generated by any of the tools in the container. The paraprobe-toolbox exemplifies how NeXus can be used to document the specific settings and results of such processing via an open standard and documentation.</p> <p>The <code>apmtools</code> container comes with a collection of getting started tutorials via a <code>Cheatsheet</code> jupyter notebook which you can access from the <code>/home/atom_probe_tools</code> sub-directory. Details dependent on the specific configuration of your NOMAD OASIS.</p>"},{"location":"examples/experiment_data/em.html","title":"Domain-specific examples for electron microscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <ul> <li>pynxtools-em</li> </ul>"},{"location":"examples/experiment_data/mpes.html","title":"Domain-specific examples for photoemission spectroscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p>"},{"location":"examples/experiment_data/nexus.html","title":"Standardized ingestion of data from materials characterization using NeXus","text":"<p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <p>NeXus is a common data format developed by an international collaboration of scientists to support the exchange, storage, and archival of scientific data, particularly from neutron, X-ray, and muon experiments which is extended to cover experiment techniques used in Materials Science. Built on top of HDF5, NeXus adds a structured, self-describing framework designed specifically for complex scientific experiments.</p> <p>The goal of NeXus is to make scientific data easier to share, analyze, and visualize, both within a facility and across different research institutions. It achieves this through:</p> <ul> <li>Well-defined metadata conventions that capture experimental context alongside the data itself.</li> <li>Application definitions that provide templates for specific experiment types to ensure consistency.</li> <li>A corresponding ontology which defines the terms and their relationship</li> <li>Standardized file structure that organizes data into hierarchical groups, folowing the ontology relationship</li> </ul> <p>NeXus files can store raw data, processed data, and the metadata necessary to fully understand how the data was collected and analyzed. This makes NeXus a valuable tool for preserving data integrity and promoting FAIR (Findable, Accessible, Interoperable, Reusable) data practices across scientific disciplines.</p> <p>Whether you are storing simple measurements or the output of complex multi-component instruments, NeXus provides a flexible, extensible framework to help ensure your data remains useful and understandable far into the future.</p> <ul> <li>FAIRmat NeXus extension proposal</li> <li>NeXusOntology</li> <li>NeXus</li> </ul>"},{"location":"examples/experiment_data/opt.html","title":"Domain-specific examples for optical spectroscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <ul> <li>pynxtools-ellips</li> </ul>"},{"location":"examples/experiment_data/pynxtools.html","title":"Parse materials characterization data with pynxtools","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p> <ul> <li>pynxtools</li> </ul>"},{"location":"examples/experiment_data/stm.html","title":"Domain-specific examples for STS / STM (scanning tunneling spectroscopy / microscopy)","text":"<p>Build upon your understanding of NOMAD's features with domain-specific examples and explanations.</p>"},{"location":"examples/experiment_data/stm.html#contextualization-for-the-technique-and-the-scientific-domain","title":"Contextualization for the technique and the scientific domain","text":"<p>A variety of file formats (coming technology instrumens) are used in the research field of scanning tunneling microscopy (STM) and scanning tunneling spectroscopy (STS) to investigate topological propertise of surface of subjected material. The pynxtools-stm plugin (note: The single plugin handles both STM as well STS techniques) of the pynxtools parsing library solves the challenges of how these formats can be parsed and normalized into a common representation that increases interoperability and adds semantic expressiveness.</p> <p>The pynxtools-stm provides a indispensable tool to transfor the STM as well as STS experimental data (sometime refered as raw data or machine data) to common standarized structure defined in NXsts (GitHub page) application definition build with the help of NeXus ontology (GitHub page). One of the main goals of such effort is to make the data comming from diverse sources comparable, searchable and shearable under the hood of NONAD research data management platform.</p> <p>For full benefits and usages of the reader please following links:</p> <ul> <li>Full Reader Documentation</li> <li>GitHub Repository Documentation on how to upload STM / STS data sets from different sources can be found here</li> </ul>"},{"location":"examples/experiment_data/stm.html#supported-file-formats","title":"Supported file formats","text":"<p>A list of the supported file formats can be found in the <code>pynxtools-stm</code> documentation.</p>"},{"location":"examples/experiment_data/xps.html","title":"Domain-specific examples for X-ray photoelectron spectroscopy","text":"<p>Attention</p> <p>We are currently working to update this content.</p>"},{"location":"examples/experiment_data/xps.html#contextualization-for-the-technique-and-the-scientific-domain","title":"Contextualization for the technique and the scientific domain","text":"<p>A variety of file formats are used in the research field of X-ray photoelectron spectroscopy and related techniques. The <code>pynxtools-xps</code> plugin of the <code>pynxtools</code> parsing library solves the challenge of how these formats can be parsed and normalized into a common representation that increases interoperability and adds semantic expressiveness.</p> <p><code>pynxtools-xps</code>, which is a plugin for pynxtools, provides a tool for reading data from various propietary and open data formats from technology partners and the wider XPS community and standardizing it such that it is compliant with the NeXus application definitions <code>NXmpes</code> and <code>NXxps</code>, which is an extension of <code>NXmpes</code>.</p> <ul> <li>Documentation</li> <li>GitHub repository</li> <li>Issue tracker</li> </ul>"},{"location":"examples/experiment_data/xps.html#how-to-upload-xps-data-to-nomad","title":"How to upload XPS data to NOMAD","text":"<p>Documentation on how to upload XPS data sets from different sources can be found here</p>"},{"location":"examples/experiment_data/xps.html#supported-file-formats","title":"Supported file formats","text":"<p>A list of the supported file formats can be found in the <code>pynxtools-xps</code> documentation.</p>"},{"location":"explanation/architecture.html","title":"Architecture","text":""},{"location":"explanation/architecture.html#a-containerized-cloud-enabled-architecture","title":"A containerized cloud enabled architecture","text":"<p>NOMAD is a modern web-application that requires a lot of services to run. Some are NOMAD specific, others are 3rd party products. While all services can be traditionally installed and run on a single sever, NOMAD advocates the use of containers and operating NOMAD in a cloud environment.</p> <p> </p> NOMAD architecture <p>NOMAD comprises two main services, its app and the worker. The app services our API, graphical user interface, and documentation. It is the outward facing part of NOMAD. The worker runs all the processing (parsing, normalization). Their separation allows to scale the system for various use-cases.</p> <p>Other services are:</p> <ul> <li>rabbitmq: a task queue that we use to distribute tasks for the worker containers</li> <li>mongodb: a no-sql database used to maintain processing state and user-metadata</li> <li>elasticsearch: a no-sql database and search engine that drives our search</li> <li>a regular file system to maintain all the files (raw and archive)</li> <li>jupyterhub: run ai toolkit notebooks</li> <li>keycloak: our SSO user management system (can be used by all Oasises)</li> <li>a content management system to provide other web-page content (not part of the Oasis)</li> </ul> <p>All NOMAD software is bundled in a single NOMAD docker image and a Python package (nomad-lab on pypi). The NOMAD docker image can be downloaded from our public registry. NOMAD software is organized in multiple git repositories. We use continuous integration to constantly provide the latest version of docker image and Python package.</p>"},{"location":"explanation/architecture.html#nomad-uses-a-modern-and-rich-stack-frameworks-systems-and-libraries","title":"NOMAD uses a modern and rich stack frameworks, systems, and libraries","text":"<p>Besides various scientific computing, machine learning, and computational material science libraries (e.g. numpy, skikitlearn, tensorflow, ase, spglib, matid, and many more), Nomad uses a set of freely available technologies that already solve most of its processing, storage, availability, and scaling goals. The following is a non comprehensive overview of used languages, libraries, frameworks, and services.</p> <p> </p> NOMAD components and dependencies"},{"location":"explanation/architecture.html#python-3","title":"Python 3","text":"<p>The backend of nomad is written in Python. This includes all parsers, normalizers, and other data processing. We only use Python 3 and there is no compatibility with Python 2. Code is formatted close to pep8, critical parts use pep484 type-hints. ruff, and mypy (static type checker) are used to ensure quality. Tests are written with pytest. Logging is done with structlog and logstash (see Elasticstack below). Documentation is driven by Sphinx.</p>"},{"location":"explanation/architecture.html#celery","title":"celery","text":"<p>Celery (+ rabbitmq) is a popular combination for realizing long running tasks in internet applications. We use it to drive the processing of uploaded files. It allows us to transparently distribute processing load while keeping processing state available to inform the user.</p>"},{"location":"explanation/architecture.html#elastic-search","title":"elastic search","text":"<p>Elasticsearch is used to store repository data (not the raw files). Elasticsearch enables flexible, scalable search and analytics.</p>"},{"location":"explanation/architecture.html#mongodb","title":"mongodb","text":"<p>Mongodb is used to store and track the state of the processing of uploaded files and the generated entries. We use mongoengine to program with mongodb.</p>"},{"location":"explanation/architecture.html#keycloak","title":"Keycloak","text":"<p>Keycloak is used for user management. It manages users and provides functions for registration, forgetting passwords, editing user accounts, and single sign-on to fairdi@nomad and other related services.</p>"},{"location":"explanation/architecture.html#fastapi","title":"FastAPI","text":"<p>The ReSTful API is build with the FastAPI framework. This allows us to automatically derive a OpenAPI description of the nomad API. Fruthermore, you can browse and use the API via OpenAPI dashboard.</p>"},{"location":"explanation/architecture.html#elasticstack","title":"Elasticstack","text":"<p>The elastic stack (previously ELK stack) is a centralized logging, metrics, and monitoring solution that collects data within the cluster and provides a flexible analytics front end for that data.</p>"},{"location":"explanation/architecture.html#javascript-react-material-ui","title":"Javascript, React, Material-UI","text":"<p>The frontend (GUI) of nomad@FAIRDI is built on the React component framework. This allows us to build the GUI as a set of re-usable components to achieve a coherent representations for all aspects of nomad, while keeping development efforts manageable. React uses JSX (a ES6 variety) that allows to mix HTML with Javascript code. The component library Material-UI (based on Google's popular material design framework) provides a consistent look-and-feel.</p>"},{"location":"explanation/architecture.html#docker","title":"docker","text":"<p>To run a nomad@FAIRDI instance, many services have to be orchestrated: the nomad app, nomad worker, mongodb, Elasticsearch, Keycloak, RabbitMQ, Elasticstack (logging), the nomad GUI, and a reverse proxy to keep everything together. Further services might be needed (e.g. JypiterHUB), when nomad grows. The container platform Docker allows us to provide all services as pre-build images that can be run flexibly on all types of platforms, networks, and storage solutions. Docker-compose allows us to provide configuration to run the whole nomad stack on a single server node.</p>"},{"location":"explanation/architecture.html#kubernetes-helm","title":"kubernetes + helm","text":"<p>To run and scale nomad on a cluster, you can use kubernetes to orchestrated the  necessary containers. We provide a helm chart with all necessary service and deployment descriptors that allow you to set up and update nomad with only a few commands.</p>"},{"location":"explanation/architecture.html#gitlab","title":"GitLab","text":"<p>Nomad as a software project is managed via GitLab. The nomad@FAIRDI project is hosted here. GitLab is used to manage versions, different branches of development, tasks and issues, as a registry for Docker images, and CI/CD platform.</p>"},{"location":"explanation/basics.html","title":"From files to data","text":"<p>NOMAD is based on a bottom-up approach to data management. Instead of only supporting data in a specific predefined format, we process files to extract data from an extendable variety of data formats.</p> <p>Converting heterogenous files into homogeneous machine actionable processed data is the basis to make data FAIR. It allows us to build search interfaces, APIs, visualization, and analysis tools independent from specific file formats.</p> <p> </p> NOMAD's datamodel and processing"},{"location":"explanation/basics.html#uploads","title":"Uploads","text":"<p>Users create uploads to organize files. Think of an upload like a project: many files can be put into a single upload and an upload can be structured with directories. You can collaborate on uploads, share uploads, and publish uploads. The files in an upload are called raw files. Raw files are managed by users and they are never changed by NOMAD.</p> <p>Note</p> <p>As a rule, raw files are not changed during processing (or otherwise). However, to achieve certain functionality, a parser, normalizer, or schema developer might decide to bend this rule. Use-cases include the generation of more mainfiles (and entries) and updating of related mainfiles to automatize ELNs, or generating additional files to convert a mainfile into a standardized format like nexus or cif.</p>"},{"location":"explanation/basics.html#files","title":"Files","text":"<p>We already said that all uploaded files are raw files. Recognized files that have an entry are called mainfiles. Only the mainfile of the entry is passed to the parser during processing. However, a parser can call other tools or read other files. Therefore, we consider all files in the same directory of the mainfile as auxillary files, even though there is not necessarily a formal relationship with the entry. If formal relationships with aux files are established, e.g. via a reference to the file within the processed data, is up to the parser.</p>"},{"location":"explanation/basics.html#entries","title":"Entries","text":"<p>All uploaded raw files are analysed to find files with a recognized format. Each file that follows a recognized format is a mainfile. For each mainfile, NOMAD will create a database entry. The entry is eternally matched to the mainfile. The entry id, for example, is a hash over the upload id and the mainfile path (and an optional key) within the upload. This matching process is automatic, and users cannot create entries manually.</p> <p>Note</p> <p>We say that raw files are not changed by NOMAD and that users cannot create entries, but what about ELNs? There is a create entry button in the UI?</p> <p>However, NOMAD will simply create an editable mainfile that indirectly creates an entry. The user might use NOMAD as an editor to change the file, but the content is determined by the users. Contrary to the processed data that is created from raw files by NOMAD.</p>"},{"location":"explanation/basics.html#datasets","title":"Datasets","text":"<p>Users can build collections of entries to form datasets. You can imagine datasets like tags or albums in other systems. Each entry can be contain in many datasets and a dataset can hold many entries. Datasets can also overlap. Datasets are only indirectly related to files. The main purpose of datasets in NOMAD is to have citable collections of data. Users can get a DOI for their datasets. Datasets have no influence on the processing of data.</p>"},{"location":"explanation/basics.html#processing","title":"Processing","text":"<p>The processing of entries is automatic. Initially and on each mainfile change, the entry corresponding to the mainfile, will be processed. Processing consist of parsing, normalizing, and persisting the created data, as explained in more detail in the Processing section.</p>"},{"location":"explanation/basics.html#parsing","title":"Parsing","text":"<p>Parsers are small programs that transform data from a recognized mainfile into a structured machine processable tree of data that we call the archive or processed data of the entry. Only one parser is used for each entry. The used parser is determined during matching and depends on the file format. A dedicated guide shows how to match a specific file from your parser. Parsers can be added to NOMAD as plugins; this is a list of all available parsers in the central installation.</p> <p>Note</p> <p>A special case is the parsing of NOMAD archive files. Usually a parser converts a file from a source format into NOMAD's archive format for processed data. But users can also create files following this format themselves. They can be uploaded either as <code>.json</code> or <code>.yaml</code> files by using the <code>.archive.json</code> or <code>.archive.yaml</code> extension. In these cases, we also considering these files as mainfiles and they are also going to be processed. Here the parsing is a simple syntax check and basically just copying the data, but normalization might still modify and augment the data substantially. One use-case for these archive files, are ELNs. Here the NOMAD UI acts as an editor for a respective <code>.json</code> file, but on each save, the corresponding file is going through all the regular processing steps. This allows ELN schema developers to add all kinds of functionality such as updating referenced entries, parsing linked files, or creating new entries for automation.</p>"},{"location":"explanation/basics.html#normalizing","title":"Normalizing","text":"<p>While parsing converts a mainfile into processed data, normalizing is only working on the processed data. Learn more about why to normalize in the documentation on structured data. There are two principle ways to implement normalization in NOMAD: normalizers and normalize functions.</p> <p>Normalizers are small programs that take processed data as input. There is a list of normalizers registered in the NOMAD configuration. In the future, normalizers might be added as plugins as well. They run in the configured order. Every normalizer is run on all entries and the normalizer might decide to do something or not, depending on what it sees in the processed data.</p> <p>Normalize functions are special functions implemented as part of section definitions in Python schemas. There is a special normalizer that will go through all processed data and execute these function if they are defined. Normalize functions get the respective section instance as input. This allows schema plugin developers to add normalizing to their sections. Read about our structured data to learn more about the different sections.</p>"},{"location":"explanation/basics.html#storing-and-indexing","title":"Storing and indexing","text":"<p>As a last technical step, the processed data is stored and some information is passed into the search index. The store for processed data is internal to NOMAD and processed data cannot be accessed directly and only via the archive API or ArchiveQuery Python library functionality. What information is stored in the search index is determined by the metadata and results sections and cannot be changed by users or plugins. However, all scalar values in the processed data are also index as key-values pairs.</p> <p>Attention</p> <p>This part of the documentation should be more substantiated. There will be a learn section about the search soon.</p>"},{"location":"explanation/data.html","title":"Data structure","text":"<p>NOMAD structures data into sections, where each section can contain data and more sections. This allows to browse complex data like you would browse files and directories on your computer. Each section follows a definition and all the contained data and subsection have a specific name, description, possible type, shape, and unit. This means that all data follows a schema. This not only helps the human exploration, but also makes it machine interpretable, increases consistency and interoperability, enables search, APIs, visualization, and analysis.</p> <p> </p> Browsing structured data in the NOMAD UI (link)"},{"location":"explanation/data.html#schema-language","title":"Schema language","text":"<p>The bases for structured data are schemas written in a schema language. Our schema language is called the NOMAD Metainfo language. The name is evocative of the rich metadata information that should be associated with the research data and made available in a machine-readable format. It defines the tools to define sections, organize definitions into packages, and define section properties (subsections and quantities).</p> <p> </p> The NOMAD Metainfo schema language for structured data definitions <p>Packages contain section definitions, section definitions contain definitions for subsections and quantities. Sections can inherit the properties of other sections. While subsections allow to define containment hierarchies for sections, quantities can use section definitions (or other quantity definitions) as a type to define references.</p> <p>If you are familiar with other schema languages and means to defined structured data (json schema, XML schema, pydantic, database schemas, ORM, etc.), you might recognize these concept under different names. Sections are similar to classes, concepts, entities, or  tables. Quantities are related to properties, attributes, slots, columns. subsections might be called containment or composition. subsections and quantities with a section type also define relationships, links, or references.</p> <p>Our guide on how to write a schema explains these concepts with an example.</p>"},{"location":"explanation/data.html#schema","title":"Schema","text":"<p>NOMAD represents many different types of data. Therefore, we cannot speak of just the one schema. The entirety of NOMAD schemas is called the NOMAD Metainfo. Definitions used in the NOMAD Metainfo fall into three different categories. First, we have sections that define a shared entry structure. Those are independent of the type of data (and processed file type). They allow to find all generic parts without any deeper understanding of the specific data. Second, we have definitions of re-usable base sections for shared common concepts and their properties. Specific schemas can use and extend these base sections. Base sections define a fixed interface or contract that can be used to build tools (e.g. search, visualizations, analysis) around them. Lastly, there are specific schemas. Those re-use base sections and complement the shared entry structure. They define specific data structures to represent specific types of data.</p> <p> </p>      The three different categories of NOMAD schema definitions"},{"location":"explanation/data.html#base-sections","title":"Base sections","text":"<p>Base section is a very loose category. In principle, every section definition can be inherited from or can be re-used in different contexts. There are some dedicated (or even abstract) base section definitions (mostly defined in the <code>nomad.datamodel.metainfo</code> package and sub-packages), but schema authors should not strictly limit themselves to these definitions. The goal is to re-use as much as possible and to not re-invent the same sections over and over again. Tools build around certain base section, provide an incentive to use them.</p> <p>Attention</p> <p>There is no detailed how-to or reference documentation on the existing base sections and how to use them yet.</p> <p>One example for re-usable base section is the workflow package. These allow to define workflows in a common way. They allow to place workflows in the shared entry structure, and the UI provides a card with workflow visualization and navigation for all entries that have a workflow inside.</p> <p>Attention</p> <p>Currently there are two version of the workflow schema. They are stored in two top-level <code>EntryArchive</code> subsections (<code>workflow</code> and <code>workflow2</code>). This will change soon to something that supports multiple workflows used in specific schemas and results.</p>"},{"location":"explanation/data.html#specific-schemas","title":"Specific schemas","text":"<p>Specific schemas allow users and plugin developers to describe their data in all detail. However, users (and machines) not familiar with the specifics, will struggle to interpret these kinda of data. Therefore, it is important to also translate (at least some of) the data into a more generic and standardized form.</p> <p> </p>      From specific data to more general interoperable data.    <p>The results section provides a shared structure designed around base section definitions. This allows you to put (at least some of) your data where it is easy to find, and in a form that is easy to interpret. Your non-interoperable, but highly detailed data needs to be transformed into an interoperable (but potentially limited) form.</p> <p>Typically, a parser will be responsible to populate the specific schema, and the interoperable schema parts (e.g. section results) are populated during normalization. This allows to separate certain aspects of conversions and potentially enables re-use for normalization routines. The necessary effort for normalization depends on how much the specific schema deviates from base-sections. There are three levels:</p> <ul> <li>the parser (or uploaded archive file) populates section results directly</li> <li>the specific schema re-uses the base sections used for the results and normalization can be automated</li> <li>the specific schema represents the same information differently and a translating normalization algorithm needs to be implemented.</li> </ul>"},{"location":"explanation/data.html#exploring-the-schema","title":"Exploring the schema","text":"<p>All built-in definitions that come with NOMAD or one of the installed plugins can be explored with the Metainfo browser. You can start with the root section <code>EntryArchive</code> and browse based on subsections, or explore the Metainfo through packages.</p> <p>To see all user provided uploaded schemas, you can use a search for the subsection <code>definition</code>. The subsection <code>definition</code> is a top-level <code>EntryArchive</code> subsection. See also our how-to on writing and uploading schemas.</p>"},{"location":"explanation/data.html#contributing-to-the-metainfo","title":"Contributing to the Metainfo","text":"<p>The shared entry structure (including section results) is part of the NOMAD source-code. It interacts with core functionality and needs to be highly controlled. Contributions here are only possible through merge requests.</p> <p>Base sections can be contributed via plugins. Here they can be explored in the Metainfo browser, your plugin can provide more tools, and you can make use of normalize functions. See also our how-to on writing schema packages. You could also provide base sections via uploaded schemas, but those are harder to explore and distribute to other NOMAD installations.</p> <p>Specific schemas can be provided via plugins or as uploaded schemas. When you upload schemas, you most likely also upload data in archive files (or use ELNs to edit such files). Here you can also provide schemas and data in the same file. In many case specific schemas will be small and only re-combine existing base sections. See also our how-to on writing YAML schemas.</p>"},{"location":"explanation/data.html#data","title":"Data","text":"<p>All processed data in NOMAD instantiates Metainfo schema definitions and the archive of each entry is always an instance of <code>EntryArchive</code>. This provides an abstract structure for all data. However, it is independent of the actual representation of data in computer memory or how it might be stored in a file or database.</p> <p>The Metainfo has many serialized forms. You can write <code>.archive.json</code> or <code>.archive.yaml</code> files yourself. NOMAD internally stores all processed data in message pack. Some of the data is stored in mongodb or elasticsearch. When you request processed data via API, you receive it in JSON. When you use the ArchiveQuery, all data is represented as Python objects (see also example in schema package documentation).</p> <p>No matter what the representation is, you can rely on the structure, names, types, shapes, and units defined in the schema to interpret the data.</p>"},{"location":"explanation/data.html#archive-files-a-shared-entry-structure","title":"Archive files: a shared entry structure","text":"<p>Broadening the discussion on the entry files that one can find in NOMAD, both schemas or processed data are serialized as the same kind of archive file, either <code>.archive.json</code> or <code>.archive.yaml</code>.</p> <p>The NOMAD archive file is indeed composed by several sections.</p> <p>NOMAD archive file:<code>EntryArchive</code></p> <ul> <li>definitions: <code>Definitions</code></li> <li>metadata: <code>EntryMetadata</code></li> <li>data: <code>EntryData</code></li> <li>run: <code>Run</code></li> <li>nexus: <code>Nexus</code></li> <li>workflow: <code>LegacyWorkflow</code></li> <li>workflow2: <code>Workflow</code></li> <li>results: <code>Results</code></li> </ul> <p>They all instantiate the same root section <code>EntryArchive</code>. They all share common sections <code>metadata:Metadata</code> and <code>results:Results</code>. They also all contain a data section, but the used section definition varies depending on the type of data of the specific entry. There is the literal <code>data:EntryData</code> subsection. Here <code>EntryData</code> is abstract and specific entries will use concrete definitions that inherit from <code>EntryData</code>. There are also specific data sections, like <code>run</code> for simulation data and <code>nexus</code> for nexus data.</p> <p>Note</p> <p>As shown in Uploading schemas, one can, in principle, create an archive file with both <code>definitions</code> and one of the data sections filled, although this is not always desired because it will stick together a schema and a particular instance of that schema. They should be kept separate so that it is still possible to generate new data files from the same schema file.</p> <p>Attention</p> <p>The results, originally only designed for computational data, will soon be revised an replaced by a different section. However, the necessity and function of a section like this remains.</p> <p> </p>      All entries instantiate the same section share the same structure."},{"location":"explanation/oasis.html","title":"Federation and Oasis","text":""},{"location":"explanation/oasis.html#why-a-federated-data-infrastructure","title":"Why a federated data infrastructure?","text":"<p>There are several benefits for using multiple NOMAD installations:</p> <ul> <li>Sovereignty: Data is stored and managed locally. This is important for data privacy and security.</li> <li>Resource: Local resources can be used to manage and analyse data. This is important for large data sets.</li> <li>Customization: Local installations can be customized (plugins). This is important to support local workflows and special file formats.</li> </ul>"},{"location":"explanation/oasis.html#what-is-a-nomad-oasis","title":"What is a NOMAD Oasis?","text":"<p>The software that runs NOMAD is Open-Source and can be used independently of the NOMAD central installation at http://nomad-lab.eu. We call any NOMAD installation that is not the central one a NOMAD Oasis.</p>"},{"location":"explanation/oasis.html#use-cases","title":"Use cases","text":"<p>There are several use-cases how the NOMAD software could be used. Of course other uses and hybrids are imaginable:</p> <ul> <li>Academia: Use the Oasis for local management of unpublished research data</li> <li>Industry: Use of Oasis to manage private data and full internal use of published data in compliance with strict privacy policies</li> <li>Mirror: Use the Oasis as a mirror that hosts a copy of all published NOMAD data</li> <li>FAIRmat: Use Oasis to form a network of repositories to build a federated data infrastructure   for materials science.   This is what we do in the FAIRmat project.</li> </ul> <p> </p> NOMAD Oasis use-cases"},{"location":"explanation/oasis.html#data-transfer","title":"Data transfer","text":"<p>Attention</p> <p>The data transfer implementation is still under development and not yet relyable for all use-cases. See \"Current Limitations and Plans\" below.</p> <p>NOMAD data is inherently file based. You upload raw-files (or in case of ELNs edit raw files in the web interface) and the NOMAD processes these files to extract data which again is stored in files. Therefore, data transfer is basically \"just\" copying files from one installation to another.</p> <p>We implemented a bundle format for published data. A bundle is a zip file that contains all raw files, archive files of an upload, and a bit of metadata, e.g. with information on upload name, dates, authors, datasets, etc. This bundle format is our tranfer file format. Bundles are exported on one end and imported on the other.</p> <p>Upon import, the reciving NOMAD replicates the state of the upload on the sending NOMAD. This means that the upload is published and the data is indexed. Users can treat the upload as if they had uploaded it on the receiving NOMAD themselves.</p> <p>There are two principle ways to transfer data:</p> <ul> <li> <p>Automated: The Oasis user interface offers an option to transfer data to the configured   central NOMAD installation. This uses an API endpoint on the Oasis that creates a bundle and pushes it   to the central NOMAD installation via the API of the central NOMAD.</p> </li> <li> <p>Manual: You can use the NOMAD CLI to export uploads as bundle files and   upload them manually to another NOMAD installation.</p> </li> <li> <p>Re-upload: Of course you could also skip the bundle format entrily and just   re-upload the raw files to another NOMAD installation.</p> </li> </ul>"},{"location":"explanation/oasis.html#current-limitations-and-plans","title":"Current Limitations and Plans","text":"<p>The basic bundle format, export, and import, and a transfer button in the UI are implemented. However, there are still limitations and plans for future development:</p> <ul> <li> <p>Trust issues: Some of the bundle metadata is normally created by   NOMAD. Timestamps like upload times are the biggest concerns here.   This means that the receiving NOMAD needs to trust that the timestamps in a bundle   are not forged. Therefore, it needs to verify that the bundle   was actually created by a trusted Oasis. Currently, Oasis have to be   configured to use a trusted user (usually the Oasis admin) to submit bundles   to the central NOMAD.   A version were timestamps are ignored and the upload is dated to the time   it was received, would also be possible implementation, but is not implemented   yet.</p> </li> <li> <p>Plugins and versions: Two installations (e.g. Oasis and central NOMAD)   might not have the same plugins installed and not have the same version   of all schemas available. Received data might not be fully processable.   Therefore, it is planned, but not yet implemented, to make all required schemas   part of the bundle.</p> </li> <li> <p>Metadata transfers: Currently, only full uploads with all raw files   and all archive files can be transferred.   It is planned to allow transfers for only the \"metadata\". Here metadata   refers to the archive files (or even just certain result sections from   said archive files). This would still allow to index and find entries on the   central NOMAD without transfering potentially huge raw files. This would allow   the central NOMAD to act as a portal for all data in the federation. This is not   yet implemented.</p> </li> </ul>"},{"location":"explanation/oasis.html#references-to-the-implementation","title":"References to the implementation","text":"<p>There are no detailed how-tos yet. Here are some pointers to reference documentation that might be relevant:</p> <ul> <li>The API endpoint to download an upload in bundle format.</li> <li>The API endpoint to upload a bundle.</li> <li>The API endpoint to publish an upload, including publishing to the configured central NOMAD.</li> <li>The CLI commands to export and import an upload as a bundle.</li> </ul>"},{"location":"explanation/plugin_system.html","title":"NOMAD plugin system","text":""},{"location":"explanation/plugin_system.html#introduction","title":"Introduction","text":"<p>NOMAD is used by many research communities with their specific data, workflows, and analysis tools. NOMAD plugins are key to adopt NOMAD to these heterogeneous environments. You can think of plugins as \u201cadd-ons\u201d that provide additional capabilities. Each plugin is a small independant software project that integrates with the core NOMAD and provides features without modifictions to the core NOMAD itself. Some key advantages of using plugins:</p> <ul> <li> <p>Modularity: You can pick and choose which features or functions to add, rather than having everything baked into the core NOMAD.</p> </li> <li> <p>Customizability: Users can add their own plugins to address specific use cases, without changing the official NOMAD software.</p> </li> <li> <p>Easy updates: If a feature needs to be updated or improved, it can be done at the plugin level, without having to release a new NOMAD version.</p> </li> <li> <p>Collaboration: Since plugins are independent, multiple developers can work on different features in parallel and with different release cycles without interfering with each other.</p> </li> </ul>"},{"location":"explanation/plugin_system.html#architecture","title":"Architecture","text":"<p>There are three core components to the plugin system:</p> <ul> <li>Distributions define lists of plugins and their version. A distribution is a small   Git and Python project that maintains a list of plugin dependencies in its <code>pyproject.toml</code>. We provide a template repository   for a quick start into creating distributions.</li> <li>Plugins are Git and Python projects that contain one or many entry points.   We provide a template repository   for a quick start into plugin development.</li> <li>Entry points are individual contributions (e.g. parsers, schemas, or apps)   which are defined using a feature of Python called entry points.</li> </ul> <p><pre><code>%%{init:{'flowchart':{'nodeSpacing': 25, 'subGraphTitleMargin': {'top': 5, 'bottom': 10}, 'padding': 10}}}%%\ngraph LR\n  subgraph NOMAD Distribution\n    subgraph NOMAD Plugin C\n      ro7(Entry point: Schema 1)\n      ro8(Entry point: Schema 2)\n    end\n    subgraph NOMAD Plugin B\n      ro4(Entry point: Schema)\n      ro5(Entry point: App 1)\n      ro6(Entry point: App 2)\n    end\n    subgraph NOMAD Plugin A\n      ro1(Entry point: Schema)\n      ro2(Entry point: Parser 1)\n      ro3(Entry point: Parser 2)\n    end\n  end</code></pre> </p> Relation between NOMAD distributions, plugins and entry points. <p>This architecture allows plugin developers to freely choose a suitable granularity for their use case: they may create a single plugin package that contains everything that e.g. a certain lab needs: schemas, parsers and apps. Alternatively they may also develop multiple plugins, each containing a single entry point. Reduction in package scope can help in developing different parts indepedently and also allows plugin users to choose only the parts that they need.</p>"},{"location":"explanation/plugin_system.html#plugin-entry-points","title":"Plugin entry points","text":"<p>Plugin entry points represent different types of customizations that can be added to a NOMAD installation. The following plugin entry point types are currently supported:</p> <ul> <li>APIs</li> <li>Apps</li> <li>Example uploads</li> <li>Normalizers</li> <li>Parsers</li> <li>Schema packages</li> </ul> <p>Entry points contain configuration, but also a resource, which lives in a separate Python module. This split enables lazy-loading: the configuration can be loaded immediately, while the resource is loaded later when/if it is required. This can significantly improve startup times, as long as all time-consuming initializations are performed only when loading the resource. This split also helps to avoid cyclical imports between the plugin code and the <code>nomad-lab</code> package.</p> <p>For example the entry point configuration for a parser is contained in <code>.../parsers/__init__.py</code> and it contains e.g. the name, version and any additional entry point-specific parameters that control its behaviour. The entry point has a <code>load</code> method than can be called lazily to return the resource, which is a <code>Parser</code> instance defined in <code>.../parsers/myparser.py</code>.</p> <p>In <code>pyproject.toml</code> you can expose plugin entry points for automatic discovery. E.g. to expose an app and a package, you would add the following to <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyapp = \"nomad_example.parsers:myapp\"\nmypackage = \"nomad_example.schema_packages:mypackage\"\n</code></pre> <p>Here it is important to use the <code>nomad.plugin</code> group name in the <code>project.entry-points</code> header. The value on the right side (<code>\"nomad_example.schema_packages:mypackage\"</code>) must be a path pointing to a plugin entry point instance inside the python code. This unique key will be used to identify the plugin entry point when e.g. accessing it to read some of it's configuration values. The name on the left side (<code>mypackage</code>) can be set freely.</p> <p>You can read more about how to write different types of entry points in their dedicated documentation pages or learn more about the Python entry point mechanism.</p>"},{"location":"explanation/plugin_system.html#plugin-configuration","title":"Plugin configuration","text":"<p>The plugin entry point configuration is an instance of a <code>pydantic</code> model. This base model may already contain entry point-specific fields (such as the file extensions that a parser plugin will match) but it is also possible to extend this model to define additional fields that control your plugin behaviour.</p> <p>Here is an example of a new plugin entry point configuration class and instance for a parser, that has a new custom <code>parameter</code> configuration added as a <code>pydantic</code> <code>Field</code>:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import ParserEntryPoint\n\n\nclass MyParserEntryPoint(ParserEntryPoint):\n    parameter: int = Field(0, description='Config parameter for this parser.')\n\nmyparser = MyParserEntryPoint(\n    name = 'MyParser',\n    description = 'My custom parser.',\n    mainfile_name_re = '.*\\.myparser',\n)\n</code></pre> <p>The plugin entry point behaviour can be controlled in <code>nomad.yaml</code> using <code>plugins.entry_points.options</code>:</p> <pre><code>plugins:\n  entry_points:\n    options:\n      \"nomad_example.parsers:myparser\":\n        parameter: 47\n</code></pre> <p>Note that the model will also validate the values coming from <code>nomad.yaml</code>, and you should utilize the validation mechanisms of <code>pydantic</code> to provide users with helpful messages about invalid configuration.</p>"},{"location":"explanation/plugin_system.html#plugin-resource","title":"Plugin resource","text":"<p>The configuration class has a <code>load</code> method that returns the entry point resource. This is typically an instance of a class, e.g. <code>Parser</code> instance in the case of a parser entry point. Here is an example of a <code>load</code> method for a parser:</p> <pre><code>class MyParserEntryPoint(ParserEntryPoint):\n\n    def load(self):\n        from nomad_example.parsers.myparser import MyParser\n\n        return MyParser(**self.dict())\n</code></pre> <p>Often when loading the resource, you will need access to the final entry point configuration defined in <code>nomad.yaml</code>. This way also any overrides to the plugin configuration are correctly taken into account. You can get the final configuration using the <code>get_plugin_entry_point</code> function and the plugin name as defined in <code>pyproject.toml</code> as an argument:</p> <pre><code>from nomad.config import config\n\nconfiguration = config.get_plugin_entry_point('nomad_example.parsers:myparser')\nprint(f'The parser parameter is: {configuration.parameter}')\n</code></pre>"},{"location":"explanation/plugin_system.html#entry-point-discovery","title":"Entry point discovery","text":"<p>Entry points are like pre-defined connectors or hooks that allow the main system to recognize and load the code from plugins without needing to hard-code them directly into the platform. This mechanism enables the automatic discovery of plugin code. The following diagram illustrates how NOMAD interacts with the entry points in a plugin:</p> <p><pre><code>%%{init:{'sequence':{'mirrorActors': false}}}%%\nsequenceDiagram\n  autonumber\n  rect\n    NOMAD-&gt;&gt;Plugin: Request entry points matching the nomad.plugin group\n    Plugin--&gt;&gt;NOMAD: Return all entry point configurations\n  end\n    Note over NOMAD: Other tasks\n  rect\n    NOMAD-&gt;&gt;Plugin: Request a specific entry point resource\n    opt\n      Plugin-&gt;&gt;NOMAD: Request configuration overrides from nomad.yaml\n      NOMAD--&gt;&gt;Plugin: Return final configuration for entry point\n    end\n    Plugin--&gt;&gt;NOMAD: Return fully initialized entry point resource\n  end</code></pre> </p> NOMAD interaction with a plugin. <ol> <li>When NOMAD starts, it scans for plugin entry points defined under the <code>nomad.plugin</code> group in all of the Python packages that have been installed.</li> <li>The plugin returns all entry points that it has registered in <code>pyproject.toml</code> under the <code>nomad.plugin</code> group. This only loads the configuration, but does not yet load the resource, i.e. main Python implementation.</li> <li>When NOMAD needs to load the actual resource for an entry point (e.g. a parser), loads it by using the configuration instance.</li> <li>When the resource is being loaded, the entry point may ask for any configuration overrides that may have been set in <code>nomad.yaml</code>.</li> <li>NOMAD will return the final validated configuration that contains the default values and possible overrides.</li> <li>The plugin loads and returns the resource using the final configuration. This typically involves creating an instance of a specific class, e.g. <code>Parser</code> in the case of parser entry points.</li> </ol>"},{"location":"explanation/plugin_system.html#learn-how-to-write-plugins","title":"Learn how to write plugins","text":"<p>You can learn more about plugin development in the introduction to plugins -page.</p>"},{"location":"explanation/processing.html","title":"Processing","text":"<p>NOMAD extracts structured data from files via processing. Processing creates entries from files. It produces the schema-based structured data associated with each entry (i.e. the entry archives). To understand the role of processing in NOMAD also read the \"From file to data\" page.</p>"},{"location":"explanation/processing.html#processing-steps","title":"Processing steps","text":"<p>Processing comprises three steps.</p> <ol> <li>Matching files to parsers that can process them. This step also creates empty entries for matched files. Those matched files are now mainfiles and forever paired with the created entries.</li> <li>Parsing and normalizing to produce entry archives for matched mainfile/entry-pairs.</li> <li>Persisting (including indexing of) the extracted data.</li> </ol> <p> </p> Processing steps and how they interact with files, entries, and archives."},{"location":"explanation/processing.html#processing-triggers-scheduling-execution","title":"Processing triggers, scheduling, execution","text":"<p>For most end-users, processing is fully automated and will be automatically run when files are added or changed. Here, processing is triggered by the file upload API. However, in more advanced scenarios, other triggers might apply. The three possible triggers are:</p> <ul> <li>New files are uploaded or existing files are changed. This includes creating and updating ELN entries. The respective file upload API will call the processing.</li> <li>The processing of an upload is manually triggered, e.g. to re-process data. This might be done via the UI, the API, or CLI.</li> <li>The processing of one entry, programmatically triggers the processing of other files, e.g. in a <code>normalize</code> function.</li> </ul> <p>Most processing is done asynchronously. Entities that need processing are scheduled in a queue. Depending on the trigger, this might happen in the NOMAD app (i.a. via API), from a shell on a NOMAD server (i.e. via CLI), or from another processing in the NOMAD worker. Tasks in the queue are picked up by the NOMAD worker. See also the architecture documentation. The worker can process many entities from the queue concurrently.</p> <p>As an exception, entities can also be processed locally and synchronously. This means the processing is not done in the NOMAD worker, but where it is called. This is used for example, when an ELN is saved to have an immediate update on the respective entry with-in one API call.</p>"},{"location":"explanation/processing.html#processed-entities","title":"Processed entities","text":"<p>We differentiate two types of entities that can be processed: uploads and entries. The same entity can only be scheduled for processing, if it is not already scheduled or processing. See also the documentation \"from files to data\" to understand the relationships between all NOMAD entities.</p>"},{"location":"explanation/processing.html#uploads","title":"Uploads","text":"<p>Upload processing is scheduled if one or many files in the upload have changed. Upload processing includes the matching step, it creates new entries, and triggers the processing of new or afflicted entries. An upload is considered processing as long as any of its entries is still processing (or scheduled to be processed).</p>"},{"location":"explanation/processing.html#entries","title":"Entries","text":"<p>In most scenarios, entry processing is not triggered individually, but as part of an upload processing. Many entries of one upload might be processed at the same time. Some order can be enforced through processing levels. Levels are part of the parser metadata and entries paired to parsers with a higher level are processed after entries with a parser of lower level. See also how to write parsers.</p>"},{"location":"explanation/processing.html#customize-processing","title":"Customize processing","text":"<p>NOMAD provides just the framework for processing. The actual work depends on plugins, parsers and schemas for specific file types. While NOMAD comes with a build-in set of plugins, you can build your own plugins to support new file types, ELNs, and workflows.</p>"},{"location":"explanation/processing.html#schemas-parsers-plugins","title":"Schemas, parsers, plugins","text":"<p>The primary function of a parser is to systematically analyze and organize the incoming data, ensuring adherence to the established schema. The interaction between a parser and a schema plays a crucial role in ensuring data consistency to a predefined structure. It takes raw data inputs and utilizes the schema as a guide to interpret and organize the information correctly. By connecting the parser to the schema, users can establish a framework for the expected data structure. The modular nature of the parser and schema relationship allows for flexibility, as the parser can be designed to accommodate various schemas, making it adaptable to different data models or updates in research requirements. This process ensures that the resulting filled template meets the compliance standards dictated by the schema.</p> <p>Processing is run on the NOMAD (Oasis) server as part of the NOMAD app or worker. In principle, executed processing code can access all files, all databases, the underlying host system, etc. Therefore, only audited processing code can be allowed to run. For this reason, only code that is part of the NOMAD installation can be used and no user uploaded Python code is allowed.</p> <p>Therefore, the primary way to customize the processing and add more matching, parsing, and normalizing, is adding plugins to a NOMAD (Oasis) installation. However, NOMAD or NOMAD plugins can define base sections that use <code>normalize</code> functions that act on certain data or annotations. Uploaded <code>.yaml</code> schemas that use a such base section, might indirectly use custom processing functionality.</p> <p>A parser plugin can define a new parser and therefore add to the matching, parsing, (and normalizing). A schema plugin defines new sections that can contain <code>normalize</code> functions that add to the normalizing. See also the how-tos on the development of parsers and schemas.</p>"},{"location":"explanation/processing.html#matching","title":"Matching","text":"<p>All parsers have a <code>is_mainfile</code> function. This is its signature:</p> <pre><code>def is_mainfile(self, filename: str, ...) -&gt; Union[bool, Iterable[str]]\n</code></pre> <p>If this function does not return <code>False</code>, the parser matches with the file and entries are created. If the return is <code>True</code>, exactly one entry will be created. If the result is an iterable of strings, the same entry is sill created, but now also additional entries are created for each string. These strings are called entry keys and the additional entries are child entries. See the also single file, multiple entries scenario.</p> <p>In principle, the <code>is_mainfile</code> implementation can do whatever it wants: consider the filename, open the file, reading it partially, reading it whole, etc. However, most NOMAD parsers extend a specialized parser class called <code>MatchingParser</code>. These parsers share one implementation of <code>is_mainfile</code> that uses certain criteria, for example:</p> <ul> <li>regular expressions on filenames</li> <li>regular expressions on mimetypes</li> <li>regular expressions on header content</li> </ul> <p>See How to write a parser for more details.</p> <p>The matching step of an upload's processing, will call this function for every file and on all parsers. There are some hidden optimizations and additional parameters, but logically every parser is tested against every file, until a parser is matched or not. The first matched parser will be used and the order of configured parser is important. If no parser can be matched, the file is not considered for processing and no entry is created.</p>"},{"location":"explanation/processing.html#parsing","title":"Parsing","text":"<p>All parsers have a <code>parse</code> function. This is its signature:</p> <pre><code>def parse(self, mainfile: str, archive: EntryArchive, ...) -&gt; None\n</code></pre> <p>This function is called when a mainfile has already been matched and the entry has already been created with an empty archive. The entry mainfile and archive are passed as parameters.</p> <p>In the case of mainfile keys, an additional keyword argument is given: <code>child_archives: Dict[str, EntryArchive]</code>. Additional child entries have been created for each key, and this dictionary allows to populate the child archives.</p> <p>Each <code>EntryArchive</code> has an <code>m_context</code> field. This context provides functions to access the file system, open other files, open the archives of other entries, create or update files, spawn the processing of created or updated files. See also the create files, spawn entries scenario.</p>"},{"location":"explanation/processing.html#normalizing","title":"Normalizing","text":"<p>After parsing, entries are \"normalized\". We distinguish normalizers and <code>normalize</code> functions.</p> <p>Normalizers are Python classes. All normalizers are called for all archives. What normalizers are called and in what order is part of the NOMAD (Oasis) configuration. Normalizers have been developed to implement code independent processing steps that have to be applied to all entries in processing computational material science data. We are deprecating the use of normalizers and prefer the use of <code>normalize</code> functions.</p> <p>When you define a schema in Python, sections are defined as Python classes. These classes can have a function with the following signature:</p> <p><pre><code>def normalize(self, archive: EntryArchive, ...)\n</code></pre> These <code>normalize</code> functions can modify the given section instance <code>self</code> or the entry <code>archive</code> as a whole. Through the <code>archive</code>, Normalize functions also have access to the <code>context</code> and can do all kinds of file or processing operations from there. See also processing scenarios.</p> <p>Normalize functions are called in a particular order that follows a depth first traversal of the archives containment tree. If section definition classes inherit from each other, it is important to include respective <code>super</code> calls in the normalize function implementation.</p>"},{"location":"explanation/processing.html#processing-scenarios","title":"Processing scenarios","text":""},{"location":"explanation/processing.html#single-file-single-entry","title":"Single file, single entry","text":"<p>This is the \"normal\" case. A parser is matched to a mainfile. During processing, only the mainfile is read to populate the <code>EntryArchive</code> with data.</p>"},{"location":"explanation/processing.html#multiple-files-single-entry","title":"Multiple files, single entry","text":"<p>Same as above: a parser is matched to a mainfile. But, during processing, the mainfile and other files are read to populate the <code>EntryArchive</code> with data. Only the mainfile path is handed to the parser. Nothing, prevents it (or subsequent <code>normalize</code> functions) to open and read other auxiliary files.</p> <p>A notable special case are ELNs with <code>normalize</code> functions and references to files. ELNs can be designed to link the ELN with uploaded files via <code>FileEditQuantities</code> (see also How to define ELNs or ELN Annotations). The underlying ELN's schema usually defines <code>normalize</code> functions that open the referenced files for more data. Certain modes of the tabular parser, for example, use this.</p>"},{"location":"explanation/processing.html#single-file-multiple-entries","title":"Single file, multiple entries","text":"<p>A parser can match a mainfile in two ways. It's <code>is_mainfile</code> function can simply return <code>True</code>, meaning that the parser can parse this file. Or, it returns a list of keys. With the list of keys NOMAD will create an additional child entry for each key, and the parser's <code>parse</code> function is passed the archives of the additional child entries. One example is a parser for a tabular format that produces individual entries for each row of the table.</p> <p>Since <code>normalize</code> functions are defined as part of <code>Sections</code> in a schema and called for section instances, the <code>normalize</code> functions are called individually for the entry and all child entries.</p> <p>Normally the IDs of entries are computed from the upload ID and the mainfile path. For entries created from a mainfile and a key, the key is also included in the ID. Also here the entry identity is internally locked to the mainfile (and the respective key).</p>"},{"location":"explanation/processing.html#creating-files-spawning-entries","title":"Creating files, spawning entries","text":"<p>During processing, parsers or <code>normalize</code> functions can also create new (or update existing) files. And the processing might also trigger processing of these freshly created (or updated) files.</p> <p>For examples an ELN might be used to link multiple files of proprietary type. A <code>normalize</code> function can mix the data from those files with data from the ELN itself to convert everything into a file with a more standardized format. In experimental settings, this allows to merge proprietary instrument output and manual notes into a standardized representation of a measurement. The goal is not to fill an archive with all data directly, but have the created file parsed and create an entry with all data this way.</p> <p>Another use-case is automation. The <code>normalize</code> function of on ELN schema might use user input to create many more ELNs. This can be use-ful for parameter studies, where one experiment has to be repeated many times in a consistent way.</p>"},{"location":"explanation/processing.html#re-processing","title":"Re-processing","text":"<p>Typically a new file is uploaded, an entry created, and processed. But, we also need to re-processing entries because either the file was changed (e.g. an ELN was changed and saved or a new version of a file was uploaded), or because the parser or schema (including <code>normalize</code> functions) was changed. While the first case, usually triggers automated re-processing, the later case required manual intervention.</p> <p>Either the user manually re-processes an upload from the UI, because they know that they changed a respective schema for example, or the NOMAD (Oasis) admin re-processes (all) uploads, e.g. via the CLI, because they know the NOMAD version or some plugin has changed.</p> <p>Depending on configuration, re-processing might only process existing entries, match for new entries, or remove entries that no longer match.</p>"},{"location":"explanation/processing.html#strategies-for-re-usable-processing-code","title":"Strategies for re-usable processing code","text":"<p>Processing takes files as input and creates entries with structured data. It is about transforming data from one representation into another. Sometime, different input representations have to be transformed into the same output. Sometimes, the same input representation has to be transformed to different outputs. Input and output representation might evolve over time and processing has to be adopted. To avoid combinatorial explosion and manage moving sources and targets, it might be wise to modularize processing code more than just having <code>is_mainfile</code>, <code>parse</code>, and <code>normalize</code> functions.</p>"},{"location":"explanation/processing.html#readers-different-input-similar-output","title":"Readers: different input, similar output","text":"<p>Consider different file formats that capture similar data. Sometimes it is even the same container format (e.g. JSON or yaml), but slightly different keys, numbers are scaled differently, units are different, etc.</p> <p>We could write multiple parser, but the part of the parser that populates the archive, would be very much the same, only the part that reads information from the files, would be different.</p> <p>It might be good to define a reader like this: <pre><code>def reader(mainfile: str) -&gt; Dict[str, Any]\n</code></pre></p> <p>and capture data in a flexible intermediate format.</p>"},{"location":"explanation/processing.html#writers-metadata-into-archives-data-into-files","title":"Writers: metadata into archives, data into files","text":"<p>Two readers that produce a dictionary with the same keys and compatible values would allow to re-use the same writer that populates an archive, e.g.:</p> <pre><code>def writer(data: Dict[str, Any], archive: EntryArchive)\n</code></pre> <p>Similar to multiple readers, we might also use multiple writers. In certain scenarios, we need to write read data both into an archive, but also into an additional file. For example, if data is large, we can only keep metadata in archives, the actual data should be written to additional HFD5 or Nexus files.</p> <p>Separation of readers and writers also allows to re-use parser components in a <code>normalize</code> function. For example, if we consider the creating files, spawning entries scenario.</p>"},{"location":"explanation/processing.html#evolution-of-data-representations","title":"Evolution of data representations","text":"<p>If the input representation changes, only the readers have to change. If output representations change, only the writers have to be changed. If a new input is added, only a reader has to be developed. If a new output becomes necessary, only a new writer has to be developed.</p> <p>Since we use a flexible intermediate representation such as a <code>Dict[str, Any]</code>, it is easy to add new keys, but it is also dangerous because no static or semantic checking occurs. Readers and writers might be a good strategy to re-use parts of a parser, but reader and writer implementations remain strongly connected and inter-dependent.</p> <p> </p> Read and written data items might overlap but are rarely the same. <p>It is also important to note, that when using multiple readers and writers, the data items (i.e. keys) put into and read from the intermediate data structure might overlap, but are not necessarily the same sets of items. One reader might read more than another, the data put into an archive (metadata) might be different, from what is put into an HDF5 file (data).</p>"},{"location":"howto/overview.html","title":"NOMAD How-to guides","text":""},{"location":"howto/overview.html#users","title":"Users","text":"<p>These how-to guides target NOMAD users and cover data management, exploration, analysis with NOMAD graphical web-interface and APIs.</p>"},{"location":"howto/overview.html#manage-and-find-data","title":"Manage and find data","text":"<p>Use NOMAD to manage, explore, and analyze data.</p> <ul> <li>Upload and publish data for supported formats</li> <li>Use ELNs</li> <li>Explore data</li> <li>Use NORTH</li> </ul>"},{"location":"howto/overview.html#programmatic-use","title":"Programmatic use","text":"<p>Use NOMAD's functions programmatically and via its APIs.</p> <ul> <li>Use the API</li> <li>Publish data using python</li> <li>Install nomad-lab</li> <li>Access processed data</li> <li>Transform data</li> </ul>"},{"location":"howto/overview.html#data-stewards-administrators-and-developers","title":"Data stewards, administrators, and developersOne last thing","text":"<p>These how-to guides allow advanced users, NOMAD administrators, data stewards, and developers to customize and operate NOMAD and NOMAD Oasis or contribute to NOMAD's development.</p> <p>If you can't find what you're looking for in our guides, contact our team for personalized help and assistance. Don't worry, we're here to help and learn what we're doing wrong!</p>"},{"location":"howto/overview.html#nomad-oasis-self-hosting","title":"NOMAD Oasis \u2014 self-hosting","text":"<p>Self-hosting NOMAD for your lab or institution.</p> <ul> <li>Configure an Oasis</li> <li>Deploy an Oasis</li> <li>Update an Oasis</li> <li>Perform admin tasks</li> </ul>"},{"location":"howto/overview.html#plugins","title":"Plugins","text":"<p>Learn how to write NOMAD plugins.</p> <ul> <li>Introduction to plugins</li> <li>Write an API</li> <li>Write an app</li> <li>Write an example upload</li> <li>Write a normalizer</li> <li>Write a parser</li> <li>Write a schema packages</li> </ul>"},{"location":"howto/overview.html#customization","title":"Customization","text":"<p>Customize NOMAD and tailor NOMAD Oasis.</p> <ul> <li>Write a schema</li> <li>Define ELNs</li> <li>Use base sections</li> <li>Parse tabular data</li> <li>Define workflows</li> <li>Work with units</li> <li>Use HDF5 to handle large quantities</li> <li>Use Mapping parser to write data on archive</li> </ul>"},{"location":"howto/overview.html#development","title":"Development","text":"<p>Become a NOMAD developer and contribute to the source code.</p> <ul> <li>Get started</li> <li>Navigate the code</li> <li>Contribute</li> <li>Extend the search</li> </ul>"},{"location":"howto/customization/base_sections.html","title":"How to use base sections","text":"<p>As previously mentioned in How to write a schema, base sections can be used when writing custom schemas to inherit properties and functionality from already defined sections. Here we explain the properties and functionality of specific base sections and how they can be used.</p>"},{"location":"howto/customization/base_sections.html#datamodelmetainfobasesections","title":"<code>datamodel.metainfo.basesections</code>","text":"<p>This built-in nomad module contains a set of base sections based on an entity-activity model.</p> <p>Info</p> <p>In this part of the documentation we use UML Class diagrams to illustrate the inheritance, composition and association between the base sections. For more information on UML Class diagrams please see en.wikipedia.org/wiki/Class_diagram.</p> <p></p> <p>All the base sections defined in this model are abstract in the sense that they cannot be instantiated in NOMAD directly. Instead, the user is expected to implement these in their own schemas by inheriting a base section and <code>nomad.datamodel.EntryData</code>. Furthermore, it is strongly encouraged to use the most specialized section applicable.</p> <p>Example</p> <p>If the user is writing a schema for an instrument in their lab, they should ideally inherit from <code>Instrument</code> (and <code>EntryData</code>) rather than directly from <code>Entity</code> or <code>BaseSection</code>.</p> <p>All sections that are intended to eventually become entries in NOMAD by inheriting from the entity-activity base sections inherit from <code>BaseSection</code>. This section provides a set of global quantities that provides basic information about the entry. Theses are:</p> <ul> <li><code>name</code>: A short human readable and descriptive name.</li> <li><code>datetime</code>: The date and time associated with this section.</li> <li><code>lab_id</code>: An ID string that is unique at least for the lab that produced this data.</li> <li><code>description</code>: Any information that cannot be captured in the other fields.</li> </ul>"},{"location":"howto/customization/base_sections.html#entity","title":"<code>Entity</code>","text":"<p>Info</p> <p>By \"Entity\" we mean:</p> <pre><code>\"An object that persists, endures, or continues to exist through time while maintaining\nits identity.\"\n\nSee [BFO_0000002](http://purl.obolibrary.org/obo/BFO_0000002){:target=\"_blank\"} for semantic context.\n</code></pre> <p>The <code>Entity</code> section is currently subclassed by <code>System</code>, <code>Collection</code> and <code>Instrument</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#collection","title":"<code>Collection</code>","text":"<p>The <code>Collection</code> section should be inherited when attempting to group entities together.</p> <p></p> <p>Example</p> <p>The user wants to write a data schema for a batch of substrates. As this is grouping entities together, they should inherit from <code>Collection</code>.</p>"},{"location":"howto/customization/base_sections.html#entityreference","title":"<code>EntityReference</code>","text":"<p>The <code>EntityReference</code> section can be composed in any <code>Activity</code> (see <code>Activity</code> below) to provide a reference to an <code>Entity</code>. The section contains the following quantities:</p> <ul> <li><code>reference</code>: A reference to a NOMAD <code>Entity</code> entry.</li> <li><code>lab_id</code>: The readable identifier for the entity.</li> <li><code>name</code>: A short descriptive name for the role of this reference   (inherited from <code>SectionReference</code>).</li> </ul> <p>The normalizer for the <code>EntityReference</code> class will attempt to fill the <code>reference</code> from the <code>lab_id</code> or vice versa.</p>"},{"location":"howto/customization/base_sections.html#instrument","title":"<code>Instrument</code>","text":"<p>The <code>Instrument</code> section should be inherited when describing any tools used for material creation or characterization.</p> <p></p>"},{"location":"howto/customization/base_sections.html#system","title":"<code>System</code>","text":"<p>The main <code>Entity</code> section is <code>System</code> which is intended to cover any material system from atomic to device scale. This section adds the property <code>elemental_composition</code> which is a repeating subsection of <code>ElementalComposition</code> sections. Each <code>elemental_composition</code> section keeps track of a single element and its atomic fraction within the system.</p> <p>There are two specializations of <code>System</code> which differentiates between the theoretical concept of a pure material, <code>PureSubstance</code>, and an actual physical material combining several pure substances, <code>CompositeSystem</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#pubchempuresubstancesection","title":"<code>PubChemPureSubstanceSection</code>","text":"<p>This is a specialization of the <code>PureSubstanceSection</code> which will automatically search the PubChem database for additional information about the substance. If a PubChem CID is specified the details are retrieved directly. Otherwise a search query is made for the filled attributes in the following order:</p> <ol> <li><code>smile</code></li> <li><code>canonical_smile</code></li> <li><code>inchi_key</code></li> <li><code>iupac_name</code></li> <li><code>name</code></li> <li><code>molecular_formula</code></li> <li><code>cas_number</code></li> </ol>"},{"location":"howto/customization/base_sections.html#activity","title":"<code>Activity</code>","text":"<p>Info</p> <p>By \"Activity\" we mean:</p> <pre><code>\"An action that has a temporal extension and for some time depends on some entity.\"\n\nSee [BFO_0000015](http://purl.obolibrary.org/obo/BFO_0000015){:target=\"_blank\"} for semantic context.\n</code></pre> <p>The <code>Activity</code> section is currently subclassed by <code>Process</code>, <code>Measurement</code>, <code>Analysis</code>, and <code>Experiment</code>. These subclasses are intended to cover all types of activities and should be used instead of inheriting directly from <code>Activity</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#experiment","title":"<code>Experiment</code>","text":"<p>The <code>Experiment</code> section should be inherited when attempting to group activities together.</p> <p></p> <p>Example</p> <p>In a sample centric view the activities are grouped together by the sample but if the researcher is instead interested in an experiment containing activities on multiple samples, the <code>Experiment</code> section can be inherited to group these together.</p>"},{"location":"howto/customization/base_sections.html#process","title":"<code>Process</code>","text":"<p>Info</p> <p>By \"Process\" we mean:</p> <pre><code>\"A planned process which results in physical changes in a specified input material.\n[ obi : prs obi : mc obi : fg obi : jf obi : bp ]\n\nSynonyms:\n\n - preparative method\n - sample preparation\n - sample preparative method\n - material transformations\"\n\nSee [OBI_0000094](http://purl.obolibrary.org/obo/OBI_0000094){:target=\"_blank\"} for semantic context.\n</code></pre> <p>The <code>Process</code> section is the base for the <code>SynthesisMethod</code> section which in turn is specialized further in the <code>nomad-material-processing</code> plugin detailed below. The main feature of the <code>Process</code> section is that it adds <code>ProcessSteps</code> with a duration.</p> <p></p> <p>Info</p> <p>By \"SynthesisMethod\" we mean:</p> <pre><code>\"A method used to synthesise a sample.\"\n\nSee [CHMO_0001301](http://purl.obolibrary.org/obo/CHMO_0001301){:target=\"_blank\"} for semantic context.\n</code></pre>"},{"location":"howto/customization/base_sections.html#measurement","title":"<code>Measurement</code>","text":"<p>Info</p> <p>By \"Measurement\" we mean:</p> <pre><code>\"A planned process with the objective to produce information about the material entity\nthat is the evaluant, by physically examining it or its proxies. [ obi : pppb ]\"\n\nSee [OBI_0000070](http://purl.obolibrary.org/obo/OBI_0000070){:target=\"_blank\"} for semantic context.\n</code></pre> <p>The <code>Measurement</code> section adds <code>samples</code> which are references to instances of (subclasses of) <code>CompositeSystem</code>.</p> <p></p>"},{"location":"howto/customization/base_sections.html#analysis","title":"<code>Analysis</code>","text":"<p>Info</p> <p>By \"Analysis\" we mean:</p> <pre><code>\"A planned process that produces output data from input data.\n\nSynonyms:\n\n - data processing\n - data analysis\"\n\nSee [OBI_0200000](http://purl.obolibrary.org/obo/OBI_0200000){:target=\"_blank\"} for semantic context.\n</code></pre> <p>The <code>Analysis</code> section provides <code>inputs</code> which are references to any section (including sub sections) of some archive. In addition, it provides the <code>outputs</code> which is a repeating section of <code>AnalysisResult</code> which are intended to be further specialized by the user.</p> <p></p>"},{"location":"howto/customization/base_sections.html#readableidentifiers","title":"<code>ReadableIdentifiers</code>","text":"<p>This base sub section is meant to be composed into the entity-activity sections mentioned above to provide a standardized readable identifier.</p> <p>It is in turn composed by the following quantities:</p> <ul> <li><code>institute</code>: Alias/short name of the home institute of the owner, i.e. HZB.</li> <li><code>owner</code>: Alias for the owner of the identified thing. This should be unique within the   institute.</li> <li><code>datetime</code>: A datetime associated with the identified thing. In case of an <code>Activity</code>,   this should be the starting time and, in case of an <code>Entity</code>, the creation time.</li> <li><code>short_name</code>: A short name of the the identified thing (e.g. the identifier scribed on   the sample, the process number, or machine name), e.g. 4001-8, YAG-2-34.   This is to be managed and decided internally by the labs, although we recommend to avoid   the following characters in it: \"_\", \"/\", \"\\\" and \".\".</li> <li><code>lab_id</code>: Full readable id. Ideally a human readable id convention, which is simple,   understandable and still have chances of becoming unique.   If the <code>owner</code>, <code>short_name</code>, <code>\u00ecnstitute</code>, and <code>datetime</code> are provided, this will   be formed automatically by joining these components by an underscore (_).   Spaces in any of the individual components will be replaced with hyphens (-).   An example would be hzb_oah_20200602_4001-08.</li> </ul> <p>If owner is not filled the field will be filled by the first two letters of the first name joined with the first two letters of the last name of the author. If the institute is not filled a institute abreviations will be constructed from the author's affiliation. If no datetime is filled, the datetime will be taken from the <code>datetime</code> property of the parent, if it exists, otherwise the current date and time will be used. If no short name is filled, the name will be taken from the parent name, if it exists, otherwise it will be taken from the archive metadata entry name, if it exists, and finally if no other options are available it will use the name of the mainfile.</p> <p>Example</p> <p>The user has created a sample section by inheriting from <code>CompositeSystem</code> and <code>EntryData</code>. Now, the user wants to automatically generate a readable <code>lab_id</code> based on the logged in author. This can be accomplished by composing the <code>ReadableIdentifiers</code> section into the users sample section:</p> <pre><code>```python\nclass MySample(CompositeSystem, EntryData):\n    '''\n    A custom sample section.\n    '''\n    m_def = Section(\n        a_template=dict(\n            sample_identifiers=dict(),\n        ),\n    )\n    sample_identifiers = SubSection(\n        section_def=ReadableIdentifiers,\n    )\n```\n</code></pre>"},{"location":"howto/customization/base_sections.html#plugin-nomad-material-processing","title":"Plugin: <code>nomad-material-processing</code>","text":"<p>This plugin contains more specialized base sections for material processing, is maintained by FAIRmat and is currently hosted on https://github.com/FAIRmat-NFDI.</p>"},{"location":"howto/customization/basics.html","title":"How to write a YAML schema package","text":"<p>This guide explains how to write and upload NOMAD schema packages in the YAML format that can be uploaded as part of your data. This is a good way to start out experimenting with custom data structures in NOMAD, but for more advanced use cases you may need to use Python schema packages. For more information on how an archive file is composed, visit Explanation &gt; Data structure.</p>"},{"location":"howto/customization/basics.html#example-data","title":"Example data","text":"<p>Let's assume we want to describe chemical compositions using the elements they contain. The following structured data (in this example as a <code>.yaml</code> document) could describe the composition of water.</p> <pre><code>composition: H2O\nelements:\n- label: H\n  density: 8.375e-05\n  isotopes: [1, 2, 3]\n- label: O\n  density: 1.141\n  isotopes: [16, 17, 18]\n</code></pre> <p>In structured data formats (such as <code>.yaml</code> or <code>.json</code>), data is put into combinations of primitive values (e.g. <code>'H2O'</code>, <code>1.141</code>), objects (a set of keys and value pairs, where values can be objects, lists, or primitive values), and lists of values.</p>"},{"location":"howto/customization/basics.html#sections","title":"Sections","text":"<p>In a schema package, we want to describe the structure of data, i.e. what are the allowed combinations of objects, lists, and primitive values. The crucial task here is to define what keys certain types of objects can have and what possible values might exist for each of these keys.</p> <p>In NOMAD, we call objects sections and we define types of objects with section definitions. Since objects can be nested, sections become like the sections and subsections of a book or paper. Sections are a representation of data and they are the building blocks for archives. Section definitions form a schema package and they are the building blocks for the metainfo.</p> <p>In the above example, we have two types of objects: an overaching object for the entire structure (with keys for <code>composition</code> and <code>elements</code>), and an additional object which describes the internal structure of <code>elements</code> (with keys for <code>label</code>, <code>density</code>, and <code>isotopes</code>). Let's start with the definition for elements. This is what the section definition looks like in NOMAD's yaml-based schema package format:</p> <pre><code>Element:\n  quantities:\n    label:\n      type: str\n    density:\n      type: np.float64\n      unit: g/cm**3\n    isotopes:\n      type: int\n      shape: ['*']\n</code></pre> <p>A section definition provides all the available keys for a section that instantiates this definition. For each key, e.g. <code>label</code>, <code>density</code>, <code>isotopes</code>, it provides more information on the possible values.</p> <p>Let's have a look at the overall definition for our chemical composition:</p> <pre><code>Composition:\n  quantities:\n    composition:\n      type: str\n  sub_sections:\n    elements:\n      section: Element\n      repeats: true\n</code></pre> <p>Again, all possible keys (<code>composition</code> and <code>elements</code>) are defined. But now we see that there are two different types of keys, quantities and subsections. We say that section definitions can have properties (e.g. the keys they define) and there are two distinct types of properties.</p>"},{"location":"howto/customization/basics.html#quantities","title":"Quantities","text":"<p>Quantities define possible primitive values. The basic properties that go into a quantity definition are:</p> <ul> <li>type: what kind of primitive value can be used, e.g. <code>str</code> or <code>np.float64</code></li> <li>shape: what is the shape of the value, e.g. scalar or list (<code>['*']</code>)</li> <li>unit: what is the physical meaning of the value</li> </ul> <p>The names of quantity definitions serve as the key, used in respective section objects.</p>"},{"location":"howto/customization/basics.html#type","title":"Type","text":"<p>This is a list of supported quantity types.</p> type description <code>string</code> <code>str</code> <code>float</code> <code>integer</code> <code>int</code> <code>boolean</code> <code>bool</code> <code>np.int32</code> Numpy based integer with 32 bits. <code>np.int64</code> Numpy based integer with 64 bits. <code>np.float32</code> Numpy based float with 32 bits. <code>np.float64</code> Numpy based float with 64 bits. <code>Datetime</code> <code>User</code> A type for NOMAD users as values. <code>Author</code> A complex type for author information. <code>{type_kind: Enum, type_data: []}</code> Use <code>type_data</code> to specify enum values as list of strings. <code>*&lt;section name&gt;*</code> To define a quantity that is a reference to a specific section."},{"location":"howto/customization/basics.html#shape","title":"Shape","text":"<p>The shape of a quantity is a list of dimensions, where each dimension defines the possible size of that dimension. The empty list (or no shape) describes a scalar value, a list with one dimension a list or vector, a list with two dimensions a matrix, etc.</p> <p>Dimensions can be given as:</p> <ul> <li>an integer number to define a fixed size, e.g. a 3x3 matrix would have shape <code>[3, 3]</code>.</li> <li>the string <code>'*'</code> to denote am arbitrary sized dimension, e.g. a list quantity would have shape <code>['*']</code>.</li> <li>A string that describes the name of a sibling quantity with an integer type, e.g. <code>['number_of_atoms', 3]</code></li> </ul>"},{"location":"howto/customization/basics.html#unit","title":"Unit","text":"<p>NOMAD manages units and data with units via the Pint Python package. A unit is given as a string that is parsed by pint. These strings can be simple units (or their aliases) or complex expressions. Here are a few examples: <code>m</code>, <code>meter</code>, <code>mm</code>, <code>millimeter</code>, <code>m/s</code>, <code>m/s**2</code>.</p> <p>While you can use all kinds of units in your uploaded schema packages, the built-in NOMAD schema (Metainfo) uses only SI units.</p>"},{"location":"howto/customization/basics.html#subsections","title":"Subsections","text":"<p>Subsections define a part-of-relationship between two sections. Subsection definitions are properties of the parent section definition and name a child section definition. In the data, we can now contain instances of the target (e.g. <code>Element</code>) in instances of the source (e.g. <code>Composition</code>). A subsection can be defined as repeating to allow many child sections of the same type. In our example, one <code>Composition</code> can contain many <code>Elements</code>.</p> <p>The names of subsection definitions serve as the key, used in respective section objects.</p>"},{"location":"howto/customization/basics.html#uploading-schema-packages","title":"Uploading schema packages","text":"<p>NOMAD archive files allow you to upload data in NOMAD's native file format. An archive file can be a .yaml or .json file. It ends with <code>.archive.json</code> or <code>.archive.yaml</code>. Archive files are mainly used to convey data. Since YAML schema packages are also \"just\" data, archive files can also be used to convey a schema package.</p> <p>You can upload schema packages and data in separate files. <code>schema_package.archive.yaml</code> <pre><code>definitions:\n  sections:\n    Element:\n      quantities:\n        label:\n          type: str\n        density:\n          type: np.float64\n          unit: g/cm**3\n        isotopes:\n          type: int\n          shape: ['*']\n    Composition:\n      quantities:\n        composition:\n          type: str\n      sub_sections:\n        elements:\n          section: Element\n          repeats: true\n</code></pre></p> <p>and <code>data.archive.yaml</code> <pre><code>data:\n  m_def: '../upload/raw/package.archive.yaml#Composition'\n  composition: 'H2O'\n  elements:\n    - label: H\n      density: 0.00008375\n      isotopes: [1, 2, 3]\n    - label: O\n      density: 1.141\n      isotopes: [16, 17, 18]\n</code></pre></p> <p>Or, you can upload the schema package and data in the same file: <pre><code>definitions:\n  sections:\n    Element:\n      quantities:\n        label:\n          type: str\n        density:\n          type: np.float64\n          unit: g/cm**3\n        isotopes:\n          type: int\n          shape: ['*']\n    Composition:\n      quantities:\n        composition:\n          type: str\n      sub_sections:\n        elements:\n          section: Element\n          repeats: true\n\ndata:\n  m_def: Composition\n  composition: H2O\n  elements:\n  - label: H\n    density: 8.375e-05\n    isotopes: [1, 2, 3]\n  - label: O\n    density: 1.141\n    isotopes: [16, 17, 18]\n</code></pre></p>"},{"location":"howto/customization/basics.html#references","title":"References","text":""},{"location":"howto/customization/basics.html#reference-quantities","title":"Reference quantities","text":"<p>We already saw that we can define a part-of relationship between sections. When we want to represent highly inter-linked data, this is often insufficient. References allow us to create a more lose relationship between sections.</p> <p>A reference is a uni-directional link between a source section and a target section. References can be defined in a schema package as a quantity in the source section definition that uses the target section definition as a type.</p> <p>Instead of connecting the elements in a composition with subsections, we can also connect a composition section to elements with a quantity:</p> <pre><code>Composition:\n  quantities:\n    composition:\n      type: str\n    elements:\n      type: Element\n      shape: ['*']\n</code></pre> <p>Here, <code>type: Element</code> refers to the section definition <code>Element</code>, very similar to <code>section: Element</code> in a subsection definition.</p> <p>We saw above that subsections are represented as nested objects in data (forcing a part-of relationship). References are represented as string-typed primitive values in serialized data. Here is an example <code>Composition</code> with references to elements:</p> <pre><code>composition: H2O\nelements: ['#/data/periodic_table/elements/0', '#/data/periodic_table/elements/1']\n</code></pre> <p>These string-references determine the target section's place in the same archive. Each <code>/</code>-separated segment represents a key. A reference starts from the root object and following the sequence of keys to a specific object (i.e. section). Here is the full archive data:</p> <pre><code>data:\n periodic_table:\n   elements:\n   - label: H\n     density: 8.375e-05\n     isotopes: [1, 2, 3]\n   - label: O\n     density: 1.141\n     isotopes: [16, 17, 18]\n compositions:\n - composition: H2O\n   elements: ['#/data/periodic_table/elements/0', '#/data/periodic_table/elements/1']\n</code></pre> <p>If you follow the keys <code>data</code>, <code>periodic_table</code>, <code>elements</code>, <code>0</code>, you reach the section that represent hydrogen. Keep in mind that lists use index-numbers as keys.</p>"},{"location":"howto/customization/basics.html#schema-package-references","title":"Schema package references","text":"<p>References can look different depending on the context. Above we saw simple references that point from one data section to another. But, you also already a saw a different type of reference. Schema packages themselves contain references: when we used <code>type: Element</code> or <code>section: Element</code> to refer to a section definition, we were writing down references that point to a section definition. Here we can use a convenience representation: <code>Element</code> simply replaces the otherwise cryptic <code>#/definitions/sections/0</code>.</p> <p>So far, we never discussed the use of <code>m_def</code>. In the examples you might have seen this as a special key in some objects. Whenever we cannot determine the section definition for a section by its context (e.g. the key/subsection used to contain it in a parent section), we use <code>m_def</code> to provide a reference to the section definition.</p>"},{"location":"howto/customization/basics.html#different-forms-of-references","title":"Different forms of references","text":"<p>Depending on where references are used, they might take a different serialized form. Here are a few examples for different reference syntax:</p> Example reference Comments <code>#/data/periodic_table/elements/0</code> Reference to a section within the subsection hierarchy of the same archive. <code>Element</code> Reference to a section definition in the same archive. Can only be used to target section definitions. <code>nomad.datamodel.metainfo.workflow</code> Reference to a section definition that was written in Python and is part of the NOMAD code. Can only be used to target section definitions. <code>../upload/raw/data.archive.yaml#/data</code> Reference to a section in a different <code>.archive.yaml</code> file of the same upload. <code>../upload/archive/mainfile/data.archive.yaml#/data</code> Reference to a section in a processed archive given by entry mainfile. <code>../upload/archive/zxhS43h2kqHsVDqMboiP9cULrS_v#/data</code> Reference to a section in a processed archive given by entry-id. <code>../uploads/zxhS43h2kqHsVDqMboiP9cULrS_v/raw/data.archive.yaml#/data</code> Reference to a section in an entry of a different upload. <code>https://mylab.eu/oasis/api/v1/uploads/zxhS43h2kqHsVDqMboiP9cULrS_v/raw/data.archive.yaml#/data</code> Reference to a section in an entry in a different NOMAD installation."},{"location":"howto/customization/basics.html#references-across-entries","title":"References across entries","text":"<p>A references in the archive of one entry can point to a section in a different entry's archive. The following two example files, exemplify this use of reference between two NOMAD entries.</p> <p>periodic_table.archive.yaml <pre><code>definitions:\n  sections:\n    Element:\n      quantities:\n        label:\n          type: str\n        density:\n          type: np.float64\n          unit: g/cm**3\n        isotopes:\n          type: int\n          shape: ['*']\n    PeriodicTable:\n      sub_sections:\n        elements:\n          repeats: true\n          section: Element\ndata:\n  m_def: PeriodicTable\n  elements:\n  - label: H\n    density: 0.00008375\n    isotopes: [1, 2, 3]\n  - label: O\n    density: 1.141\n    isotopes: [16, 17, 18]\n</code></pre></p> <p>composition.archive.yaml <pre><code>definitions:\n  sections:\n    Composition:\n      quantities:\n        composition:\n          type: str\n        elements:\n          type: ../upload/raw/periodic_table.archive.yaml#Element\n          shape: ['*']\ndata:\n  m_def: Composition\n  composition: 'H2O'\n  elements:\n    - ../upload/raw/periodic_table.archive.yaml#data/elements/0\n    - ../upload/raw/periodic_table.archive.yaml#data/elements/1\n</code></pre></p> <p>These inter-entry references have two parts: <code>&lt;entry&gt;#&lt;section&gt;</code>, where entry is a path or URL denoting the target entry and section a path within the target entry's subsection containment hierarchy.</p> <p>Please note that also schema packages can be spread over multiple files. In the above example, one file contained the schema package and data for a periodic table and another file contained schema package and data for the composition of water (using the periodic table).</p>"},{"location":"howto/customization/basics.html#base-sections-and-inheritance","title":"Base sections and inheritance","text":"<p>We add a relationship between section definitions that allows us to create more specialized definitions from more abstract definitions. Here the properties of the abstract definition are inherited by the more specialized definitions</p>"},{"location":"howto/customization/basics.html#base-sections","title":"Base sections","text":"<p>Here is a simple schema package with two specialization of the same abstract section definition: <pre><code>definitions:\n  sections:\n    Process:\n      quantities:\n        time:\n          type: Datetime\n    Evaporation:\n      base_section: Process\n      quantities:\n        pressure:\n          type: np.float64\n          unit: Pa\n    Annealing:\n      base_section: Process\n      quantities:\n        temperature:\n          type: np.float64\n          unit: K\n</code></pre></p> <p>The two specialized definitions <code>Annealing</code> and <code>Evaporation</code> define the abstract definition <code>Process</code> via the <code>base_section</code> property. With this <code>Annealing</code> and <code>Evaporation</code> inherit the quantity <code>time</code>. We do not need to repeat quantities from the base section, and we can add more properties. Here is an example <code>Evaporation</code> using both the inherited and added quantity:</p> <pre><code>data:\n  m_def: Evaporation\n  time: '2022-10-13 12:00:00'\n  pressure: 100\n</code></pre>"},{"location":"howto/customization/basics.html#polymorphy","title":"Polymorphy","text":"<p>What happens if we reference abstract definitions in subsections or reference quantities? Here is an subsection example. In one schema, we define the relationship between <code>Sample</code> and <code>Process</code>. In another schema, we want to add more specializations to what a process is.</p> <p>abstract.archive.yaml <pre><code>definitions:\n  sections:\n    Process:\n      quantities:\n        time:\n          type: Datetime\n    Sample:\n      sub_sections:\n        processes:\n          section: Process\n          repeats: true\n</code></pre></p> <p>specialized.archive.yaml <pre><code>definitions:\n  sections:\n    Evaporation:\n      base_section: ../upload/raw/abstract.archive.yaml#Process\n      quantities:\n        pressure:\n          type: np.float64\n          unit: Pa\n    Annealing:\n      base_section: ../upload/raw/abstract.archive.yaml#Process\n      quantities:\n        temperature:\n          type: np.float64\n          unit: K\n</code></pre></p> <p>The section definition use in the subsection <code>processes</code> defines what a contained section has to be \"at least\". Meaning that any section based on a specialization of <code>Process</code> would be a valid <code>processes</code> subsection.</p> <p>specialized.archive.yaml <pre><code>definitions:\n  # see above\ndata:\n  m_def: ../upload/raw/abstract.archive.yaml#Sample\n  processes:\n  - m_def: Evaporation\n    time: '2022-10-13'\n    pressure: 100\n  - m_def: Annealing\n    time: '2022-10-13'\n    temperature: 342\n</code></pre></p> <p>The fact that a subsection or reference target can have different \"forms\" (i.e. based on different specializations) is called polymorphism in object-oriented data modelling.</p>"},{"location":"howto/customization/basics.html#pre-defined-sections","title":"Pre-defined sections","text":"<p>NOMAD provides a series of built-in section definitions. For example, there is <code>EntryArchive</code>, a definition for the top-level object in all NOMAD archives (e.g. <code>.archive.yaml</code> files). Here is a simplified except of the main NOMAD schema <code>nomad.datamodel</code>:</p> <pre><code>EntryArchive:\n  sub_sections:\n    metadata:\n      section: EntryMetadata\n    definitions:\n      section: nomad.metainfo.Package\n    data:\n      section: EntryData\n    # ... many more\nEntryData:\n  # empty\n</code></pre> <p>Compare this to the previous examples: we used the top-level keys <code>definitions</code> and <code>data</code> without really explaining why. Here you can see why. The <code>EntryArchive</code> property <code>definitions</code> allows us to put a schema package into our archives. And the <code>EntryArchive</code> property <code>data</code> allows us to put data into archives that is a specialization of <code>Schema</code>. The <code>Schema</code> definition is empty. It is merely an abstract placeholder that allows you to add specialized data sections to your archive. Therefore, all section definitions that define a top-level data section, should correctly use <code>nomad.datamodel.Schema</code> as a base section. This would be the first \"correct\" example:</p> <pre><code>definitions:\n  sections:\n    Greetings:\n      base_section: nomad.datamodel.EntryData\n      quantities:\n        message:\n          type: str\ndata:\n  m_def: MyData\n  message: Hello World\n</code></pre> <p>Here are a few other built-in section definitions and packages of definitions:</p> Section definition or package Purpose nomad.datamodel.EntryArchive Used for the root object of all NOMAD entries nomad.datamodel.EntryMetadata Used to add standard NOMAD metadata such as ids, upload, processing, or author information to entries. nomad.datamodel.EntryData An abstract section definition for the <code>data</code> section. nomad.datamodel.ArchiveSection Allows to put <code>normalize</code> functions into your section definitions. nomad.datamodel.metainfo.eln.* A package of section definitions to inherit commonly used quantities for ELNs. These quantities are indexed and allow specialization to utilize the NOMAD search. nomad.datamodel.metainfo.workflow.* A package of section definitions use by NOMAD to define workflows nomad.metainfo.* A package that contains all definitions of definitions, e.g. NOMAD's \"schema language\". Here you find definitions for what a sections, quantity, subsections, etc. is. nomad.parsing.tabular.TableData Allows to inherit parsing of references .csv and .xls files. See the detailed description to learn how to include this class and its annotations in a yaml schema. nomad.datamodel.metainfo.basesections.HDF5Normalizer Allows to link quantities to hdf5 dataset, improving performance for large data. This class and the related annotations are included in a yaml schema. Dedicated classes can be used to write a parser."},{"location":"howto/customization/basics.html#hdf5normalizer","title":"HDF5Normalizer","text":"<p>A different flavor of reading HDF5 files into NOMAD quantities is through defining a custom schema and inheriting <code>HDF5Normalizer</code> into base-sections. Two essential components of using <code>HDF5Normalizer</code> class is to first define a quantity that is annotated with <code>FileEditQuantity</code> field to enable one to drop/upload the <code>*.h5</code> file, and to define relevant quantities annotated with <code>path</code> attribute under <code>hdf5</code>. These quantities are then picked up by the normalizer to extract the values to be found denoted by the <code>path</code>. The supported <code>Hierarchical Data Format</code> file extensions are:</p> <ul> <li><code>.h5</code> </li> <li><code>.hdf5</code></li> <li><code>.he5</code></li> <li><code>.h5part</code></li> <li><code>.nxs</code></li> <li><code>.mat</code></li> <li><code>.nc4</code></li> </ul> <p>A minimum example to import your hdf5 and map it to NOMAD quantities is by using the following custom schema:</p> <pre><code>definitions:\n  name: 'hdf5'\n  sections:\n    Test_HDF5:\n      base_sections:\n        - 'nomad.datamodel.data.EntryData'\n        - 'nomad.datamodel.metainfo.basesections.HDF5Normalizer'\n      quantities:\n        datafile:\n          type: str\n          m_annotations:\n            eln:\n              component: FileEditQuantity\n        charge_density:\n          type: np.float32\n          shape: [ '*', '*', '*' ]\n          m_annotations:\n            hdf5:\n              path: '/path/to/charge_density'\n</code></pre>"},{"location":"howto/customization/basics.html#separating-data-and-schema-package","title":"Separating data and schema package","text":"<p>As we saw above, a NOMAD entry can contain schema package <code>definitions</code> and <code>data</code> at the same time. To organize your schema package and data efficiently, it is often necessary to re-use schema packages and certain data in other entries. You can use references to spread your schema packages and data over multiple entries and connect the pieces via references.</p> <p>Here is a simple schema package, stored in a NOMAD entry with mainfile name <code>package.archive.yaml</code>:</p> <pre><code> definitions:\n  sections:\n    Composition:\n      quantities:\n        composition:\n          type: str\n        base_composition:\n          type: Composition\n      sub_sections:\n        elements:\n          section: Element\n          repeats: True\n    Element:\n      quantities:\n        label:\n          type: str\n    Solution:\n      quantities:\n        solvent:\n          type: Composition\n      sub_sections:\n        solute:\n          section: Composition             \n</code></pre> <p>Now, we can re-use this schema package in many entries via references. Here, we extend a schema contained in the package and instantiate definitions is a separate mainfile <code>data-and-package.archive.yaml</code>:</p> <pre><code> definitions:\n  sections:\n    SpecialElement:\n      # Extending the definition from another entry\n      base_section: '../upload/raw/package.archive.yaml#Element'\n      quantities:\n        atomic_weight:\n          type: float\n          unit: 'g/mol'\ndata:\n  # Instantiating the definition from another entry\n  m_def: '../upload/raw/package.archive.yaml#Composition'\n  composition: 'H2O'\n  elements:\n    # Implicitly instantiate Element as defined for Composition.elements\n    - label: H\n    # Explicitly instantiate SpecialElement as a polymorph substitute\n    - m_def: SpecialElement\n      label: O\n      atomic_weight: 15.9994         \n</code></pre> <p>Here is a last example that re-uses the schema and references data from the two entries above:</p> <pre><code>definitions:\n  sections:\n    Composition:\n      quantities:\n        composition:\n          type: str\n        base_composition:\n          type: Composition\n      sub_sections:\n        elements:\n          section: Element\n          repeats: True\n    Element:\n      quantities:\n        label:\n          type: str\n    Solution:\n      quantities:\n        solvent:\n          type: Composition\n      sub_sections:\n        solute:\n          section: Composition\n</code></pre> <p>Attention</p> <p>You cannot create definitions that lead to circular loading of <code>*.archive.yaml</code> files. Each <code>definitions</code> section in an NOMAD entry represents a schema package. Each schema package needs to be fully loaded and analyzed before it can be used by other schema packages in other entries. Therefore, two schema packages in two entries cannot reference each other.</p>"},{"location":"howto/customization/basics.html#conventions","title":"Conventions","text":""},{"location":"howto/customization/basics.html#conventions-for-labels","title":"Conventions for labels","text":"<p>When assigning labels within your codebase, it's essential to follow consistent naming conventions for clarity and maintainability. The following guidelines outline the conventions for labeling different elements:</p> <ul> <li> <p>Sections: Labels for sections should adhere to Python convention of CapitalizedCamelCase. This means that each word in the label should begin with a capital letter, and there should be no spaces between words. For example: <code>SectionLabelOne</code>, <code>SectionLabelTwo</code>.</p> </li> <li> <p>Quantities and Subsections: Labels for quantities and subsections should be in lower_case. This convention involves writing all lowercase letters and separating words with whitespace. Abbreviations within these labels may be capitalized to enhance scientific readability. For example: <code>quantity label</code>, <code>subsection label</code>, <code>IV label</code>.</p> </li> </ul>"},{"location":"howto/customization/elns.html","title":"How to define and use ELNs in NOMAD","text":""},{"location":"howto/customization/elns.html#schemas-for-elns","title":"Schemas for ELNs","text":"<p>A schema defines all possible data structures. With small additions to our schemas, we can instruct NOMAD to provide respective editors for data. This allows us to build Electronic Lab Notebooks (ELNs) as tools to acquire data in a formal and structured way. For schemas with ELN annotations, users can create new entries in the NOMAD repository and edit the archive (structured data) of these entries directly in the GUI.</p>"},{"location":"howto/customization/elns.html#annotations","title":"Annotations","text":"<p>Definitions in a schema can have annotations. With these annotations you can provide additional information that NOMAD can use to alter its behavior around these definitions. Annotations are named blocks of key-value pairs:</p> <pre><code>definitions:\n  sections:\n    MyAnnotatedSection:\n      m_annotations:\n        annotation_name:\n          key1: value\n          key2: value\n</code></pre> <p>Many annotations control the representation of data in the GUI. This can be for plots or data entry/editing capabilities. There are three main categories of annotations relevant to ELNs. You find a reference of all annotations here.</p>"},{"location":"howto/customization/elns.html#example-eln-schema","title":"Example ELN schema","text":"<p>The is the commented ELN schema from our ELN example upload that can be created from NOMAD's upload page: <pre><code># Schemas can be defined as yaml files like this. The archive.yaml format will be\n# interpreted by nomad as a nomad archive. Therefore, all definitions have to be\n# put in a top-level section called \"definitions\"\ndefinitions:\n  # The \"definitions\" section is interpreted as a nomad schema package\n  # Schema packages can have a name:\n  name: 'Electronic Lab Notebook example schema'\n  # Schema packages contain section definitions. This is where the interesting schema\n  # information begins.\n  sections:\n    # Here we define a section called \"Chemical\":\n    Chemical:\n      # Section definition can have base_sections. Base sections are other schema\n      # definition and all properties of these will be inherited.\n      base_sections:\n        - 'nomad.datamodel.metainfo.eln.Chemical'  # Provides typical quantities like name, descriptions, chemical_formula and makes those available for search\n        - 'nomad.datamodel.data.EntryData'  # Declares this as a top-level entry section. This determines the types of entries you can create. With this we will be able to create a \"Chemical\" entry.\n      # All definitions, sections, sub_sections, quantities, can provide a description.\n      description: |\n        This is an example description for Chemical.\n        A description can contain **markdown** markup and TeX formulas, like $\\sum\\limits_{i=0}^{n}$.\n      # Sections define quantities. Quantities allow to manage actual data. Quantities\n      # can have various types, shapes, and units.\n      quantities:\n        # Here we define a quantity called \"from\"\n        form:\n          # This defines a Enum type with pre-defined possible values.\n          type:\n            type_kind: Enum\n            type_data:\n              - crystalline solid\n              - powder\n          # Annotations allow to provide additional information that is beyond just defining\n          # the possible data.\n          m_annotations:\n            # The eln annotation allows add the quantity to a ELN\n            eln:\n              component: EnumEditQuantity  # A form field component for EnumQuantities that uses a pull down menu.\n        cas_number:\n          type: str\n          m_annotations:\n            eln:\n              component: StringEditQuantity\n        ec_number:\n          type: str\n          m_annotations:\n            eln:\n              component: StringEditQuantity\n    Instrument:\n      base_sections:\n        - nomad.datamodel.metainfo.eln.Instrument\n        - nomad.datamodel.data.EntryData\n    Process:\n      base_section: nomad.datamodel.metainfo.eln.Process\n      quantities:\n        instrument:\n          type: Instrument\n          m_annotations:\n            eln:\n              component: ReferenceEditQuantity\n    Sample:\n      m_annotations:\n        # The template annotation allows to define what freshly created entries (instances of this schema) will look like.\n        # In this example we create a sample with an empty pvd_evaporation process.\n        template:\n          processes:\n            pvd_evaporation: {}\n      base_sections:\n        - 'nomad.datamodel.metainfo.eln.Sample'\n        - 'nomad.datamodel.data.EntryData'\n      quantities:\n        name:\n          type: str  # The simple string type\n          default: Default Sample Name\n          m_annotations:\n            eln:\n              component: StringEditQuantity  # A simple text edit form field\n        tags:\n          type:\n            type_kind: Enum\n            type_data:\n              - internal\n              - collaboration\n              - project\n              - other\n          shape: ['*']  # Shapes define non scalar values, like lists ['*'], vectors ['*', 3], etc.\n          m_annotations:\n            eln:\n              component: AutocompleteEditQuantity  # Allows to edit enums with an auto complete text form field\n        chemicals:\n          type: Chemical  # Types can also be other sections. This allows to reference a different section.\n          shape: ['*']\n          m_annotations:\n            eln:\n              component: ReferenceEditQuantity  # A editor component that allows to select from available \"Chemical\"s\n        substrate_type:\n          type:\n            type_kind: Enum\n            type_data:\n              - Fused quartz glass\n              - SLG\n              - other\n          m_annotations:\n            eln:\n              component: RadioEnumEditQuantity\n        substrate_thickness:\n          type: np.float64\n          unit: m\n          m_annotations:\n            eln:\n              component: NumberEditQuantity\n        sample_is_from_collaboration:\n          type: bool\n          m_annotations:\n            eln:\n              component: BoolEditQuantity\n      # Besides quantities, a section can define sub_sections. This allows hierarchies\n      # of information.\n      sub_sections:\n        # Here we define a sub_section of \"Sample\" called \"processes\"\n        processes:\n          section:\n            # The sub-section's section, is itself a section definition\n            m_annotations:\n              eln:  # adds the sub-section to the eln and allows users to create new instances of this sub-section\n            # We can also nest sub_sections. It goes aribitrarely deep.\n            sub_sections:\n              pvd_evaporation:\n                section:\n                  base_sections: ['Process', 'nomad.parsing.tabular.TableData', 'nomad.datamodel.metainfo.plot.PlotSection']\n                  m_annotations:\n                    # We can use the eln annotations to put the section to the overview\n                    # page, and hide unwanted inherited quantities.\n                    eln:\n                      overview: true\n                      hide: ['name', 'lab_id', 'description', 'method']\n                    # Plots are shown in the eln. Currently we only support simple x,y\n                    # line plots\n                    plotly_graph_object:\n                    - data:\n                        - x: \"#time\"\n                          y: \"#chamber_pressure\"\n                        - x: \"#time\"\n                          y: \"#substrate_temperature\"\n                          yaxis: y2\n                      layout:\n                        title:\n                          text: Pressure and Temperature over Time\n                        yaxis2:\n                          overlaying: y\n                          side: right\n                    - data:\n                        x: \"#time\"\n                        y: \"#chamber_pressure\"\n                    - data:\n                        x: \"#time\"\n                        y: \"#substrate_temperature\"\n                  quantities:\n                    data_file:\n                      type: str\n                      description: |\n                        A reference to an uploaded .csv produced by the PVD evaporation instruments\n                        control software.\n                      m_annotations:\n                        # The tabular_parser annotation, will treat the values of this\n                        # quantity as files. It will try to interpret the files and fill\n                        # quantities in this section (and sub_section) with the column\n                        # data of .csv or .xlsx files. There is also a mode option that by default, is set to column.\n                        tabular_parser:\n                          parsing_options:\n                            sep: '\\t'\n                            comment: '#'\n                        browser:\n                          adaptor: RawFileAdaptor  # Allows to navigate to files in the data browser\n                        eln:\n                          component: FileEditQuantity  # A form field that allows to drop and select files.\n                    time:\n                      type: np.float64\n                      shape: ['*']\n                      unit: s\n                      m_annotations:\n                        # The tabular annotation defines a mapping to column headers used in\n                        # tabular data files\n                        tabular:\n                          name: Process Time in seconds\n                    chamber_pressure:\n                      type: np.float64\n                      shape: ['*']\n                      unit: mbar\n                      m_annotations:\n                        eln:\n                          defaultDisplayUnit: mbar\n                        tabular:\n                          name: Vacuum Pressure1\n                    substrate_temperature:\n                      type: np.float64\n                      shape: ['*']\n                      unit: kelvin\n                      m_annotations:\n                        tabular:\n                          name: Substrate PV\n                          unit: degC\n              hotplate_annealing:\n                section:\n                  base_section: Process\n                  m_annotations:\n                    # We can use the eln annotations to put the section to the overview\n                    # page, and hide unwanted inherited quantities.\n                    eln:\n                      overview: true\n                      hide: ['name', 'lab_id', 'description']\n                  quantities:\n                    set_temperature:\n                      type: np.float64  # For actual numbers, we use numpy datatypes\n                      unit: K  # The unit system is based on Pint and allows all kinds of abreviations, prefixes, and complex units\n                      m_annotations:\n                        eln:\n                          component: NumberEditQuantity  # A component to enter numbers (with units)\n                    duration:\n                      type: np.float64\n                      unit: s\n                      m_annotations:\n                        eln:\n                          component: NumberEditQuantity\n</code></pre></p> <p>NOTE: Defining Labels for Quantities</p> <p>When defining labels for quantities, utilize the display annotations and ensure that you follow the conventions as described  here.</p>"},{"location":"howto/customization/hdf5.html","title":"How to use HDF5 to handle large quantities","text":"<p>The NOMAD schemas and processed data system are designed to describe and manage intricate hierarchies of connected data. This is ideal for metadata and lots of small data quantities, but does not work for large quantities. Quantities are atomic and are always managed as a whole; there is currently no functionality to stream or splice large quantities. Consequently, tools that produce or work with such data cannot scale.</p> <p>To address the issue, the option to use auxiliary storage systems optimized for large data is implemented. In the following we discuss two quantity types to enable the writing of large datasets to HDF5: <code>HDF5Reference</code> and <code>HDF5Dataset</code>. These are defined in <code>nomad.datamodel.hdf5</code>. Another class called <code>HDF5Normalizer</code> defined in <code>nomad.datamodel.metainfo.basesections</code> can be inherited and used directly in a yaml schema.</p>"},{"location":"howto/customization/hdf5.html#hdf5reference","title":"HDF5Reference","text":"<p>HDF5Reference is a metainfo quantity type intended to reference datasets in external raw HDF5 files. It is assumed that the dataset exists in an HDF5 file and the reference is assigned to this quantity. Static methods to read from and write to an HDF5 file are implemented. The following example illustrates how to use these.</p> <pre><code>from nomad.datamodel import ArchiveSection\nfrom nomad.datamodel.hdf5 import HDF5Reference\n\nclass LargeData(ArchiveSection):\n    value = Quantity(type=HDF5Reference)\n</code></pre> <p>The writing and reading of quantity values to and from an HDF5 file occur during processing. For illustration purposes, we mock this by creating <code>ServerContext</code>. Furthermore, we use this section definition for the <code>data</code> sub-section of EntryArchive.</p> <pre><code>import numpy as np\n\nfrom nomad.datamodel import EntryArchive, EntryMetadata\nfrom nomad.datamodel.context import ServerContext\nfrom nomad.files import StagingUploadFiles\nfrom nomad.processing import Upload\n\nupload_files = StagingUploadFiles(upload_id='test_upload', create=True)\nupload = Upload(upload_id='test_upload')\nupload_files.add_rawfiles('external.h5')\ncontext = ServerContext(upload=upload)\n\narchive = EntryArchive(\n    m_context=context,\n    metadata=EntryMetadata(upload_id=upload.upload_id, entry_id='test_entry'),\n    data=LargeData(),\n)\n\ndata = np.eye(3)\npath = 'external.h5#path/to/data'\nHDF5Reference.write_dataset(archive, data, path)\narchive.data.value = path\nHDF5Reference.read_dataset(archive, path)\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n</code></pre> <p>We use <code>write_dataset</code> to write our data into a raw HDF5 file in <code>test_upload</code> with the filename and dataset location in <code>path</code>. Additionally, archive is required to resolve the upload metadata. We then assign the reference to the dataset to <code>value</code>. To reference a file in another upload, follow the same form for reference values e.g. <code>/uploads/&lt;upload_id&gt;/raw/large_data.hdf5#group/large_field</code>.</p> <p>Important</p> <p>When reassigning a different value for an HDF5 archive quantity, it is necessary that the data attributes (shape and type) are preserved.</p> <p>To read a dataset, use <code>read_dataset</code> and provide a reference. This will return the value cast in the type of the dataset.</p>"},{"location":"howto/customization/hdf5.html#hdf5dataset","title":"HDF5Dataset","text":"<p>To use HDF5 storage for archive quantities, one should use <code>HDF5Dataset</code>.</p> <pre><code>from nomad.datamodel.hdf5 import HDF5Dataset\n\nclass LargeData(ArchiveSection):\n    value = Quantity(type=HDF5Dataset)\n</code></pre> <p>The assigned value will also be written to the archive HDF5 file and serialized as <code>/uploads/test_upload/archive/test_entry#/data/value</code>.</p> <p>To read the dataset, one shall use the context manager <code>with</code> to ensure the file is closed properly when done.</p> <pre><code>archive.data.value = np.ones(3)\n\nserialized = archive.m_to_dict()\nserialized['data']['value']\n# '/uploads/test_upload/archive/test_entry#/data/value'\n\ndeserialized = archive.m_from_dict(serialized, m_context=archive.m_context)\nwith deserialized.data.value as dataset:\n    print(dataset[:])\n# array([1., 1., 1.])\n</code></pre> <p>It is possible to assign to an archive quantity an array or another archive quantity. In the second case, the dataset created in the HDF5 file will contain a link and not a copy of the array:</p> <pre><code>from nomad.datamodel.hdf5 import HDF5Dataset\n\nclass LargeData(ArchiveSection):\n    value_1 = Quantity(type=HDF5Dataset)\n    value_2 = Quantity(type=HDF5Dataset)\n</code></pre> <pre><code>archive.data.value_1 = np.ones(3)\n\narchive.data.value_2 = archive.data.value_1\n</code></pre>"},{"location":"howto/customization/hdf5.html#visualizing-archive-hdf5-quantities","title":"Visualizing archive HDF5 quantities","text":"<p>NOMAD clients (e.g. NOMAD UI) can pick up on these HDF5 serialized quantities and provide respective functionality (e.g. showing a H5Web view).</p> <p> </p> Visualizing archive HDF5 reference quantity using H5Web. <p>When multiple quantities need to be displayed in the same plot, some attributes in the HDF5 file groups are needed, in order for h5web to be able to render a plot. The H5WebAnnotation class contains the attributes to be included in the groups (dataset) of HDF5 file, provided as section (quantity) annotations.</p> <p>In the following example, the <code>value</code> quantity has a dedicated default h5web rendering. Adding some annotation in the corresponding section would trigger another plot rendering, where <code>value</code> vs. <code>time</code> plot is shown. The errors to the dataset can be specified by <code>errors</code> annotation.</p> <pre><code>class MySection(ArchiveSection):\n\n    m_def = Section(a_h5web=H5WebAnnotation(axes='time', signal='value'))\n\n    value = Quantity(\n        type=HDF5Dataset,\n        unit='dimensionless',\n        shape=[],\n        a_h5web=H5WebAnnotation(\n            long_name='power',\n            errors='value_e'\n        ),\n    )\n    value_e = Quantity(\n        type=HDF5Dataset\n    )\n    time = Quantity(\n        type=HDF5Dataset,\n        unit='s'\n        shape=[]\n    )\n</code></pre> <p> </p> Including attributes to HDF5 groups to have composite plots using H5Web. <p>To include plots corresponding to sub sections, one can provide a list of the section paths to the annotation <code>paths</code>. The following example will trigger a rendering of the plot corresponding to the first <code>my_sub</code> section. One can also use wildcards <code>*</code> to include all sub sections and <code>**</code> to recursively search sub sections.</p> <pre><code>class MySubSection(ArchiveSection):\n\n    m_def = Section(a_h5web=H5WebAnnotation(axes='x', signal='y'))\n\n    x = Quantity(\n        type=HDF5Reference\n    )\n\n    y = Quantity(\n        type=HDF5Reference\n    )\n\nclass MySection(ArchiveSection):\n\n    m_def = Section(a_h5web=H5WebAnnotation(paths=['my_sub/0']))\n\n    my_sub = SubSection(sub_section=MySubSection, repeats=True)\n</code></pre>"},{"location":"howto/customization/hdf5.html#metadata-for-large-quantities","title":"Metadata for large quantities","text":"<p>Attention</p> <p>This will be implemented and documented soon.</p>"},{"location":"howto/customization/mapping_parser.html","title":"How to write data to archive with MappingParser","text":"<p><code>MappingParser</code> is a generic parser class implemented in <code>nomad.parsing.file_parser/mapping_parser.py</code> to handle the conversion to and from a data object and a python dictionary. We refer to an instance of the this class as 'mapping parser' throughout this section. In the following, the abstract properties and methods of the mapping parser are explained. The various implementations of the mapping parser are also defined and <code>Mapper</code> which is required to convert a mapping parser into another mapping parser is explained as well.</p>"},{"location":"howto/customization/mapping_parser.html#mappingparser","title":"MappingParser","text":"<p>The mapping parser has several abstract properties and methods and the most important ones are listed in the following:</p> <ul> <li><code>filepath</code>: path to the input file to be parsed</li> <li><code>data_object</code>: object resulting from loading the file in memory with <code>load_file</code></li> <li><code>data</code>: dictionary representation of <code>data_object</code></li> <li><code>mapper</code>: instance of <code>Mapper</code> required by <code>convert</code></li> <li><code>load_file</code>: method to load the file given by <code>filepath</code></li> <li><code>to_dict</code>: method to convert <code>data_object</code> into <code>data</code></li> <li><code>from_dict</code>: method to convert <code>data</code> into <code>data_object</code></li> <li><code>convert</code>: method to convert to another mapping parser</li> </ul> <p><code>data_object</code> can be an <code>XML</code> element tree or a <code>metainfo</code> section for example depending on the inheriting class. In order to convert a mapping parser to another parser, the target parser must provide a <code>Mapper</code> object. We refer to this simply as mapper throughout.</p> <p>In the following, we describe the currently implemented mapping parsers.</p>"},{"location":"howto/customization/mapping_parser.html#xmlparser","title":"XMLParser","text":"<p>This is mapping parser for XML files. It uses <code>lxml</code> to load the file as an element tree. The dictionary is generated by iteratively parsing the elements of the tree in <code>to_dict</code>. The values parsed from element <code>text</code> are automatically converted to a corresponding data type. If attributes are present, the value is wrapped in a dictionary with key given by <code>value_key</code> ('__value' by default) while the attribute keys are prefixed by <code>attribute_prefix</code> ('@' by default). The following XML:</p> <pre><code>&lt;a&gt;\n  &lt;b name='item1'&gt;name&lt;/b&gt;\n  &lt;b name='item2'&gt;name2&lt;/b&gt;\n&lt;/a&gt;\n</code></pre> <p>will be converted to:</p> <pre><code>    data = {\n      'a' : {\n        'b': [\n          {'@name': 'item1', '__value': 'name'},\n          {'@name': 'item2', '__value': 'name2'}\n        ]\n      }\n    }\n</code></pre> <p>The conversion can be reversed using the <code>from_dict</code> method.</p>"},{"location":"howto/customization/mapping_parser.html#hdf5parser","title":"HDF5Parser","text":"<p>This is the mapping parser for HDF5 files. It uses <code>h5py</code> to load the file as an HDF5 group. Similar to XMLParser, the HDF5 datasets are iteratively parsed from the underlying groups and if attributes are present these are also parsed. The <code>from_dict</code> method is also implemented to convert a dictionary into an HDF5 group.</p>"},{"location":"howto/customization/mapping_parser.html#metainfoparser","title":"MetainfoParser","text":"<p>This is the mapping parser for NOMAD archive files or metainfo sections. It accepts a schema root node annotated with <code>MappingAnnotation</code> as <code>data_object</code>. <code>create_mapper</code> generates the actual mapper as matching the <code>annotation_key</code>. If a <code>filepath</code> is specified, it instead falls back on the <code>ArchiveParser</code>. (Note: Under development, more info on the <code>ArchiveParser</code> to be added).  </p> <p>The annotation should always point to a parsed value via a <code>path</code> (JMesPath format). It may optionally specify a multi-argument <code>operator</code> for data mangling.   In this case, specify a tuple consisting of:</p> <ul> <li>the operator name, defined within the same scope.</li> <li>a list of paths with the corresponding values for the operator arguments.  </li> </ul> <p>Similar to <code>MSection</code>, it can be converted to (<code>to_dict</code>) or from (<code>from_dict</code>) a Python <code>dict</code>. Other attributes are currently accessible.</p> <pre><code>from nomad.datamodel.metainfo.annotations import Mapper as MappingAnnotation\n\nclass BSection(ArchiveSection):\n    v = Quantity(type=np.float64, shape=[2, 2])\n    v.m_annotations['mapping'] = dict(\n        xml=MappingAnnotation(mapper='.v'),\n        hdf5=MappingAnnotation(mapper=('get_v', ['.v[0].d'])),\n    )\n\n    v2 = Quantity(type=str)\n    v2.m_annotations['mapping'] = dict(\n        xml=MappingAnnotation(mapper='.c[0].d[1]'),\n        hdf5=MappingAnnotation(mapper='g.v[-2]'),\n    )\n\nclass ExampleSection(ArchiveSection):\n    b = SubSection(sub_section=BSection, repeats=True)\n    b.m_annotations['mapping'] = dict(\n        xml=MappingAnnotation(mapper='a.b1'), hdf5=MappingAnnotation(mapper='.g1')\n    )\n\nExampleSection.m_def.m_annotations['mapping'] = dict(\n    xml=MappingAnnotation(mapper='a'), hdf5=MappingAnnotation(mapper='g')\n)\n\nparser = MetainfoParser()\np.data_object = ExampleSection(b=[BSection()])\np.annotation_key = 'xml'\np.mapper\n# Mapper(source=Path(path='a'....\n</code></pre>"},{"location":"howto/customization/mapping_parser.html#converting-mapping-parsers","title":"Converting mapping parsers","text":"<p>The following is a sample python code to illustrate the mapping of the contents of an HDF5 file to an archive. First, we create a <code>MetainfoParser</code> object for the archive. The annotation key is set to <code>hdf5</code> which will generate a mapper from the <code>hdf5</code> annotations defined in the definitions. Essentially, only metainfo sections and quantities with the <code>hdf5</code> annotation will be mapped. The mapper will contain paths for the source (HDF5) and the target (archive). The archive is then set to the archive parser <code>data_object</code>. Here, the archive already contains some data which should be merged to data that will be parsed. Next, a parser for HDF5 data is created. We use a custom class of the <code>HDF5Parser</code> which implements the <code>get_v</code> method defined in <code>BSection.v</code> In this example, we do not read the data from the HDF5 file but instead generate it from a dictionary by using the <code>from_dict</code> method. By invoking the <code>convert</code> method, the archive parser data object is populated with the corresponding HDF5 data.</p> <pre><code>    class ExampleHDF5Parser(HDF5Parser):\n        @staticmethod\n        def get_v(value):\n            return np.array(value)[1:, :2]\n\n    archive_parser = MetainfoParser()\n    archive_parser.annotation_key = 'hdf5'\n    archive_parser.data_object = ExampleSection(b=[BSection(v=np.eye(2))])\n\n    hdf5_parser = ExampleHDF5Parser()\n    d = dict(\n        g=dict(\n            g1=dict(v=[dict(d=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))]),\n            v=['x', 'y', 'z'],\n            g=dict(\n                c1=dict(\n                    i=[4, 6],\n                    f=[\n                        {'@index': 0, '__value': 1},\n                        {'@index': 2, '__value': 2},\n                        {'@index': 1, '__value': 1},\n                    ],\n                    d=[dict(e=[3, 0, 4, 8, 1, 6]), dict(e=[1, 7, 8, 3, 9, 1])],\n                ),\n                c=dict(v=[dict(d=np.eye(3), e=np.zeros(3)), dict(d=np.ones((3, 3)))]),\n            ),\n        )\n    )\n    hdf5_parser.from_dict(d)\n\n    hdf5_parser.convert(archive_parser)\n\n    # &gt;&gt;&gt; archive_parser.data_object\n    # ExampleSection(b, b2)\n    # &gt;&gt;&gt; archive_parser.data_object.b[1].v\n    # array([[4., 5.],\n    #   [7., 8.]])\n</code></pre>"},{"location":"howto/customization/mapping_parser.html#mapper","title":"Mapper","text":"<p>A mapper is necessary in order to convert a mapping parser to a target mapping parser by mapping data from the source to the target. There are three kinds of mapper: <code>Map</code>, <code>Evaluate</code> and <code>Mapper</code> each inheriting from <code>BaseMapper</code>. A mapper has attributes source and target which define the paths to the source data and target, respectively. <code>Map</code> is intended for mapping data directly from source to target. The path to the data is given by the attribute <code>path</code>. <code>Evaluate</code> will execute a function defined by <code>function_name</code> with the arguments given by the mapped values of the paths in <code>function_args</code>. Lastly, <code>Mapper</code> allows the nesting of mappers by providing a list of mappers to its attribute <code>mapper</code>. All the paths are instances of <code>Path</code> with the string value of the path to the data given by the attribute <code>path</code>. The value of path should follow the jmespath specifications but could be prefixed by <code>.</code> which indicates that this is a path relative to the parent. This will communicate to the mapper which source to get the data.</p> <pre><code>    Mapper(\n        source=Path(path='a.b2', target=Path(path='b2'), mapper=[\n            Mapper(\n                source=Path(path='.c', parent=Path(path='a.b2')),\n                target=Path(path='.c', parent=Path(path='b2')), mapper=[\n                    Map(\n                        target=Path(\n                            path='.i', parent=Path(path='.c', parent=Path(path='b2'))\n                        ),\n                        path=Path(\n                            path='.d', parent=Path(path='.c' parent=Path(path='a.b2'))\n                        )\n                    ),\n                    Evaluate(\n                        target=Path(\n                            path='.g', parent=Path(path='.c', parent=Path(path='b2'))\n                        ),\n                        function_name='slice', function_args=[Path(path='a.b2.c.f.g.i')]\n                    )\n                ]\n            )\n        ),\n    )\n</code></pre>"},{"location":"howto/customization/tabular.html","title":"How to parse tabular data","text":"<p>Refer to the Reference guide for the full list of annotations connected to this parser and to the Tabular parser tutorial  for a detailed description of each of them.</p>"},{"location":"howto/customization/tabular.html#preparing-the-tabular-data-file","title":"Preparing the tabular data file","text":"<p>NOMAD and <code>Excel</code> support multiple-sheets data manipulations and imports. Each quantity in the schema will be annotated with a source path composed by sheet name and column header. The path to be used with the tabular data displayed below would be <code>Sheet1/My header 1</code> and it would be placed it the <code>tabular</code> annotation, see schema annotations.</p> <p> </p> <p>In the case there is only one sheet in the Excel file, or when using a <code>.csv</code> file that is a single-sheet format, the sheet name is not required in the path.</p> <p>The data sheets can be stored in one or more files depending on the user needs. Each sheet can independently be organized in one of the following ways:</p> <p>1) Columns:  each column contains an array of cells that we want to parse into one quantity. Example: time and temperature arrays to be plotted as x and y.</p> <p> </p> <p>2) Rows:  each row contains a set of cells that we want to parse into a section, i. e. a set of quantities. Example: an inventory tabular data file (for substrates, precursors, or more) where each column represents a property and each row corresponds to one unit stored in the inventory.</p> <p> </p> <p>3) Rows with repeated columns:</p> <p>in addition to the mode 2), whenever the parser detects the presence of multiple columns (or multiple sets of columns) with same headers, these are taken as multiple instances of a subsection. More explanations will be delivered when showing the schema for such a structure. Example: a crystal growth process where each row is a step of the crystal growth and the repeated columns describe the \"precursor materials\", that can be more than one during such processes and they are described by the same \"precursor material\" section.</p> <p> </p> <p>Furthermore, we can insert comments before our data, we can use a special character to mark one or more rows as comment rows. The special character is annotated within the schema in the <code>tabular</code> annotation, see schema annotations:</p> <p> </p>"},{"location":"howto/customization/tabular.html#inheriting-the-tabledata-base-section","title":"Inheriting the TableData base section","text":"<p><code>TableData</code> can be inherited adding the following lines in the yaml schema file:</p> <pre><code>MySection:\n  base_sections:\n    - nomad.datamodel.data.EntryData\n    - nomad.parsing.tabular.TableData\n</code></pre> <p><code>EntryData</code> is usually also necessary as we will create entries from the section we are defining. <code>TableData</code> provides a customizable checkbox quantity, called <code>fill_archive_from_datafile</code>, to turn the tabular parser <code>on</code> or <code>off</code>. To avoid the parser running everytime a change is made to the archive data, it is sufficient to uncheck the checkbox. It is customizable in the sense that if you do not wish to see this checkbox at all, you can configure the <code>hide</code> parameter of the section's <code>m_annotations</code> to hide the checkbox. This in turn sets the parser to run everytime you save your archive. To hide it, add the following lines:</p> <pre><code>MySection:\n  base_sections:\n    - nomad.datamodel.data.EntryData\n    - nomad.parsing.tabular.TableData\n  m_annotations:\n    eln:\n      hide: ['fill_archive_from_datafile']\n</code></pre> <p>Be cautious though! Turning on the tabular parser (or checking the box) on saving your data will cause losing/overwriting your manually-entered data by the parser!</p>"},{"location":"howto/customization/tabular.html#importing-data-in-nomad","title":"Importing data in NOMAD","text":"<p>After writing a schema file and creating a new upload in NOMAD (or using an existing upload), it is possible to upload the schema file. After creating a new Entry out of one section of the schema, the tabular data file must be dropped in the quantity designated by the <code>FileEditQuantity</code> annotation. After clicking save the parsing will start. In the Overview page of the NOMAD upload, new Entries are created and appended to the Processed data section. In the Entry page, clicking on DATA tab (on top of the screen) and in the Entry lane, the data is populated under the <code>data</code> subsection.</p>"},{"location":"howto/customization/tabular.html#hands-on-examples-of-all-tabular-parser-modes","title":"Hands-on examples of all tabular parser modes","text":"<p>In this section eight examples will be presented, containing all the features available in tabular parser. Refer to the Tutorial for more comments on the implications of the structures generated by the following yaml files.</p>"},{"location":"howto/customization/tabular.html#1-column-mode-current-entry-parse-to-root","title":"1. Column mode, current Entry, parse to root","text":"<p>The first case gives rise to the simplest data archive file. Here the tabular data file is parsed by columns, directly within the Entry where the <code>TableData</code> is inherited and filling the quantities in the root level of the schema (see dedicated how-to to learn how to inherit tabular parser in your schema).</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry of the parsed quantities.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data (<code>root</code> in this case).</li> <li>quantities parsed in <code>column</code> mode must have the <code>shape: ['*']</code> attribute, that means they are arrays and not scalars.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 1'\n  sections:\n    MySection1:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: current_entry\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#2-column-mode-current-entry-parse-to-my-path","title":"2. Column mode, current Entry, parse to my path","text":"<p>The parsing mode presented here only differs from the previous for the <code>sections</code> annotations. In this case the section that we want to fill with tabular data can be nested arbitrarily deep in the schema and the <code>sections</code> annotation must be filled with a forward slash path to the desired section, e. g. <code>my_sub_section/my_sub_sub_section</code>.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry of the parsed quantities.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>).</li> <li>quantities parsed in <code>column</code> mode must have the <code>shape: ['*']</code> attribute, that means they are arrays and not scalars.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 2'\n  sections:\n    MySection2:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: current_entry\n                  sections:\n                    - my_sub_section_2\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_sub_section_2:\n          section: '#/MySubSection2'\n    MySubSection2:\n      m_annotations:\n        eln:\n      quantities:\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#3-row-mode-current-entry-parse-to-my-path","title":"3. Row mode, current Entry, parse to my path","text":"<p>The current is the first example of parsing in row mode. This means that every row of the excel file while be placed in one instance of the section that is defined in <code>sections</code>. This section must be decorated with <code>repeats: true</code> annotation, it will allow to generate multiple instances that will be appended in a list with sequential numbers. Instead of sequential numbers, the list can show specific names if <code>label_quantity</code> annotation is appended to the repeated section. This annotation is included in the how-to example. The section is written separately in the schema and it does not need the <code>EntryData</code> inheritance because the instances will be grafted directly in the current Entry. As explained below, it is not possible for <code>row</code> and <code>current_entry</code> to parse directly in the root because we need to create multiple instances of the selected subsection and organize them in a list.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry of the parsed quantities.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reaches it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>).</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 3'\n  sections:\n    MySection3:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: current_entry\n                  sections:\n                    - my_repeated_sub_section_3\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_repeated_sub_section_3:\n          repeats: true\n          section: '#/MySubSection3'\n    MySubSection3:\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      quantities:\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#4-column-mode-single-new-entry-parse-to-my-path","title":"4. Column mode, single new Entry, parse to my path","text":"<p>One more step of complexity is added here: the parsing is not performed in the current Entry, but a new Entry it automatically generated and filled. This structure foresees a parent Entry where we collect one or more tabular data files and possibly other info while we want to separate a specific entity of our data structure in another searchable Entry in NOMAD, e. g. a substrate Entry or a measurement Entry that would be collected inside a parent experiment Entry. We need to inherit <code>SubSect</code> class from <code>EntryData</code> because these will be standalone archive files in NOMAD. Parent and children Entries are connected by means of the <code>ReferenceEditQuantity</code> annotation in the parent Entry schema. This annotation is attached to a quantity that becomes a hook to the other ones, It is a powerful tool that allows to list in the overview of each Entry all the other referenced ones, allowing to build paths of referencing available at a glance.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the parent Entry, the data is parsed in the child Entry.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>)</li> <li>quantities parsed in <code>column</code> mode must have the <code>shape: ['*']</code> attribute, that means they are arrays and not scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a NOMAD Entry archive file.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 4'\n  sections:\n    MySection4:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: single_new_entry\n                  sections:\n                    - my_subsection_4\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_subsection_4:\n          section:\n            m_annotations:\n              eln:\n            quantities:\n              my_ref_quantity:\n                type: '#/MySubSection4'\n                m_annotations:\n                  eln:\n                    component: ReferenceEditQuantity\n    MySubSection4:\n      base_sections:\n      - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      quantities:\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#5-row-mode-single-new-entry-parse-to-my-path","title":"5. Row mode, single new Entry, parse to my path","text":"<p>Example analogous to the previous, where the new created Entry contains now a repeated subsection with a list of instances made from each line of the tabular data file, as show in the Row mode, current Entry, parse to my path case.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the parent Entry, the data is parsed in the child Entry.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>)</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a NOMAD Entry archive file.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 5'\n  sections:\n    MySection5:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: single_new_entry\n                  sections:\n                    - my_subsection_5/my_repeated_sub_section\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_subsection_5:\n          section:\n            m_annotations:\n              eln:\n            quantities:\n              my_ref_quantity:\n                type: '#/MySubSection5'\n                m_annotations:\n                  eln:\n                    component: ReferenceEditQuantity\n    MySubSection5:\n      base_sections:\n      - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      sub_sections:\n        my_repeated_sub_section:\n          repeats: true\n          section:\n            quantities:\n              my_quantity_1:\n                type: str\n                m_annotations:\n                  tabular:\n                    name: \"My header 1\"\n              my_quantity_2:\n                type: str\n                m_annotations:\n                  tabular:\n                    name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#6-row-mode-multiple-new-entries-parse-to-root","title":"6. Row mode, multiple new entries, parse to root","text":"<p>The last feature available for tabular parser is now introduced: <code>multiple_new_entries</code>. It is only meaningful for <code>row</code> mode because each row of the tabular data file will be placed in a new Entry that is an instance of a class defined in the schema, this would not make sense for columns, though, as they usually need to be parsed all together in one class of the schema, for example the \"timestamp\" and \"temperature\" columns in a spreadsheet file would need to lie in the same class as they belong to the same part of experiment. A further comment is needed to explain the combination of this feature with <code>root</code>. As mentioned before, using <code>root</code> foresees to graft data directly in the present Entry. In this case, this means that a manyfold of Entries will be generated based on the only class available in the schema. These Entries will not be bundled together by a parent Entry but just live in our NOMAD Upload as a spare list. They might be referenced manually by the user with <code>ReferenceEditQuantity</code> in other archive files. Bundling them together in one overarching Entry already at the parsing stage would require the next and last example to be introduced.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the parent Entry, the data is parsed in the children Entries.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a NOMAD Entry archive file.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 6'\n  sections:\n    MySection6:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: multiple_new_entries\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#7-row-mode-multiple-new-entries-parse-to-my-path","title":"7. Row mode, multiple new entries, parse to my path","text":"<p>As anticipated in the previous example, <code>row</code> mode in connection to <code>multiple_new_entries</code> will produce a manyfold of instances of a specific class, each of them being a new Entry. In the present case, each instance will also automatically be placed in a <code>ReferenceEditQuantity</code> quantity lying in a subsection defined within the parent Entry, coloured in plum in the following example image.</p> <p>Important</p> <ul> <li><code>data_file</code> quantity, i.e. the tabular data file name, is located in the same Entry, the data is parsed in the children Entries.</li> <li>double check that <code>mapping_options &gt; sections</code> contains the right path. It should point to the (sub)section where the quantities are decorated with <code>tabular</code> annotation, i. e., the one to be filled with tabular data.</li> <li>the section to be parsed can be arbitrarily nested, given that the path provided in <code>sections</code> reachs it (e. g. <code>my_sub_sec/my_sub_sub_sec</code>)</li> <li>quantities parsed in <code>row</code> mode are scalars.</li> <li>inherit also the subsection from <code>EntryData</code> as it must be a standalone NOMAD archive file.</li> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 7'\n  sections:\n    MySection7:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: multiple_new_entries\n                  sections:\n                    - my_repeated_sub_section_7\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_repeated_sub_section_7:\n          repeats: true\n          section:\n            m_annotations:\n              eln:\n            quantities:\n              my_ref_quantity:\n                type: '#/MySubSection7'\n                m_annotations:\n                  eln:\n                    component: ReferenceEditQuantity\n    MySubSection7:\n      base_sections:\n      - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: 'my_quantity_1'\n      quantities:\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#8-the-sub-subsection-nesting-schema","title":"8. The Sub-Subsection nesting schema","text":"<p>If the tabular data file contains multiple columns with exact same name, there is a way to parse them using <code>row</code> mode. As explained in previous examples, this mode creates an instance of a subsection of the schema for each row of the file. Whenever column with same name are found they are interpreted as multiple instances of a sub-subsection nested inside the subsection. To build a schema with such a feature it is enough to have two nested classes, each of them bearing a <code>repeats: true</code> annotation. This structure can be applied to each and every of the cases above with <code>row</code> mode parsing.</p> <p>Important</p> <ul> <li>make use of <code>repeats: true</code> in the subsection within the parent section <code>MySection</code> and also in the sub-subsection within <code>MySubSect</code>.</li> <li><code>label_quantity</code> annotation uses a quantity as name of the repeated section. If it is not provided, a sequential number will be used for each instance.</li> </ul> <pre><code>definitions:\n  name: 'My test ELN 8'\n  sections:\n    MySection8:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: current_entry\n                  sections:\n                    - my_repeated_sub_section_8\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n      sub_sections:\n        my_repeated_sub_section_8:\n          repeats: true\n          section: '#/MySubSection8'\n    MySubSection8:\n      m_annotations:\n        eln:\n      more:\n        label_quantity: '#/data/my_quantity_1'\n      quantities:\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n      sub_sections:\n        my_repeated_sub_sub_section:\n          repeats: true\n          section:\n            more:\n              label_quantity: my_quantity_2\n            quantities:\n              my_quantity_2:\n                type: str\n                m_annotations:\n                  tabular:\n                    name: \"My header 2\"\n</code></pre>"},{"location":"howto/customization/tabular.html#9-not-possible-implementations","title":"9. Not possible implementations","text":"<p>Some combinations of <code>mapping_options</code>, namely <code>file_mode</code>, <code>mapping_mode</code>, and <code>sections</code>, can give rise to not interpretable instructions or not useful data structure. For the sake of completeness, a brief explanation of the five not possible cases will be provided.</p>"},{"location":"howto/customization/tabular.html#91-row-mode-current-entry-parse-to-root","title":"9.1 Row mode, current Entry, parse to root","text":"<p><code>row</code> mode always requires a section instance to be populated with one row of cells from the tabular data file. Multiple instances are hence generated from the rows available in the file. The instances are organized in a list and the list must be necessarily hosted as a subsection in some parent section. That's why, within the parent section, a path in <code>sections</code> must be provided different from <code>root</code>.</p>"},{"location":"howto/customization/tabular.html#92-column-mode-single-new-entry-parse-to-root","title":"9.2 Column mode, single new Entry, parse to root","text":"<p>This would create a redundant Entry with the very same structure of the one where the <code>data_file</code> quantity is placed, the structure would furthermore miss a reference between the two Entries. A better result is achieved using a path in <code>sections</code> that would create a new Entry and reference it in the parent one.</p>"},{"location":"howto/customization/tabular.html#93-row-mode-single-new-entry-parse-to-root","title":"9.3 Row mode, single new Entry, parse to root","text":"<p>As explained in the first section of not possible cases, when parsing in row mode we create multiple instances that cannot remain as standalone floating objects. They must be organized as a list in a subsection of the parent Entry.</p>"},{"location":"howto/customization/tabular.html#94-column-mode-multiple-new-entries-parse-to-root","title":"9.4 Column mode, multiple new entries, parse to root","text":"<p>This case would create a useless set of Entries containing one array quantity each. Usually, when parsing in column mode we want to parse together all the columns in the same section.</p>"},{"location":"howto/customization/tabular.html#95-column-mode-multiple-new-entries-parse-to-my-path","title":"9.5 Column mode, multiple new entries, parse to my path","text":"<p>This case would create a useless set of Entries containing one array quantity each. Usually, when parsing in column mode we want to parse together all the columns in the same section.</p>"},{"location":"howto/customization/units.html","title":"How to work with units","text":"<p>Units are a very important part of any scientific work. Units are also a common source of problems when multiple people are working with the same data. Sometimes this has far-reaching consequences as demonstrated by the Mars Climate Orbiter incident. This document explains the possibilities and best practices when working with units within the NOMAD infrastructure.</p>"},{"location":"howto/customization/units.html#available-unit-names","title":"Available unit names","text":"<p>The available unit names are controlled by the <code>nomad/units/defaults_en.txt</code> file. This is a plain text file that is internally read by the Pint library that we use for unit transformations in the NOMAD Python backend. These definitions and the associated conversion factors are then used always when transforming between two units in the NOMAD platform. Our graphical user interface also performs unit conversions in Javascript, and the same definitions and conversion factors are translated to the frontend through an environment config file (<code>env.js</code>).</p> <p>It is possible to add new units in this file to make them available both in the Python and in the Javascript environment. You can read more on how units are defined here.</p> <p>All units support the use of SI-prefixes. This means that if the unit <code>meter</code> has been defined, you are able to then automatically use <code>kilometers</code>, <code>centimeters</code>, <code>millimeters</code>, etc.</p>"},{"location":"howto/customization/units.html#defining-units-for-storing-data","title":"Defining units for storing data","text":"<p>From a practical point of view, there needs to be single choice for the unit in which the data is stored on hard disks, databases or in search engines. This choice is controlled by the <code>unit</code> attribute of a <code>Quantity</code>:</p> <pre><code>from nomad.metainfo import Quantity\n\nmy_energies = Quantity(\n  dtype=float,\n  unit='eV'\n)\n</code></pre> <p>The data will always be stored in this unit and will be returned in this unit when using e.g. the API.</p>"},{"location":"howto/customization/units.html#using-units-in-python","title":"Using units in Python","text":"<p>When creating custom schemas or parsers in the NOMAD framework, it is important to use the same unit definitions and conversion factors throughout. You should never define custom unit conversion routines of factors, but instead use the <code>nomad.units</code> package. This is important to ensure the interoperability of data.</p> <p>Here is an example of how you could work with units in Python:</p> <pre><code>import numpy as np\nfrom nomad.units import ureg  # Always import from here, never use another registry!\n\nfrom nomad.metainfo import MSection, Quantity\n\n\n# Here is a section with a quantity definition\nclass MySection(MSection):\n    my_energies = Quantity(\n        type=np.float64,\n        shape=[2],\n        unit='eV'\n    )\n\n\nmy_section = MySection()\n\n# If we assign a plain number array to a quantity, it is assumed to be given in\n# the units defined in the Quantity\nenergies = np.array([1, 1])\nmy_section.my_energies = energies\nprint(my_section.my_energies)  # [1 1] electron_volt\n\n# We can make a plain number array into a Pint Quantity by multiplying with\n# a unit\nenergies_with_unit = energies * ureg('hartree')\n\n# All numpy operations still work as normal, but the unit information is always\n# stored alongside\nenergies_with_unit *= 10\n\n# If you now assign this data into a NOMAD Quantity, the unit conversion is done\n# automatically\nmy_section.my_energies = energies_with_unit\nprint(my_section.my_energies)  # [272.11386245988473 272.11386245988473] electron_volt\n</code></pre>"},{"location":"howto/customization/units.html#defining-units-for-displaying-data","title":"Defining units for displaying data","text":"<p>When data is being displayed by the GUI, the unit can be choosen independently from the unit used in storing the data. There are several reasons for doing this:</p> <ul> <li>Maybe the unit is stored in SI units for consistency, but when viewing the  data you want to view it in some more field-specific units</li> <li>Maybe the unit is stored in some field specific units, but when demonstrating  your work you wish to use more standardized units.</li> <li>Maybe due to your background, you are more familiar with a specific unit, and  viewing the data in this unit helps you to understand it better. E.g. a  physicist might be familiar with working with electron volts, whereas a chemist  might prefer kilocalorie per mole.</li> </ul> <p>Currently the display unit is controlled through the ELN annotation, like this:</p> <pre><code>distance = Quantity(\n  dtype=float,\n  unit='meter',\n  a_eln=dict(defaultDisplayUnit='millimeter')\n)\n</code></pre>"},{"location":"howto/customization/workflows.html","title":"How to define workflows","text":""},{"location":"howto/customization/workflows.html#the-built-in-abstract-workflow-schema","title":"The built-in abstract workflow schema","text":"<p>Workflows are an important aspect of data as they explain how the data came to be. Let's first clarify that workflow refers to a workflow that already happened and that has produced input and output data that are linked through tasks that have been performed . This often is also referred to as data provenance or provenance graph.</p> <p>The following shows the overall abstract schema for worklows that can be found in <code>nomad.datamodel.metainfo.workflow</code> (blue):</p> <p></p> <p>The idea is that workflows are stored in a top-level archive section along-side other sections that contain the inputs and outputs. This way the workflow or provenance graph is just additional piece of the archive that describes how the data in this (or other archives) is connected.</p> <p>Let'c consider an example workflow. Imagine a geometry optimization and ground state calculation performed by two individual DFT code runs. The code runs are stored in NOMAD entries <code>geom_opt.archive.yaml</code> and <code>ground_state.archive.yaml</code> using the <code>run</code> top-level section.</p>"},{"location":"howto/customization/workflows.html#example-workflow","title":"Example workflow","text":"<p>Here is a logical depiction of the workflow and all its tasks, inputs, and outputs.</p> <p></p>"},{"location":"howto/customization/workflows.html#simple-workflow-entry","title":"Simple workflow entry","text":"<p>The following archive shows how to create such a workflow based on the given schema. Here we only model the <code>GeometryOpt</code> and <code>GroundStateCalculation</code> as two tasks with respective inputs and outputs that use references to entry archives of the respective code runs.</p> <pre><code>workflow2:\n  inputs:\n    - name: input system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n    - name: ground state calculation of relaxed system\n      section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n  tasks:\n    - name: GeometryOpt\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n      outputs:\n        - name: relaxed system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n\n    - name: GroundStateCalculation\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      outputs:\n        - name: ground state\n          section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n</code></pre>"},{"location":"howto/customization/workflows.html#nested-workflows-in-one-entry","title":"Nested workflows in one entry","text":"<p>Since a <code>Workflow</code> instance is also a <code>Tasks</code> instance due to inheritance, we can nest workflows. Here we detailed the <code>GeometryOpt</code> as a nested workflow:</p> <pre><code>workflow2:\n  inputs:\n    - name: input system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n    - name: ground state calculation of relaxed system\n      section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n  tasks:\n    - name: GeometryOpt\n      m_def: nomad.datamodel.metainfo.workflow.Workflow\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n      outputs:\n        - name: relaxed system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      tasks:\n        - inputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n          outputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/1'\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/calculation/0'\n        - inputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/1'\n          outputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/2'\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/calculation/1'\n        - inputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/2'\n          outputs:\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/3'\n            - section: '../upload/raw/geom_opt.archive.yaml#/run/0/calculation/2'\n    - name: GroundStateCalculation\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      outputs:\n        - name: ground state\n          section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n</code></pre>"},{"location":"howto/customization/workflows.html#nested-workflows-in-multiple-entries","title":"Nested Workflows in multiple entries","text":"<p>Typically, we want to colocate our individual workflows with their inputs and outputs. In the case of the geometry optimization, we might want to put this into the archive of the geometry optimization code run. So the <code>geom_opt.archive.yaml</code> might contain its own section <code>workflow2</code> that only contains the <code>GeometryOpt</code> workflow and uses local references to its inputs and outputs:</p> <pre><code>workflow2:\n  name: GeometryOpt\n  inputs:\n    - name: input system\n      section: '#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '#/run/0/system/-1'\n  tasks:\n    - inputs:\n        - section: '#/run/0/system/0'\n      outputs:\n        - section: '#/run/0/system/1'\n        - section: '#/run/0/calculation/0'\n    - inputs:\n        - section: '#/run/0/system/1'\n      outputs:\n        - section: '#/run/0/system/2'\n        - section: '#/run/0/calculation/1'\n    - inputs:\n        - section: '#/run/0/system/2'\n      outputs:\n        - section: '#/run/0/system/3'\n        - section: '#/run/0/calculation/2'\nrun:\n  - program:\n      name: 'VASP'\n    system: [{}, {}, {}]\n    calculation: [{}, {}, {}]\n</code></pre> <p>When we want to detail the complex workflow, we now need to refer to a nested workflow in a different entry. This cannot be done directly, because <code>Workflow</code> instances can only contain <code>Task</code> instances and not reference them. Therefore, we added a <code>TaskReference</code> section definition that can be used to create proxy instances for tasks and workflows:</p> <pre><code>workflow2:\n  inputs:\n    - name: input system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/0'\n  outputs:\n    - name: relaxed system\n      section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n    - name: ground state calculation of relaxed system\n      section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/raw/geom_opt.archive.yaml#/workflow2'\n    - name: GroundStateCalculation\n      inputs:\n        - name: input system\n          section: '../upload/raw/geom_opt.archive.yaml#/run/0/system/-1'\n      outputs:\n        - name: ground state\n          section: '../upload/raw/ground_state.archive.yaml#/run/0/calculations/0'\n</code></pre>"},{"location":"howto/customization/workflows.html#extending-the-workflow-schema","title":"Extending the workflow schema","text":"<p>The abstract workflow schema above allows us to build generalized tools for workflows, like workflow searches, navigation in workflow, graphical representations of workflows, etc. But, you can still augment the given section definitions with more information through inheritance. These information can be specialized references to denote inputs and outputs, can be additional workflow or task parameters, and much more.</p> <p>In this example, we created a special workflow section definition <code>GeometryOptimization</code> that defines a parameter <code>threshold</code> and an additional reference to the final calculation of the optimization:</p> <pre><code>definitions:\n  sections:\n    GeometryOptimizationWorkflow:\n      base_section: nomad.datamodel.metainfo.workflow.Workflow\n      quantities:\n        threshold:\n          type: float\n          unit: eV\n        final_calculation:\n          type: runschema.calculation.Calculation\n\nworkflow2:\n  m_def: GeometryOptimizationWorkflow\n  final_calculation: '#/run/0/calculation/-1'\n  threshold: 0.029\n  name: GeometryOpt\n  inputs:\n    ...\n</code></pre>"},{"location":"howto/customization/workflows.html#how-to-use-the-workflow-visualizer","title":"How to use the workflow visualizer","text":"<p>The entry overview page will show an interactive graph of the <code>workflow2</code> section if defined. In the following example, a workflow containing three tasks <code>Single Point</code>, <code>Geometry Optimization</code> and <code>Phonon</code> is shown.</p> <p></p> <p>The nodes (inputs, tasks and outputs) are shown from left to right for the current workflow layer. The edges (arrows) from (to) a node denotes an input (output) to a section in the target node. One can see the description for the nodes and edges by hovering over them. When the inputs and outputs are clicked, the linked section is shown in the archive browser. By clicking on a task, the graph zooms into the nested workflow layer. By clicking on the arrows, only the relevant linked nodes are shown. One can go back to the previous view by clicking on the current workflow node.</p> <p>A number of controls are also provided on top of the graph. The first enables a filtering of the nodes following a python-like syntax i.e., list (comma-separated) or range (colon-separated). Negative index and percent are also supported. By default, the task nodes can be filtered but can be changed to inputs or outputs by clicking on one of the respective nodes. By clicking on the <code>play</code> button, a force-directed layout of the task nodes is enabled. The other tools enable to toggle the legend, go back to a previous view and reset the view.</p>"},{"location":"howto/develop/code.html","title":"How to navigate the code","text":"<p>NOMAD is a complex project with lots of parts. This guide gives you a rough overview about the codebase and ideas about what to look at first.</p>"},{"location":"howto/develop/code.html#git-projects","title":"Git Projects","text":"<p>There is one main NOMAD project (and its fork on GitHub). This project contains all the framework and infrastructure code. It instigates all checks, builds, and deployments for the public NOMAD service, the NOMAD Oasis, and the <code>nomad-lab</code> Python package. All contributions to NOMAD have to go through this project eventually.</p> <p>All (Git) projects that NOMAD depends on are either a Git submodule (you find them all in the <code>dependencies</code> directory or its subdirectories) or they are listed as PyPI packages in the <code>pyproject.toml</code> of the main project (or one of its submodules).</p> <p>You can also have a look at the built-in plugins that constitute the majority of these projects. The only other projects are MatID, DOS fingerprints, and the NOMAD Remote Tools Hub.</p> <p>Note</p> <p>The GitLab organization nomad-lab and the GitHub organizations for FAIRmat and the NOMAD CoE all represent larger infrastructure and research projects, and they include many other Git projects that are not related. When navigating the codebase, only follow the submodules.</p>"},{"location":"howto/develop/code.html#python-code","title":"Python code","text":"<p>There are three main directories with Python code:</p> <ul> <li> <p><code>nomad</code>: The actual NOMAD code. It is structured into more subdirectories and modules.</p> </li> <li> <p><code>tests</code>: Tests (pytest) for the NOMAD code.   It follows the same module structure, but Python files are prefixed with <code>test_</code>.</p> </li> <li> <p><code>examples</code>: A few small Python scripts that might be linked in the documentation.</p> </li> </ul> <p>The <code>nomad</code> directory contains the following \"main\" modules. This list is not extensive but should help you to navigate the codebase:</p> <ul> <li> <p><code>app</code>: The FastAPI APIs: v1 and v1.2   NOMAD APIs,   OPTIMADE,   DCAT,   h5grove, and more.</p> </li> <li> <p><code>archive</code>: Functionality to store and access archive files. This is the storage format   for all processed data in NOMAD. See also the docs on   structured data.</p> </li> <li> <p><code>cli</code>: The command line interface (based on   Click). Subcommands are   structured into submodules.</p> </li> <li> <p><code>config</code>: NOMAD is configured through the <code>nomad.yaml</code> file. This contains all the   (Pydantic) models and default config   parameters.</p> </li> <li> <p><code>datamodel</code>: The built-in schemas (e.g. <code>nomad.datamodel.metainfo.workflow</code> used to   construct workflows). The base sections and section for the shared entry structure.   See also the docs on the datamodel and   processing.</p> </li> <li> <p><code>metainfo</code>: The Metainfo system, e.g. the schema language that NOMAD uses.</p> </li> <li> <p><code>normalizing</code>: All the normalizers. See also the docs on   processing.</p> </li> <li> <p><code>parsing</code>: The base classes for parsers, matching functionality, parser initialization,   some fundamental parsers like the archive parser. See also the docs on   processing.</p> </li> <li> <p><code>processing</code>: It's all about processing uploads and entries. The interface to   Celery and   MongoDB.</p> </li> <li> <p><code>units</code>: The unit and unit conversion system based on   Pint.</p> </li> <li> <p><code>utils</code>: Utility modules, e.g. the structured logging system   (structlog), id generation, and hashes.</p> </li> <li> <p><code>files.py</code>: Functionality to maintain the files for uploads in staging and published.   The interface to the file system.</p> </li> <li> <p><code>search.py</code>: The interface to   Elasticsearch.</p> </li> </ul>"},{"location":"howto/develop/code.html#gui-code","title":"GUI code","text":"<p>The NOMAD UI is written as a React single-page application (SPA). It uses (among many other libraries) MUI, Plotly, and D3. The GUI code is maintained in the <code>gui</code> directory. Most relevant code can be found in <code>gui/src/components</code>. The application entry point is <code>gui/src/index.js</code>.</p>"},{"location":"howto/develop/code.html#documentation","title":"Documentation","text":"<p>The documentation is based on MkDocs. The important files and directories are:</p> <ul> <li> <p><code>docs</code>: Contains all the Markdown files that contribute to the documentation system.</p> </li> <li> <p><code>mkdocs.yml</code>: The index and configuration of the documentation. New files have to be   added here as well.</p> </li> <li> <p><code>nomad/mkdocs.py</code>: Python code that defines   macros which can be   used in Markdown.</p> </li> </ul>"},{"location":"howto/develop/code.html#other-top-level-directories","title":"Other top-level directories","text":"<ul> <li> <p><code>dependencies</code>: Contains all the submodules, e.g. the parsers.</p> </li> <li> <p><code>ops</code>: Contains artifacts to run NOMAD components, e.g. <code>docker-compose.yaml</code> files,   and our Kubernetes Helm chart.</p> </li> <li> <p><code>scripts</code>: Contains scripts used during the build or for certain development tasks.</p> </li> </ul>"},{"location":"howto/develop/contrib.html","title":"How to contribute","text":"<p>Note</p> <p>The NOMAD source code is maintained in two synchronized projects on GitHub and a GitLab run by MPCDF. Everyone can contribute on GitHub. The GitLab instance requires an account for active contribution. This not an ideal situation: there are historic reasons and there is a lot of buy-in into the GitLab CI/CD system. This guide addresses contributions to both projects.</p>"},{"location":"howto/develop/contrib.html#issues","title":"Issues","text":""},{"location":"howto/develop/contrib.html#issue-trackers","title":"Issue trackers","text":"<p>Everyone can open a new issue in our main GitHub project.</p> <p>Use issues to ask questions, report bugs, or suggest features. If in doubt, use the main project to engage with us. If you address a specific plugin (e.g. parser), you can also post into the respective projects. See also the list of built-in plugins.</p> <p>If you are a member of FAIRmat, the NOMAD CoE, or are a close collaborator, you probably have an MPCDF GitLab account (or should ask us for one). Please use the issue tracker on our main GitLab project. This is where most of the implementation work is planned and executed.</p>"},{"location":"howto/develop/contrib.html#issue-content","title":"Issue content","text":"<p>A few tips that will help us to solve your issues quicker:</p> <ul> <li> <p>Focus on the issue. You don't need to greet us or say thanks and goodbye. Let's keep it   technical.</p> </li> <li> <p>Use descriptive short issue titles. Use specific words over general words.</p> </li> <li> <p>Describe the observed problem and not the assumed causes. Clearly separate speculation   from the problem description.</p> </li> <li> <p>Bugs: Think how we could reproduce the problem:</p> <ul> <li>What NOMAD URL are you using (UI), which package version (Python)?</li> <li>Is there an upload or entry id that we can look at?</li> <li>Example files or code snippets?</li> <li>Don't screenshot code, copy and paste instead. Use   code blocks.</li> </ul> </li> <li> <p>Features: Augment your feature descriptions with a use case that helps us understand   the feature and its scope.</p> </li> </ul>"},{"location":"howto/develop/contrib.html#issues-labels-gitlab","title":"Issues labels (GitLab)","text":"<p>On the main GitLab project, there are three main categories for labels. Ideally, each issue gets one of each category:</p> <ul> <li> <p>State label (grey): These are used to manage when and how the issue is addressed. This should be given by the NOMAD team member who is currently responsible to moderate the development. If it's a simple fix and you want to do it yourself, assign yourself and use \"bugfixes\".</p> </li> <li> <p>Component label (purple): These denote the part of the NOMAD software that is most likely effected. Multiple purple labels are possible.</p> </li> <li> <p>Kind label (red): Whether this is a bug, feature, refactoring, or documentation issue.</p> </li> </ul> <p>Unlabeled issues will get labeled by the NOMAD team as soon as possible. You can provide labels yourself.</p>"},{"location":"howto/develop/contrib.html#documentation","title":"Documentation","text":"<p>The documentation is part of NOMAD's main source code project. You can raise issues about the documentation as usual. To make changes, create a merge or pull request.</p> <p>See also the documentation part in our code navigation guide.</p>"},{"location":"howto/develop/contrib.html#plugins","title":"Plugins","text":"<p>Also read the guide on how to develop, publish, and distribute plugins.</p>"},{"location":"howto/develop/contrib.html#built-in-plugins","title":"Built-in plugins","text":"<p>Most plugins that are maintained by the NOMAD team are built-in plugins (e.g. all the parsers). These plugins are also available on the public NOMAD service.</p> <p>These plugins are tied to the main project's source code via dependencies.</p> <p>To contribute to these plugins, use the respective GitHub projects. See also the list of built-in plugins. The same rules apply there.</p> <pre><code>git checkout something-with-changes\ngit status\n</code></pre>"},{"location":"howto/develop/contrib.html#3rd-party-plugins","title":"3rd-party plugins","text":"<p>Please open an issue, if you want to announce a plugin or contribute a plugin as a potential built-in plugin (i.e. as part of the public NOMAD service).</p>"},{"location":"howto/develop/contrib.html#branches-and-tags","title":"Branches and Tags","text":"<p>On the main GitLab project we use protected and feature branches. You must not commit to protected branches directly (even if you have the rights).</p> <ul> <li> <p><code>develop</code>: a protected branch and the default branch. It contains the latest,   potentially unreleased, features and fixes. This is the main working branch.</p> </li> <li> <p><code>master</code>: a protected branch. Represents the latest stable release (usually what the   current official NOMAD runs on).</p> </li> <li> <p>feature branches: this is where you work. Typically they are automatically (use the   \"Create merge request\" button) named after issues: <code>&lt;issue-number&gt;-&lt;issue-title&gt;</code>.</p> </li> <li> <p><code>vX.X.X</code> or <code>vX.X.XrcX</code>: tags for (pre-)releases.</p> </li> </ul> <p>The <code>develop</code> branch and release tags are automatically synchronized to the GitHub project. Otherwise, this project is mostly the target for pull requests and does not contain other relevant branches.</p>"},{"location":"howto/develop/contrib.html#merge-requests-mr-gitlab","title":"Merge requests (MR, GitLab)","text":""},{"location":"howto/develop/contrib.html#create-the-mr","title":"Create the MR","text":"<p>Ideally, have an issue first and create the merge request (and branch) in the GitLab UI. There is a \"Create merge request\" button on each issue. When done manually, branches should be based on the <code>develop</code> branch and merge request should target <code>develop</code> as well.</p>"},{"location":"howto/develop/contrib.html#commit","title":"Commit","text":"<p>Make sure you follow our code guidelines and set up your IDE to enforce style checks, linting, and static analysis. You can also run tests locally. Try to keep a clean commit history and follow our Git tips.</p> <p>Usually only one person is working on a feature branch. Rebasing, amendments, and force pushes are allowed.</p>"},{"location":"howto/develop/contrib.html#changelog","title":"Changelog","text":"<p>We have an automatically generated changelog in the repository file <code>CHANGELOG.md</code>. This changelog is produced from commit messages and to maintain this file, you need to write commit messages accordingly.</p> <p>To trigger a changelog entry, your commit needs to end with a so-called Git trailer named <code>Changelog</code>. A typical commit message for a changelog entry should look like this:</p> <pre><code>A brief one-line title of the change.\n\nA longer *Markdown*-formatted description of the change. Keep in mind that GitLab will\nautomatically link the changelog entry with this commit and a respective merge requests.\nYou do not need to manually link to any GitLab resources.\n\nThis could span multiple paragraphs. However, keep it short. Documentation should go into\nthe actual documentation, but you should mention breaks in backward compatibility,\ndeprecation of features, etc.\n\nChangelog: Fixed\n</code></pre> <p>The trailer value (<code>Fixed</code> in the example) has to be one of the following values:</p> <ul> <li> <p><code>Fixed</code> for bugfixes.</p> </li> <li> <p><code>Added</code> for new features.</p> </li> <li> <p><code>Changed</code> for general improvements, e.g. updated documentation, refactoring, improving performance, etc.</p> </li> </ul> <p>These categories are consistent with keepachangelog.com. For more information about the changelog generation read the GitLab documentation.</p>"},{"location":"howto/develop/contrib.html#cicd-pipeline-and-review","title":"CI/CD pipeline and review","text":"<p>If you push to your merge requests, GitLab will run an extensive CI/CD pipeline. You need to pay attention to failures and resolve them before review.</p> <p>To review GUI changes, you can deploy your branch to the dev cluster via CI/CD actions.</p> <p>Find someone on the NOMAD developer team to review your merge request and request a review through GitLab. The review should be performed shortly and should not stall the merge request longer than two full work days.</p> <p>The reviewer needs to be able to learn about the merge request and what it tries to achieve. This information can come from:</p> <ul> <li>the linked issue</li> <li>the merge request description (may contain your commit message) and your comments</li> <li>threads that were opened by the author to attach comments to specific places in the code</li> </ul> <p>The reviewer will open threads that need to be solved by the merge request author. If all threads are resolved, you can request another review.</p> <p>For complex merge requests, you can also comment your code and create threads for the reviewer to resolve. This will allow you to explain your changes.</p>"},{"location":"howto/develop/contrib.html#merge","title":"Merge","text":"<p>The branch should be recently rebased with <code>develop</code> to allow a smooth merge:</p> <pre><code>git fetch\ngit rebase origin/develop\ngit push origin &lt;branch&gt; -f\n</code></pre> <p>The merge is usually performed by the merge request author after a positive review.</p> <p>If you could not keep a clean Git history with your commits, squash the merge request. Make sure to edit the commit message when squashing to have a changelog entry.</p> <p>Make sure to delete the branch on merge. The respective issue usually closes itself, due to the references put in by GitLab.</p>"},{"location":"howto/develop/contrib.html#pull-requests-pr-github","title":"Pull requests (PR, GitHub)","text":"<p>You can fork the main NOMAD project and create pull requests following the usual GitHub flow. Make sure to target the <code>develop</code> branch. A team member will pick up your pull request and automatically copy it to GitLab to run the pipeline and potentially perform the merge. This process is made transparent in the pull request discussion. Your commits and your authorship is maintained in this process.</p> <p>Similar rules apply to all the parser and plugin projects. Here, pipelines are run directly on GitHub. A team member will review and potentially merge your pull requests.</p>"},{"location":"howto/develop/contrib.html#tips-for-a-clean-git-history","title":"Tips for a clean Git history","text":"<p>It is often necessary to consider code history to reason about potential problems in our code. This can only be done if we keep a \"clean\" history.</p> <ul> <li> <p>Use descriptive commit messages. Use simple verbs (added, removed, refactored,   etc.) name features and changed components.   Include issue numbers   to create links in GitLab.</p> </li> <li> <p>Learn how to amend to avoid lists of small related commits.</p> </li> <li> <p>Learn how to rebase. Only merging feature branches should create merge commits.</p> </li> <li> <p>Squash commits when merging.</p> </li> <li> <p>Some videos on more advanced Git usage:</p> <ul> <li>Tools &amp; Concepts for Matering Version Control with Git</li> <li>Interactive Rebase, Cherry-Picking, Reflog, Submodules, and more</li> </ul> </li> </ul>"},{"location":"howto/develop/contrib.html#amend","title":"Amend","text":"<p>While working on a feature, there are certain practices that will help us to create a clean history with coherent commits, where each commit stands on its own.</p> <pre><code>git commit --amend\n</code></pre> <p>If you committed something to your own feature branch and then realize by CI that you have some tiny error in it you need to fix, try to amend this fix to the last commit. This will avoid unnecessary tiny commits and foster more coherent single commits. With <code>--amend</code> you are adding changes to the last commit, i.e. editing the last commit. When you push, you need to force it, e.g. <code>git push origin feature-branch --force-with-lease</code>. So be careful, and only use this on your own branches.</p>"},{"location":"howto/develop/contrib.html#rebase","title":"Rebase","text":"<pre><code>git rebase &lt;version-branch&gt;\n</code></pre> <p>Let's assume you work on a bigger feature that takes more time. You might want to merge the version branch into your feature branch from time to time to get the recent changes. In these cases, use <code>rebase</code> and not <code>merge</code>. Rebasing puts your branch commits in front of the merged commits instead of creating a new commit with two ancestors. It moves the point where you initially branched away from the version branch to the current position in the version branch. This will avoid merges, merge commits, and generally leaves us with a more consistent history. You can also rebase before creating a merge request, which allows no-op merges. Ideally, the only real merges that we ever have are between version branches.</p>"},{"location":"howto/develop/contrib.html#squash","title":"Squash","text":"<pre><code>git merge --squash &lt;other-branch&gt;\n</code></pre> <p>When you need multiple branches to implement a feature and merge between them, try to use <code>--squash</code>. Squashing puts all commits of the merged branch into a single commit. It allows you to have many commits and then squash them into one. This is useful if these commits were made just to synchronize between workstations, due to unexpected errors in CI/CD, because you needed a save point, etc. Again the goal is to have coherent commits, where each commit makes sense on its own.</p> <p>Squashing can also be applied on a selection of commits during an interactive rebase.</p>"},{"location":"howto/develop/normalizing.html","title":"Normalizing","text":""},{"location":"howto/develop/normalizing.html#the-update_entry-method","title":"The <code>update_entry</code> method","text":"<p>The root context, which is available from the <code>.m_context</code> of a <code>EntryArchive</code>, which could be accessed via <code>section.m_root().m_context</code> if <code>section</code> is attached to a <code>EntryArchive</code>, provides the functionality to update/create child entries on-the-fly and invoke the processing if necessary.</p> <p>Note</p> <p>The usage of this functionality is strongly discouraged and should be avoided if possible.</p> <p>The method has the following signature.</p> <pre><code>    @contextmanager\n    def update_entry(\n        self,\n        mainfile: str,\n        *,\n        write: bool = False,\n        process: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Open the target file and send it to the updater function.\n        The updater function shall return the updated file content.\n        The updated file will be stored and processed if needed.\n\n        WARNING:\n            If `process=True`, the updated file will be processed immediately.\n            Please be aware of the fact that this method may be called during the processing of\n            the parent/main file.\n            This means if there are any data dependencies, there is a risk of infinite loops,\n            racing conditions and/or other unexpected behavior.\n            You must carefully design the logic to mitigate these risks.\n\n        To use this function, you shall use the with-statement as follows:\n\n        ```python\n        with context.update_entry('mainfile.json',**kwargs) as content:\n            # do something with content\n        ```\n\n        Parameters:\n            mainfile: The relative path (from upload root) to the file to update.\n            write: Whether to write the updated file back to the storage.\n                If False, no processing will be triggered whatsoever.\n            process: Whether to trigger processing of the updated file.\n        \"\"\"\n        ...\n</code></pre> <p>It is wrapped with a <code>@contextmanager</code> decorator, thus it shall be used with a <code>with</code> block. It yields a plain <code>dict</code> object that represents the content of the file.</p> <pre><code># get the context from the current archive\ncontext = section.m_root().m_context\n# create/update the file 'mainfile.json' and process it\nwith context.update_entry('mainfile.json', process=True) as content_dict:\n    # do something with content\n    content_dict['key'] = 'value'\n    ...\n</code></pre> <p>The main file must be a <code>json</code> or <code>yaml</code> file. Other formats are not supported.</p> <p>If only need to read the content, leave <code>write=False</code>. Otherwise, set <code>write=True</code> to store the updated content back to the storage.</p> <p>It is possible to invoke the processing immediately by setting <code>process=True</code>. However, this is not recommended due to various security concerns.</p> <p>The following caveats must be acknowledged when using this method:</p> <ol> <li>The specific logic of creating/updating the file must be re-entrant safe, see details.    To put simply, the first call and subsequent calls must yield the same result regardless of what is already stored in the file.</li> <li>A child entry must not be accessed by multiple parent entries.    Because the parent entries are processed in parallel (by multiple <code>celery</code> workers), there is a risk of racing conditions if the child entry is accessed by multiple parent entries.</li> <li>The child entry shall not modify the parent entry (and any other entries).    Otherwise, there is a risk of infinite loops and data corruption.</li> <li>A child entry shall not depend on other child entries.</li> </ol>"},{"location":"howto/develop/release.html","title":"How to release a new NOMAD version","text":""},{"location":"howto/develop/release.html#what-is-a-release","title":"What is a release","text":"<p>NOMAD is a public service, a Git repository, a Python package, and a docker image. What exactly is a NOMAD release? It is all of the following:</p> <ul> <li>a version tag on the main NOMAD git project, e.g. <code>v1.3.0</code></li> <li>a gitlab release based on a tag with potential release notes</li> <li>a version of the <code>nomad-lab</code> Python package released to pypi.org, e.g. <code>nomad-lab==1.3.0</code>.</li> <li>a docker image tag, e.g. <code>gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:v1.3.0</code></li> <li>the docker image tag <code>stable</code> points to the image with the latest release tag</li> </ul>"},{"location":"howto/develop/release.html#steps-towards-a-new-release","title":"Steps towards a new release","text":"<ul> <li>Silently create a new version tag in the <code>v1.3.0</code> format.</li> <li>Deploy the build from this tag to the public NOMAD deployments. What deployments are updated might depend on the current needs. But usually the production and test deployment should be updated.</li> <li>Release the Python package to the local gitlab registry. (This will update the NORTH Jupyter image in the next nightly build and most likely effect plugins)</li> <li>Bump the <code>latest</code> docker image tag.</li> <li>For minor and major releases, encourage (Oasis) users to test the public services and the latest docker image for a short trial phase (e.g. 3 days). For patch releases this step should be skipped.</li> <li>Create a gitlab release from the tag with potential release notes. Those notes should also be added to the README.md. It is ok, if the updated README.md is not part of the release itself.</li> <li>Bump the <code>stable</code> docker image tag.</li> <li>Publish the Python package to pypi.org</li> </ul>"},{"location":"howto/develop/release.html#how-to-deal-with-hotfixes","title":"How to deal with hotfixes","text":"<p>This depends on the current <code>develop</code> branch and requires a judgement call. There are two opposing scenarios:</p> <ol> <li> <p>The <code>develop</code> branch only contains minor fixes or fix/features that are not likely to effect the released functionality. In this case, a new release with an increased patch version is the right call.</p> </li> <li> <p>The <code>develop</code> branch adds major refactorings and commits that likely effect the released functionality. In this case, a <code>v1.3.0-hotfix</code> branch should be created. After adding commits with the hotfix, the release process can be applied to the hotfix branch in order to create a <code>v1.3.1</code> release that only contains the hotfixes and not the changes on develop. After the <code>v1.3.1</code> release, the <code>v1.3.0-hotfix</code> branch is merged back into develop. Hotfix branches should not live longer than a week.</p> </li> </ol>"},{"location":"howto/develop/release.html#major-minor-patch-versions","title":"Major, minor, patch versions","text":"<ul> <li> <p>patch: No significant refactorings. Only new/updated features behind disabled feature switches. Bugfixes. Might mark features as deprecated.</p> </li> <li> <p>minor: Might enabled new features by default. Can contain major refactorings (especially if they effect to plugin developers, data stewards etc.). Might finally deprecate features. Should \"basically\" be backwards compatible.</p> </li> <li> <p>major: Breaking changes and will require data migration.</p> </li> </ul> <p>What is a breaking change and what does \"basically\" backwards compatible mean? We develop experimental functionality and often need multiple iterations to get a feature right. This also means that we technically introduce breaking changes far more often than we can issue major releases. It is again a judgement call to decide on major vs minor. The following things would generally not be considered breaking and would be considered backwards compatible:</p> <ul> <li>the breaking change is for a feature that is not enabled by default</li> <li>data migration is necessary for new functionality, but optional for existing functionality</li> <li>it is unlikely that plugins not developed by FAIRmat are effected</li> <li>it is unlikely that data beyond the central NOMAD deployments need to be migrated</li> </ul>"},{"location":"howto/develop/release.html#release-schedule","title":"Release schedule","text":"<p>Patch releases should happen frequently and at least once every other month. Also minor releases should be done semi regular. Important new features or at least bi-annual FAIRmat events should trigger a minor release. Major releases require more involved planning, data migration, and respective instructions and assistance to NOMAD (Oasis) users. They are also political. Therefore, they do not a have a regular schedule.</p> <p>With a one <code>develop</code> branch Git strategy, there might be necessary exceptions to regular patch releases. In general, new features should be protected by feature switches, and should not be an issue. However, major refactorings that might effect multiple components are hard to hide behind a feature switch. In such cases, the release schedule might be put on hold for another month or two.</p>"},{"location":"howto/develop/search.html","title":"How to extend the search","text":""},{"location":"howto/develop/search.html#the-search-indices","title":"The search indices","text":"<p>NOMAD uses Elasticsearch as the underlying search engine. The respective indices are automatically populated during processing and other NOMAD operations. The indices are built from some of the archive information of each entry. These are mostly the sections <code>metadata</code> (ids, user metadata, other \"administrative\" and \"internal\" metadata) and <code>results</code> (a summary of all extracted (meta)data). However, these sections are not indexed verbatim. What exactly and how it is indexed is determined by the Metainfo and the <code>elasticsearch</code> Metainfo extension.</p>"},{"location":"howto/develop/search.html#the-elasticsearch-metainfo-extension","title":"The <code>elasticsearch</code> Metainfo extension","text":"<p>Here is the definition of <code>results.material.elements</code> as an example:</p> <pre><code>class Material(MSection):\n    ...\n    elements = Quantity(\n        type=MEnum(chemical_symbols),\n        shape=[\"0..*\"],\n        default=[],\n        description='Names of the different elements present in the structure.',\n        a_elasticsearch=[\n            Elasticsearch(material_type, many_all=True),\n            Elasticsearch(suggestion=\"simple\")\n        ]\n    )\n</code></pre> <p>Extensions are denoted with the <code>a_</code> prefix as in <code>a_elasticsearch</code>. Since extensions can have all kinds of values, the <code>elasticsearch</code> extension is rather complex and uses the <code>Elasticsearch</code> class.</p> <p>There can be multiple values. Each <code>Elasticsearch</code> instance configures a different part of the index. This means that the same quantity can be indexed multiple time. For example, if you need a text- and a keyword-based search for the same data. Here is a version of the <code>metadata.mainfile</code> definition as another example:</p> <pre><code>mainfile = metainfo.Quantity(\n    type=str, categories=[MongoEntryMetadata, MongoSystemMetadata],\n    description='The path to the mainfile from the root directory of the uploaded files',\n    a_elasticsearch=[\n        Elasticsearch(_es_field='keyword'),\n        Elasticsearch(\n            mapping=dict(type='text', analyzer=path_analyzer.to_dict()),\n            field='path', _es_field='')\n    ]\n)\n</code></pre>"},{"location":"howto/develop/search.html#the-different-indices","title":"The different indices","text":"<p>The first (optional) argument for <code>Elasticsearch</code> determines where the data is indexed. There are three principle places:</p> <ul> <li>the entry index (<code>entry_type</code>, default)</li> <li>the materials index (<code>material_type</code>)</li> <li>the entries within the materials index (<code>material_entry_type</code>)</li> </ul>"},{"location":"howto/develop/search.html#entry-index","title":"Entry index","text":"<p>This is the default and is used even if another (additional) value is given. All data is put into the entry index.</p>"},{"location":"howto/develop/search.html#materials-index","title":"Materials index","text":"<p>This is a separate index from the entry index and contains aggregated material information. Each document in this index represents a material. We use a hash over some material properties (elements, system type, symmetry) to define what a material is and which entries belong to which material.</p> <p>Some parts of the material documents contain the material information that is always the same for all entries of this material. Examples are elements, formulas, symmetry.</p>"},{"location":"howto/develop/search.html#material-entries","title":"Material entries","text":"<p>The materials index also contains entry-specific information that allows to filter materials for the existence of entries with certain criteria. Examples are publish status, user metadata, used method, or property data.</p>"},{"location":"howto/develop/search.html#adding-quantities","title":"Adding quantities","text":"<p>In principle, all quantities could be added to the index, but for convention and simplicity, only quantities defined in the sections <code>metadata</code> and <code>results</code> should be added. This means that if you want to add custom quantities from your parser, for example, you will also need to customize the results normalizer to copy or reference parsed data.</p>"},{"location":"howto/develop/search.html#the-search-api","title":"The search API","text":"<p>The search API does not have to change. It automatically supports all quantities with the <code>elasticsearch</code> extension. The keys that you can use in the API are the Metainfo paths of the respective quantities, e.g. <code>results.material.elements</code> or <code>mainfile</code> (note that the <code>metadata.</code> prefix is always omitted). If there are multiple <code>elasticsearch</code> annotations for the same quantity, all but one define a <code>field</code> parameter, which is added to the quantity path, e.g. <code>mainfile.path</code>.</p>"},{"location":"howto/develop/search.html#the-search-web-interface","title":"The search web interface","text":"<p>Attention</p> <pre><code>Coming soon ...\n</code></pre>"},{"location":"howto/develop/setup.html","title":"How to get started in development","text":"<p>This is a step-by-step guide to get started with NOMAD development. You will clone all sources, set up a Python and Node.js environment, install all necessary dependencies, run the infrastructure in development mode, learn to run the test suites, and set up Visual Studio Code for NOMAD development.</p> <p>This is not about working with the NOMAD Python package <code>nomad-lab</code>. You can find its documentation here.</p>"},{"location":"howto/develop/setup.html#clone-the-sources","title":"Clone the sources","text":"<p>If you're planning on developing the core <code>nomad</code> package alongside other plugins, consider using the <code>nomad-distro-dev</code> setup as described at the end of this page.</p> <p>If not already done, you should clone NOMAD. If you have an account at the MPDCF Gitlab, you can clone with the SSH URL:</p> <pre><code>git clone git@gitlab.mpcdf.mpg.de:nomad-lab/nomad-FAIR.git nomad\n</code></pre> <p>Otherwise, clone using the HTTPS URL:</p> <pre><code>git clone https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR.git nomad\n</code></pre> <p>Then change directory to <code>nomad</code></p> <pre><code>cd nomad\n</code></pre> <p>There are several branches in the repository. The <code>master</code> branch contains the latest released version, but there is also a <code>develop</code> (new features) and <code>release</code> branch (hotfixes). There are also tags for each version called <code>vX.X.X</code>. Check out the branch you want to work on.</p> <pre><code>git checkout develop\n</code></pre> <p>The development branches are protected and you should create a new branch including your changes.</p> <pre><code>git checkout -b &lt;my-branch-name&gt;\n</code></pre> <p>This branch can be pushed to the repo, and then later may be merged to the relevant branch.</p>"},{"location":"howto/develop/setup.html#installation","title":"Installation","text":""},{"location":"howto/develop/setup.html#set-up-a-python-environment","title":"Set up a Python environment","text":"<p>The NOMAD code currently requires Python 3.12. You should work in a Python virtual environment.</p> <p>For developers using VSCode, in case you encounter any issues with breakpoints not triggering, consider upgrading to py3.12.</p>"},{"location":"howto/develop/setup.html#pyenv","title":"Pyenv","text":"<p>If your host machine has an older version installed, you can use pyenv to use Python 3.12 in parallel with your system's Python.</p>"},{"location":"howto/develop/setup.html#virtualenv","title":"Virtualenv","text":"<p>Create a virtual environment. It allows you to keep NOMAD and its dependencies separate from your system's Python installation. Make sure that the virtual environment is based on Python 3.12. Use either the built-in <code>venv</code> module (see example) or virtualenv.</p> <pre><code>python3 -m venv .pyenv\nsource .pyenv/bin/activate\n</code></pre>"},{"location":"howto/develop/setup.html#conda","title":"Conda","text":"<p>If you are a conda user, there is an equivalent, but you have to install <code>pip</code> and the right Python version while creating the environment.</p> <pre><code>conda create --name nomad_env pip python=3.12\nconda activate nomad_env\n</code></pre> <p>To install libmagick for Conda, you can use (other channels might also work):</p> <pre><code>conda install -c conda-forge --name nomad_env libmagic\n</code></pre>"},{"location":"howto/develop/setup.html#upgrade-pip","title":"Upgrade pip","text":"<p>Make sure you have the most recent version of <code>pip</code>:</p> <pre><code>pip install --upgrade pip\n</code></pre>"},{"location":"howto/develop/setup.html#install-missing-system-libraries-eg-on-windows-macos","title":"Install missing system libraries (e.g. on Windows, MacOS)","text":"<p>Even though the NOMAD infrastructure is written in Python, there are C libraries required by some of our Python dependencies. Specifically, the libmagic library, which allows determining the MIME type of files, and the hdf5 library, which is essential for handling HDF5 files, must be installed on most Unix/Linux systems.</p> <p>The absence of these libraries can lead to issues during installation or runtime.</p> <p>For macOS:</p> <pre><code>brew install hdf5 libmagic file-formula\n</code></pre> <p>For Windows (pre-compiled binaries for <code>hdf5</code> are included in the dependencies):</p> <p>-libmagic: We include python-magic-bin as a dependency for Windows users. If you encounter an error such as NameError: name '_compressions' is not defined, try uninstalling and reinstalling the library:</p> <pre><code>pip uninstall python-magic-bin\npip install python-magic-bin\n</code></pre> <p>You can confirm that the magic library is correctly installed by running:</p> <pre><code>python -c \"import magic\"\n</code></pre>"},{"location":"howto/develop/setup.html#install-nomad","title":"Install NOMAD","text":"<p>The following command can be used to install NOMAD.</p> <pre><code>./scripts/setup_dev_env.sh\n</code></pre> Installation details <p>Here is more detailed rundown of the installation steps.</p> <p>Previous build is cleaned:</p> <pre><code>rm -rf nomad/app/static/docs\nrm -rf nomad/app/static/gui\nrm -rf site\n</code></pre> <p>We use uv to install the packages. You can install uv using <code>pip install uv</code>.</p> <p>Next we install the <code>nomad</code> package itself (including all extras). The <code>-e</code> option will install NOMAD with symbolic links that allow you to change the code without having to reinstall after each change.</p> <p>The -c flag restricts the installation of dependencies to the versions specified in the provided requirements file, ensuring that only those versions are installed.</p> <pre><code>uv pip install -e .[parsing,infrastructure,dev] -c requirements-dev.txt\n</code></pre> <p>Install \"default\" plugins. TODO: This can be removed once we have proper proper distribution <pre><code>uv pip install -r requirements-plugins.txt\n</code></pre></p> <p>Next we build the documentation.</p> <pre><code>sh scripts/generate_docs_artifacts.sh\nmkdocs build\nmkdir -p nomad/app/static/docs\ncp -r site/* nomad/app/static/docs\n</code></pre> <p>The NOMAD GUI requires a static <code>.env</code> file, which can be generated with:</p> <pre><code>python -m nomad.cli dev gui-env &gt; gui/.env.development\n</code></pre> <p>This file includes some of the server details needed so that the GUI can make the initial connection properly. If, for example, you change the server address in your NOMAD configuration file, it will be necessary to regenerate this <code>.env</code> file. In production this file will be overridden.</p> <p>In addition, you have to do some more steps to prepare your working copy to run all the tests, see below.</p>"},{"location":"howto/develop/setup.html#run-the-infrastructure","title":"Run the infrastructure","text":""},{"location":"howto/develop/setup.html#install-docker","title":"Install Docker","text":"<p>You need to install Docker. Docker nowadays comes with Docker Compose (<code>docker compose</code>) built-in. Prior, you needed to install the standalone Docker Compose (<code>docker-compose</code>).</p>"},{"location":"howto/develop/setup.html#run-required-3rd-party-services","title":"Run required 3rd party services","text":"<p>To run NOMAD, some 3rd party services are needed</p> <ul> <li>Elasticsearch: NOMAD's search and analytics engine</li> <li>MongoDB: used to store processing state</li> <li>RabbitMQ: a task queue used to distribute work in a cluster</li> </ul> <p>All 3rd party services should be run via <code>docker compose</code> (see below). Keep in mind that <code>docker compose</code> configures all services in a way that mirrors the configuration of the Python code in <code>nomad/config.py</code> and the GUI config in <code>gui/.env.development</code>.</p> <p>The default virtual memory for Elasticsearch will likely be too low. On Linux, you can run the following command as root:</p> <pre><code>sysctl -w vm.max_map_count=262144\n</code></pre> <p>To set this value permanently, see here. Then you can run all services with:</p> <pre><code>cd ops/docker-compose/infrastructure\ndocker compose up -d elastic mongo rabbitmq\ncd ../../..\n</code></pre> <p>If your system almost ran out of disk space, Elasticsearch enforces a read-only index block (read more), but after clearing up the disk space you need to reset it manually using the following command:</p> <pre><code>curl -XPUT -H \"Content-Type: application/json\" http://localhost:9200/_all/_settings -d '{\"index.blocks.read_only_allow_delete\": false}'\n</code></pre> <p>Note that the Elasticsearch service has a known problem in quickly hitting the virtual memory limits of your OS. If you experience issues with the Elasticsearch container not running correctly or crashing, try increasing the virtual memory limits as shown here.</p> <p>To shut down everything, just <code>ctrl-c</code> the running output. If you started everything in deamon mode (<code>-d</code>) use:</p> <pre><code>docker compose down\n</code></pre> <p>Usually these services are used only by NOMAD, but sometimes you also need to check something or do some manual steps. You can access MongoDB and Elasticsearch via your preferred tools. Just make sure to use the right ports.</p>"},{"location":"howto/develop/setup.html#run-nomad","title":"Run NOMAD","text":""},{"location":"howto/develop/setup.html#nomadyaml","title":"<code>nomad.yaml</code>","text":"<p>Before you run NOMAD for development purposes, you should configure it to use the test realm of our user management system. By default, NOMAD will use the <code>fairdi_nomad_prod</code> realm. Create a <code>nomad.yaml</code> file in the root folder:</p> <pre><code>keycloak:\n  realm_name: fairdi_nomad_test\n</code></pre> <p>You might also want to exclude some of the default plugins, or only include the plugins you'll need. Especially plugins with slower start-up and import times due to instantiation of large schemas (e.g. nexus create couple thousand definitions for 70+ applications) can often be excluded:</p> <pre><code>plugins:\n  exclude:\n    - parsers/nexus\n</code></pre> <p>Note that this will lead to failing tests for the excluded plugins.</p>"},{"location":"howto/develop/setup.html#app-and-worker","title":"App and worker","text":"<p>NOMAD consists of the NOMAD app/API, a Celery worker, and the GUI. You can run the app and the worker with the NOMAD CLI. These commands will run the services and display their log output. You should open them in separate shells as they run continuously. They will not watch code changes and you have to restart manually.</p> <pre><code>nomad admin run app\n</code></pre> <pre><code>nomad admin run worker\n</code></pre> <p>Or both together in one process:</p> <pre><code>nomad admin run appworker\n</code></pre> <p>On MacOS you might run into multiprocessing errors. That can be solved as described here.</p> <p>The app will run at port 8000 by default.</p> <p>Depending on your preference, it is also possible to run the app and worker together with a python script.</p> <pre><code>python3 scripts/run_dev_env.py\n</code></pre> <p>The <code>scripts/run_dev_env.py</code> enables develop mode by default such that only one worker is used for FastAPI and Celery. This helps save resource usage, which is often useful for development purposes.</p> <p>There are more configurations available. You can see them by running the following.</p> <pre><code>nomad admin run --help\n</code></pre> <p>To run the worker directly with Celery, do (from the root)</p> <pre><code>celery -A nomad.processing worker -l info\n</code></pre> <p>If you run the GUI on its own (e.g. with the React dev server below), you also need to start the app manually. The GUI and its dependencies run on Node.js and the Yarn dependency manager. Read their documentation on how to install them for your platform.</p> <pre><code>cd gui\nyarn\nyarn start\n</code></pre> <p>Note that the current codebase requires Node.js version 20.</p>"},{"location":"howto/develop/setup.html#jupyterhub","title":"JupyterHub","text":"<p>NOMAD also has a built-in JupyterHub that is used to launch remote tools (e.g. Jupyter notebooks).</p> <p>To run JupyterHub, some additional configuration might be necessary.</p> <pre><code>north:\n  hub_connect_ip: 'host.docker.internal'\n  jupyterhub_crypt_key: '&lt;crypt key&gt;'\n</code></pre> <p>On Windows system, you might have to activate further specific functionality:</p> <pre><code>north:\n  hub_connect_ip: 'host.docker.internal'\n  hub_connect_url: 'http://host.docker.internal:8081'\n  windows: true\n  jupyterhub_crypt_key: '&lt;crypt key&gt;'\n</code></pre> <ul> <li> <p>If you are not on Linux, you need to configure how JupyterHub can reach your host   network from docker containers. For Windows and MacOS you need to set <code>hub_connect_ip</code>   to <code>host.docker.internal</code>. For Linux you can leave it out and use the default   <code>172.17.0.1</code>, unless you changed your docker configuration.</p> </li> <li> <p>You have to generate a <code>crypt key</code> with <code>openssl rand -hex 32</code>.</p> </li> <li> <p>You might need to install   configurable-http-proxy.</p> </li> </ul> <p>The <code>configurable-http-proxy</code> comes as a Node.js package. See Node.js for how to install <code>npm</code>. The proxy can be globally installed with:</p> <pre><code>npm install -g configurable-http-proxy\n</code></pre> <p>JupyterHub is a separate application. You can run JuypterHub similar to the other part:</p> <pre><code>nomad admin run hub\n</code></pre> <p>To run JupyterHub directly, do (from the root)</p> <pre><code>jupyterhub -f nomad/jupyterhub_config.py --port 9000\n</code></pre>"},{"location":"howto/develop/setup.html#running-tests","title":"Running tests","text":""},{"location":"howto/develop/setup.html#backend-tests","title":"Backend tests","text":"<p>To run the tests some additional settings and files are necessary that are not part of the codebase.</p> <p>You have to provide static files to serve the docs and NOMAD distribution:</p> <pre><code>./scripts/generate_docs_artifacts.sh\nrm -rf site &amp;&amp; mkdocs build &amp;&amp; mv site nomad/app/static/docs\n</code></pre> <p>You need to have the infrastructure partially running: <code>elastic</code>, <code>mongo</code>, <code>rabbitmq</code>. The rest should be mocked or provided by the tests. Make sure that you do not run any worker, as they will fight for tasks in the queue. To start the infrastructure and run the tests, use:</p> <pre><code>cd ops/docker-compose/infrastructure\ndocker compose up -d elastic mongo rabbitmq\ncd ../../..\npytest -sv tests\n</code></pre> <p>Note</p> <p>Some of these tests will fail because a few large files are not included in the Git repository. You may ignore these for local testing, they are still checked by the CI/CD pipeline:</p> <pre><code>FAILED tests/archive/test_archive.py::test_read_springer - AttributeError: 'NoneType' object has no attribute 'seek'\nFAILED tests/normalizing/test_material.py::test_material_bulk - assert None\nFAILED tests/normalizing/test_system.py::test_springer_normalizer - IndexError: list index out of range\n</code></pre> <p>If you excluded plugins in your NOMAD config, then those tests will also fail.</p> <p>We use Ruff and Mypy to maintain code quality. Additionally, we recommend installing the Ruff plugins for your code editor to streamline the process. To execute Ruff and Mypy from the command line, you can utilize the following command: <pre><code>nomad dev qa --skip-tests\n</code></pre></p> <p>We use ruff as a linter and as an autoformatter. If you only want to lint your code, you can run: <pre><code>ruff check .\n</code></pre></p> <p>To format your code you can run: <pre><code>ruff format .\n</code></pre></p> <p>To run all tests and code QA:</p> <pre><code>nomad dev qa\n</code></pre> <p>This mimics the tests and checks that the GitLab CI/CD will perform.</p>"},{"location":"howto/develop/setup.html#custom-pytest-options","title":"Custom pytest options","text":""},{"location":"howto/develop/setup.html#-celery-inspect-timeout","title":"<code>--celery-inspect-timeout</code>","text":"<p>To ensure that all tests are independent despite reusing the same queue and workers, the <code>worker</code> fixture cleans up all running tasks after the test. This involves by default a timeout of one second, which accumulates over all tests using that fixture. In one local development environment this made a difference of about 14 min vs. 7 min without the timeout.</p> <p>If you want to speed up your local testing, you can use <code>pytest --celery-inspect-timeout 0.1</code> to shorten the timeout to a tenth of a second. Be aware that this might leave tasks running, which can affect later tests.</p>"},{"location":"howto/develop/setup.html#-fixture-filters","title":"<code>--fixture-filters</code>","text":"<p>You may want to run only tests that use a specific fixture, e.g. if you are editing that fixture. For example, to run only tests with the <code>worker</code> fixture, use <code>pytest --fixture-filters worker</code>. If you list more than one, all of them must be requested by the test to be included.</p> <p>You can also negate a fixture by prefixing its name with <code>!</code>. For example, to run all tests that do not depend on the <code>worker</code> fixture, use <code>pytest --fixture-filters '!worker'</code> (quotes are needed for <code>!</code>).</p> <p>Note that if <code>test1</code> depends on <code>fixture1</code>, and <code>fixture1</code> depends on <code>fixture2</code>, then <code>test1</code> also depends on <code>fixture2</code>, even though it was not explicitly listed as a parameter.</p>"},{"location":"howto/develop/setup.html#frontend-tests","title":"Frontend tests","text":"<p>We use <code>testing-library</code> to implement our GUI tests and <code>testing-library</code> itself uses <code>Jest</code> to run the tests. Tests are written in <code>*.spec.js</code> files that accompany the implementation. Tests should focus on functionality, not on implementation details: <code>testing-library</code> is designed to enforce this kind of testing.</p> <p>Note</p> <p>When testing HTML output, the elements are rendered using jsdom: this is not completely identical to using an actual browser (e.g. does not support WebGL), but in practice is realistic enough for the majority of the test.</p>"},{"location":"howto/develop/setup.html#test-structure","title":"Test structure","text":"<p>We have adopted a <code>pytest</code>-like structure for organizing the test utilities: each source code folder may contain a <code>conftest.js</code> file that contains utilities that are relevant for testing the code in that particular folder. These utilities can usually be placed into the following categories:</p> <ul> <li> <p>Custom renders: When testing React components, the   <code>render</code> function   is used to display them on the test DOM. Typically your components require   some parts of the infrastructure to work properly, which is achieved by   wrapping your component with other components that provide a context. Custom   render functions can do this automatically for you, e.g. the default render   as exported from <code>src/components/conftest.js</code> wraps your components with an   infrastructure that is very similar to the production app. See   here   for more information.</p> </li> <li> <p>Custom queries: See   here   for more information.</p> </li> <li> <p>Custom expects: These are reusable functions that perform actual tests using   the <code>expect</code> function. Whenever the same tests are performed by several   <code>*.spec.js</code> files, you should formalize these common tests into an   <code>expect*</code> function and place it in a relevant <code>conftest.js</code> file.</p> </li> </ul> <p>Often your components will need to communicate with the API during tests. One should generally avoid using manually created mocks for the API traffic, and instead prefer using API responses that originate from an actual API call during testing. Manually created mocks require a lot of manual work in creating them and keeping them up-to-date and true integration tests are impossible to perform without live communication with an API. In order to simplify the API communication during testing, you can use the <code>startAPI</code>+<code>closeAPI</code> functions, that will prepare the API traffic for you. A simple example could look like this:</p> <pre><code>import React from 'react'\nimport { waitFor } from '@testing-library/dom'\nimport { startAPI, closeAPI, screen } from '../../conftest'\nimport { renderSearchEntry, expectInputHeader } from '../conftest'\n\ntest('periodic table shows the elements retrieved through the API', async () =&gt; {\n  startAPI('&lt;state_name&gt;', '&lt;snapshot_name&gt;')\n  renderSearchEntry(...)\n  expect(...)\n  closeAPI()\n})\n</code></pre> <p>Here the important parameters are:</p> <ul> <li> <p><code>&lt;state_name&gt;</code>: Specifies an initial backend configuration for this test. These   states are defined as Python functions that are stored in   <code>nomad-FAIR/tests/states</code>, example given below. These functions may, for example,   prepare several uploads entries, datasets, etc. for the test.</p> </li> <li> <p><code>&lt;snapshot_name&gt;</code>: Specifies a filepath for reading/recording pre-recorded API   traffic.</p> </li> </ul> <p>An example of a simple test state could look like this:</p> <pre><code>from nomad import infrastructure\nfrom nomad.utils import create_uuid\nfrom nomad.utils.exampledata import ExampleData\n\ndef search():\n    infrastructure.setup()\n    main_author = infrastructure.user_management.get_user(username=\"test\")\n    data = ExampleData(main_author=main_author)\n    upload_id = create_uuid()\n    data.create_upload(upload_id=upload_id, published=True, embargo_length=0)\n    data.create_entry(\n        upload_id=upload_id,\n        entry_id=create_uuid(),\n        mainfile=\"test_content/test_entry/mainfile.json\",\n        results={\n            \"material\": {\"elements\": [\"C\", \"H\"]},\n            \"method\": {},\n            \"properties\": {}\n        }\n    )\n    data.save()\n</code></pre> <p>When running in the online mode (see below), this function will be executed in order to prepare the application backend. The <code>closeAPI</code> function will handle cleaning the test state between successive <code>startAPI</code> calls: it will completely wipe out MongoDB, Elasticsearch and the upload files.</p>"},{"location":"howto/develop/setup.html#running-frontend-tests","title":"Running frontend tests","text":"<p>The tests can be run in two different modes. Offline testing uses pre-recorded files to mock the API traffic during testing. This allows one to run tests more quickly without a server. During online testing, the tests perform calls to a running server where a test state has been prepared. This mode can be used to perform integration tests but also to record the snapshot files needed by the offline testing.</p>"},{"location":"howto/develop/setup.html#offline-testing","title":"Offline testing","text":"<p>This is the way our CI pipeline runs the tests and should be used locally, e.g. whenever you wish to reproduce pipeline errors or when your tests do not involve any API traffic.</p> <ol> <li>Ensure that the GUI artifacts are up-to-date:</li> </ol> <pre><code>./scripts/generate_gui_test_artifacts.sh\n</code></pre> <p>As snapshot tests do not connect to the server, the artifacts cannot be    fetched dynamically from the server and static files need to be used instead.    Note that you should not push these files to Git: the CI/CD pipeline will    automatically generate them.</p> <ol> <li>Run <code>yarn test</code> to run the whole suite or <code>yarn test [&lt;filename&gt;]</code> to run a    specific test.</li> </ol>"},{"location":"howto/develop/setup.html#online-testing","title":"Online testing","text":"<p>When you wish to record API traffic for offline testing, or to perform integration tests, you will need to have a server running with the correct configuration. To do this, follow these steps:</p> <ol> <li> <p>Have the docker infrastructure running: <code>docker compose up</code></p> </li> <li> <p>Have the <code>nomad appworker</code> running with the config found in    <code>gui/tests/nomad.yaml</code>:    <code>export NOMAD_CONFIG=gui/tests/nomad.yaml; nomad admin run appworker</code></p> </li> <li> <p>Activate the correct Python virtual environment before running the tests    with Yarn (Yarn will run the Python functions that prepare the state).</p> </li> <li> <p>Run the tests with <code>yarn test-record [&lt;filename&gt;]</code> if you wish to record a    snapshot file or <code>yarn test-integration [&lt;filename&gt;]</code> if you want the    perform the test without any recording.</p> </li> </ol>"},{"location":"howto/develop/setup.html#build-custom-oasis-image","title":"Build custom Oasis image","text":"<p>To create a custom Oasis image (e.g. with custom plugins), you could use the nomad-distro-template by clicking the <code>Use this template</code> button. For detailed instructions on building and deploying custom Oasis images, please refer to the documentation in that repository.</p>"},{"location":"howto/develop/setup.html#setup-your-ide","title":"Setup your IDE","text":"<p>The documentation section for development guidelines (see below) provide details on how the code is organized, tested, formatted, and documented. To help you meet these guidelines, we recommend to use a proper IDE for development and ditch any Vim/Emacs (mal-)practices.</p> <p>We strongly recommend that all developers use Visual Studio Code (VS Code). (This is a completely different product than Visual Studio.) It is available for free for all major platforms here.</p> <p>You should launch and run VS Code directly from the project's root directory. The source code already contains settings for VS Code in the <code>.vscode</code> directory. The settings contain the same setup for style checks, linter, etc. that is also used in our CI/CD pipelines. In order to actually use the these features, you have to make sure that they are enabled in your own User settings:</p> <pre><code>\"python.linting.mypyEnabled\": true,\n\"python.testing.pytestEnabled\": true,\n\"[python]\": {\n  \"editor.formatOnSave\": true,\n  \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n}\n</code></pre> <p>The settings also include a few launch configuration for VS Code's debugger. You can create your own launch configs in <code>.vscode/launch.json</code> (also in <code>.gitignore</code>).</p> <p>The settings expect that you have installed a Python environment at <code>.pyenv</code> as described in this tutorial (see above).</p>"},{"location":"howto/develop/setup.html#nomad-distro-dev-development-environment-for-the-core-nomad-package-and-nomad-plugins","title":"<code>nomad-distro-dev</code>: Development environment for the core <code>nomad</code> package and NOMAD plugins","text":"<p>When developing <code>nomad</code> alongside multiple plugins, managing different repositories and installations can become complex. <code>nomad-distro-dev</code> simplifies this by allowing you to develop <code>nomad</code> and multiple plugins in one environment.</p> <p>Quick Setup</p> <ol> <li>Fork the nomad-distro-dev repository on GitHub.</li> <li>Follow the setup steps listed in the README of the fork.</li> <li>Once configured, you'll be able to use a single VSCode window to work on nomad and all your plugins.</li> </ol> <p>The setup should generally work without any issues, but there are some troubleshooting tips on this page that you can come back to if needed.</p>"},{"location":"howto/graph-api/basics.html","title":"The Graph(-style) API","text":"<p>A GraphQL-like API is implemented to allow flexible and accurate data fetching.</p> technical details <p>It is categorized as a <code>GraphQL</code>-like API implemented within the <code>REST</code> style framework <code>FastAPI</code>. Because <code>GraphQL</code> requires static, explicitly defined schemas ahead of time while <code>NOMAD</code> supports data with dynamic schema, it cannot be implemented directly using existing <code>GraphQL</code> tools. As a result, there are no GUI tools available unfortunately.</p>"},{"location":"howto/graph-api/basics.html#overview","title":"Overview","text":"<p>In general, <code>REST</code> is good for simple data fetching, while the project gets more complex, APIs in REST style become more complex and less flexible. When building a complex page, often a single request is not enough, and multiple requests are needed to fetch all the necessary data. <code>GraphQL</code> aims to solve this over-/under-fetching problem so that both performance and bandwidth can be optimized.</p> <p>You walk into a restaurant, go through the menu, and order several dishes at once. The kitchen prepares all the dishes and serves them to you, nothing more, nothing less. This is effectively how <code>GraphQL</code> works.</p> <p>In <code>NOMAD</code>, we mimic this behaviour with a <code>GraphQL</code>-like API. The only endpoint involved is <code>/graph/query</code>. All the magic happens there. But before that, we shall first explain some basic concepts.</p>"},{"location":"howto/graph-api/basics.html#basic-usage-data-fetching","title":"Basic Usage (Data Fetching)","text":"<p>Imagine there is an example upload with the upload ID <code>example_upload_id</code>. The metadata of this upload is stored in <code>MongoDB</code>. In our mental model, it is possible to organize all uploads in a tree structure, under the root <code>uploads</code>. This better helps us to define interactions among different data structures (to be explained later).</p> <p>If one uses the endpoint <code>/uploads/{upload_id}</code> to fetch the upload metadata, the response would look like the following (after some restructuring according to the mental model).</p> <pre><code>{\n  \"uploads\":{\n    \"example_upload_id\":{\n      \"process_running\":true,\n      \"current_process\":\"process_upload\",\n      \"process_status\":\"WAITING_FOR_RESULT\",\n      \"last_status_message\":\"Waiting for results (level 0)\",\n      \"complete_time\":\"2025-05-27T10:03:54.115000\",\n      \"upload_id\":\"example_upload_id\",\n      \"upload_name\":\"Free energy simulation\",\n      \"upload_create_time\":\"2025-05-27T10:03:35.048000\",\n      \"published\":false,\n      \"with_embargo\":false,\n      \"embargo_length\":0,\n      \"license\":\"CC BY 4.0\"\n    }\n  }\n}\n</code></pre> <p>How would one tell the server if, say, for example, only <code>upload_name</code> is needed? With <code>GraphQL</code>, one simply needs to 'ask for what you need', following the structure of the data. To mimic this, the request may look like the following.</p> <pre><code>{\n  \"uploads\":{\n    \"example_upload_id\":{\n      \"upload_name\":\"I want this!\",\n    }\n  }\n}\n</code></pre> <p>But it is not practical to use a string to express potentially complex intentions. Instead, we want to use a more structured way to express the request. To this end, <code>NOMAD</code> defines a request configuration model (referred to as 'config').</p> <pre><code>class RequestConfig(BaseModel):\n    \"\"\"\n    A class to represent the query configuration.\n    An instance of `RequestConfig` shall be attached to each required field.\n    The `RequestConfig` is used to determine the following.\n        1. Whether the field should be included/excluded.\n        2. For reference, whether the reference should be resolved, and how to resolve it.\n    Each field can be handled differently.\n    \"\"\"\n\n    directive: DirectiveType = Field(\n        DirectiveType.plain,\n        description=\"\"\"\n        Indicate whether to include or exclude the current quantity/section.\n        References can be resolved using `resolved`.\n        The `*` is a shortcut of `plain`.\n        \"\"\",\n    )\n\n    # ... other fields omitted for brevity ...\n</code></pre> complete definition <p>The complete definition of the <code>RequestConfig</code> can be found in <code>nomad/graph/model.py</code>.</p> <p>To fetch the desired field, one shall attach a <code>RequestConfig</code> under the key <code>m_request</code>. The <code>plain</code> directive tells the server to include the field in the response.</p> <pre><code>{\n  \"uploads\":{\n    \"example_upload_id\":{\n      \"upload_name\":{\"m_request\":{\"directive\":\"plain\"}}\n    }\n  }\n}\n</code></pre> <p>Now it is possible to fetch whatever wanted from the upload metadata. For example, if one wants to fetch the <code>upload_name</code> and <code>upload_create_time</code>, the request would look like this:</p> <pre><code>{\n  \"uploads\":{\n    \"example_upload_id\":{\n      \"upload_name\":{\"m_request\":{\"directive\":\"plain\"}},\n      \"upload_create_time\":{\"m_request\":{\"directive\":\"plain\"}}\n    }\n  }\n}\n</code></pre>"},{"location":"howto/graph-api/basics.html#existing-data-resources","title":"Existing Data Resources","text":"<p>As of this writing, there are a few existing data resources (called <code>Documents</code>) stored in <code>MongoDB</code>:</p> <ol> <li><code>uploads</code> (stored in MongoDB): The metadata of an upload, including, <code>upload_id</code>, <code>upload_name</code>, <code>main_author</code>, etc.</li> <li><code>entries</code> (stored in MongoDB): The metadata of an entry, including, <code>entry_id</code>, <code>entry_create_time</code>, <code>mainfile</code>, etc.</li> <li><code>datasets</code> (stored in MongoDB): The metadata of a dataset, including, <code>dataset_id</code>, <code>dataset_name</code>, <code>user_id</code>, etc.</li> <li><code>groups</code> (stored in MongoDB): The metadata of a user group, including, <code>owner</code>, <code>members</code>, etc.</li> </ol> <p>One can apply the same logic to fetch data from these structures. For example, to fetch the <code>entry_id</code> and <code>entry_create_time</code> of an entry with ID <code>example_entry_id</code>, the request would look like this:</p> <pre><code>{\n  \"entries\":{\n    \"example_entry_id\":{\n      \"entry_id\":{\"m_request\":{\"directive\":\"plain\"}},\n      \"entry_create_time\":{\"m_request\":{\"directive\":\"plain\"}}\n    }\n  }\n}\n</code></pre> <p>The top-level keys should be one of <code>uploads</code>, <code>entries</code>, <code>datasets</code>, or <code>groups</code> to indicate which data resource to query.</p> <p>This concludes the basic usage of the <code>/graph/query</code> endpoint. There are other resources to explore, which will be explained in the subsequent pages.</p>"},{"location":"howto/graph-api/graph.structure.html","title":"The Graph Structure","text":"<p>In the previous page, we explained how to fetch data from existing data structures. This along does not make it a 'graph' because we only have isolated 'nodes'. To navigate from one data structure to another, we need to introduce 'edges' to link 'nodes' together.</p>"},{"location":"howto/graph-api/graph.structure.html#graph-edges","title":"Graph Edges","text":"<p>The data structures in <code>NOMAD</code> are inherently linked together. For example, an upload is a collection of entries, an entry corresponding to an archive, a user group is a collection of users, each user may be the owner of several uploads, etc.</p> <p>Thus, it is natural to link these data structures together via special tokens. The following figure illustrates the relationships between the data structures in <code>NOMAD</code> with the special tokens highlighted.</p> <p></p> <p>For example, if there is an upload with ID <code>example_upload_id</code>, and it has an entry with ID <code>example_entry_id</code>, the request to fetch the <code>entry_id</code>and <code>entry_create_time</code> of the entry, together with <code>upload_create_time</code> in the upload, would look like this.</p> <pre><code>{\n  \"uploads\": {\n    \"example_upload_id\": {\n      \"upload_create_time\": {\"m_request\": {\"directive\": \"plain\"}},\n      \"entries\": {\n        \"example_entry_id\": {\n          \"entry_id\": {\"m_request\": {\"directive\": \"plain\"}},\n          \"entry_create_time\": {\"m_request\": {\"directive\": \"plain\"}}\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Here it uses the special token <code>entries</code> to navigate from the upload to the entry. If needed, one can further navigate from the entry to the archive, or from the upload to the file system, etc. This is the essence of the graph: to link data structures together via edges, allowing for complex queries and data retrieval.</p> <p>It shall be noted that those 'special tokens' do not exist in the original documents stored in <code>MongoDB</code>. They are defined by the graph API to establish the relationships between the data structures.</p>"},{"location":"howto/graph-api/graph.structure.html#fuzzy-fetching","title":"Fuzzy Fetching","text":"<p>Imagine we start with an upload with a known ID <code>example_upload_id</code>, and we want to find all entries that belong to this upload. How can we achieve this without knowing the exact entry IDs a priori?</p> <p>One can use the special key <code>*</code> to represent all entries under the upload. Thus, all entries under the upload can be fetched as follows:</p> <pre><code>{\n  \"uploads\": {\n    \"example_upload_id\": {\n      \"entries\": {\n        \"*\": {\n          \"entry_id\": {\"m_request\": {\"directive\": \"plain\"}},\n          \"entry_create_time\": {\"m_request\": {\"directive\": \"plain\"}}\n        }\n      }\n    }\n  }\n}\n</code></pre> default pagination <p>To avoid performance issues, the server will paginate the results by default. To control the pagination, one can use the <code>pagination</code> field in the request configuration (see below).</p> no universality <p>The <code>*</code> wildcard is not universal and only works for homogeneous data. This means it can only be used to represent <code>upload_id</code>, <code>entry_id</code>, <code>dataset_id</code>, etc., for data that follows a fixed schema (<code>MongoDB</code>). It won't work in archives, the corresponding metainfo (definitions), and alike.</p>"},{"location":"howto/graph-api/graph.structure.html#fuzzy-query","title":"Fuzzy Query","text":"<p>The request configuration allows one to perform fuzzy queries to further filter data before fetching, via the <code>query</code> and <code>pagination</code> fields.</p> <pre><code>class RequestConfig(BaseModel):\n    # ... other fields omitted for brevity ...\n\n    pagination: None | dict = Field(\n        None,\n        description=\"\"\"\n        The pagination configuration used for MongoDB search.\n        This setting does not propagate to its children.\n        For Token.ENTRIES, Token.UPLOADS and Token.DATASETS, different validation rules apply.\n        Please refer to `DatasetPagination`, `UploadProcDataPagination`, `MetadataPagination` for details.\n        \"\"\",\n    )\n    query: None | dict = Field(\n        None,\n        description=\"\"\"\n        The query configuration used for either mongo or elastic search.\n        This setting does not propagate to its children.\n        It can only be defined at the root levels including Token.ENTRIES, Token.UPLOADS and 'm_datasets'.\n        For Token.ENTRIES, the query is used in elastic search. It must comply with `WithQuery`.\n        For Token.UPLOADS, the query is used in mongo search. It must comply with `UploadProcDataQuery`.\n        For Token.DATASETS, the query is used in mongo search. It must comply with `DatasetQuery`.\n        For Token.GROUPS, the query is used in mongo search. It must comply with `UserGroupQuery`.\n        \"\"\",\n    )\n\n    # ... other fields omitted for brevity ...\n</code></pre> <p>To instruct the server to perform a query, one need to attach a request config with valid <code>query</code> and <code>pagination</code> (optional) with the <code>m_request</code> key under the root level. The root level is the top-level following the special tokens, such as <code>uploads</code>, <code>entries</code>, etc.</p> <p>Combined with fuzzy fetching, one can perform filter and fetch. For example, if one wants to fetch all entries under an upload with a specific parser name, the request would look like this.</p> <pre><code>{\n  \"uploads\":{\n    \"example_upload_id\":{\n      \"entries\":{\n        \"m_request\":{ \"query\":{ \"parser_name\":\"desired_parser\" } },\n        \"*\":{\n          \"entry_id\":{ \"m_request\":{ \"directive\":\"plain\" } },\n          \"entry_create_time\":{ \"m_request\":{ \"directive\":\"plain\" } }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>As explained in the comments, depending on the specific token, the query must comply with the corresponding query model. The <code>pagination</code> field shall also comply with the corresponding pagination model. Those models are used in conventional REST endpoints, and one can find more details in the corresponding documentation.</p>"},{"location":"howto/manage/eln.html","title":"How to use ELNs","text":"<p>This guide describes how to manually create entries and enter information via ELNs (electronic lab notebooks). NOMAD ELNs allow you to acquire consistently structured data from users to augment uploaded files.</p> <p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"howto/manage/eln.html#create-a-basic-eln-entry","title":"Create a basic ELN entry","text":"<p>Go to <code>PUBLISH</code> / <code>Uploads</code>. Here you can create an upload with the <code>CREATE A NEW UPLOAD</code> button. This will bring you to the upload page.</p> <p>Click the <code>CREATE ENTRY</code> button. This will bring-up a dialog to choose an ELN schema. All ELNs (as any entry in NOMAD) needs to follow a schema. You can choose from uploaded custom schemas or NOMAD built-in schemas. You can choose the <code>Basic ELN</code> to create a simple ELN entry.</p> <p>The name of your ELN entry, will be the filename for your ELN without the <code>.archive.json</code> ending that will be added automatically. You can always find and download your ELNs on the <code>FILES</code> tab.</p> <p>The <code>Basic ELN</code> offers you simple fields for a name, tags, a date/time, and a rich text editor to enter your notes.</p>"},{"location":"howto/manage/eln.html#add-your-own-eln-schema","title":"Add your own ELN schema","text":"<p>To make NOMAD ELNs more useful, you can define your own schema to create you own data fields, create more subsections, reference other entries, and much more.</p> <p>You should have a look at our ELN example upload. Go to <code>PUBLISH</code> / <code>Uploads</code> and click the <code>ADD EXAMPLE UPLOADS</code> button. The <code>Electronic Lab Notebook</code> example, will contain a schema and entries that instantiate different parts of the schema. The *ELN example sample (<code>sample.archive.json</code>) demonstrates what you can do.</p> <p>Follow the How-to write a schema and How-to define ELN guides to create you own customized of ELNs.</p>"},{"location":"howto/manage/eln.html#integration-of-third-party-elns","title":"Integration of third-party ELNs","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p> <p>The code base is part of the FAIRmat-NFDI{:target=\"_ blank\"} repository.</p> <p>NOMAD offers integration with third-party ELN providers, simplifying the process of connecting and interacting with external platforms. Three main external ELN solutions that are integrated into NOMAD are: elabFTW, Labfolder and chemotion. The process of data retrieval and data mapping onto NOMAD's schema varies for each of these third-party ELN provider as they inherently allow for certain ways of communicating with their database. Below you can find a How-to guide on importing your data from each of these external repositories.</p>"},{"location":"howto/manage/eln.html#elabftw-integration","title":"elabFTW integration","text":"<p>elabFTW is part of the ELN Consortium and supports exporting experimental data in ELN file format. ELNFileFormat is a zipped file that contains metadata of your elabFTW project along with all other associated data of your experiments.</p> <p>How to import elabFTW data into NOMAD:</p> <p>Go to your elabFTW experiment and export your project as <code>ELN Archive</code>. Save the file to your filesystem under your preferred name and location (keep the <code>.eln</code> extension intact). To parse your ebalFTW data into NOMAD, go to the upload page of NOMAD and create a new upload. In the <code>overview</code> page, upload your exported file (either by drag-dropping it into the click or drop files box or by navigating to the path where you stored the file). This causes triggering NOMAD's parser to create as many new entries in this upload as there are experiments in your elabFTW project.</p> <p>You can inspect the parsed data of each of your entries (experiments) by going to the DATA tab of each Entry page. Under Entry column, click on data section. Now a new lane titled <code>ElabFTW Project Import</code> should be visible. Under this section, (some of) the metadata of your project is listed. There two subsections: 1) experiment_data, and 2) experiment_files.</p> <p>experiment_data section contains detailed information of the given elabFTW experiment, such as links to external resources and extra fields. experiment_files section is a list of subsections containing metadata and additional info of the files associated with the experiment.</p>"},{"location":"howto/manage/eln.html#labfolder-integration","title":"Labfolder integration","text":"<p>Labfolder provides API endpoints to interact with your ELN data. NOMAD makes API calls to retrieve, parse and map the data from your Labfolder instance/database to a NOMAD's schema. To do so, the necessary information are listed in the table below:</p> <p>project_url:         The URL address to the Labfolder project. it should follow this pattern:         'https://your-labfolder-server/eln/notebook#?projectIds=your-project-id'. This is used to setup         the server and initialize the NOMAD schema.</p> <p>labfolder_email:         The email (user credential) to authenticate and login the user. Important Note: this         information is discarded once the authentication process is finished.</p> <p>password:         The password (user credential) to authenticate and login the user. Important Note: this         information is discarded once the authentication process is finished.</p> <p>How to import Labfolder data into NOMAD:</p> <p>To get your data transferred to NOMAD, first go to NOMAD's upload page and create a new upload. Then click on <code>CREATE ENTRY</code> button. Select a name for your Entry and pick <code>Labfolder Project Import</code> from the <code>Built-in schema</code> dropdown menu. Then click on <code>CREATE</code>. This creates an Entry where you can insert your user information. Fill the <code>Project url</code>, <code>Labfolder email</code> and <code>password</code> fields. Once completed, click on the <code>save icon</code> in the top-right corner of the screen. This triggers NOMAD's parser to populate the schema of current ELN. Now the metadata and all files of your Labfolder project should be populated in this Entry.</p> <p>The <code>elements</code> section lists all the data and files in your projects. There are 6 main data types returned by Labfolder's API: <code>DATA</code>, <code>FILE</code>, <code>IMAGE</code>, <code>TABLE</code>, <code>TEXT</code> and <code>WELLPLATE</code>. <code>DATA</code> element is a special Labfolder element where the data is structured in JSON format. Every data element in NOMAD has a special <code>Quantity</code> called <code>labfolder_data</code> which is a flattened and aggregated version of the data content. <code>IMAGE</code> element contains information of any image stored in your Labfolder project. <code>TEXT</code> element contains data of any text field in your Labfodler project.</p>"},{"location":"howto/manage/eln.html#chemotion-integration","title":"Chemotion integration","text":"<p>NOMAD supports importing your data from Chemotion repository via <code>chemotion</code> parser. The parser maps your data that is structured under chemotion schema, into a predefined NOMAD schema. From your Chemotion repo, you can export your entire data as a zip file which then is used to populate NOMAD schema.</p> <p>How to import Chemotion data into NOMAD:</p> <p>Go to your Chemotion repository and export your project. Save the file to your filesystem under your preferred name and location (<code>your_file_name.zip</code>). To get your data parsed into NOMAD, go to the upload page of NOMAD and create a new upload. In the <code>overview</code> page, upload your exported file (either by drag-dropping it into the click or drop files box or by navigating to the path where you stored the file). This causes triggering NOMAD's parser to create one new Entry in this upload.</p> <p>You can inspect the parsed data of each of this new Entry by navigating to the DATA tab of the current Entry page. Under Entry column, click on data section. Now a new lane titled <code>Chemotion Project Import</code> should be visible. Under this section, (some of) the metadata of your project is listed. Also, there are various (sub)sections which are either filled depending on whether your datafile contains information on them.</p> <p>If a section contains an image (or attachment) it is appended to the same section under <code>file</code> Quantity.</p>"},{"location":"howto/manage/eln.html#openbis-integration","title":"Openbis integration","text":"<p>Openbis provides API endpoints to interact with your ELN data. NOMAD makes API calls to retrieve, parse, and map the data from your Openbis instance/database to NOMAD's schema. The necessary information is listed in the table below:</p> <ul> <li>project_url: The URL address to the Openbis project. It should follow this pattern: <code>https://openbis.example.com</code>.   This is used to set up the server and initialize the NOMAD schema.</li> <li>username: The username (user credential) to authenticate and log in the user. Important Note: this information   is discarded once the authentication process is finished.</li> <li>password: The password (user credential) to authenticate and log in the user. Important Note: this information   is discarded once the authentication process is finished.</li> </ul>"},{"location":"howto/manage/eln.html#how-to-import-openbis-data-into-nomad","title":"How to Import Openbis Data into NOMAD","text":"<p>To get your data transferred to NOMAD, follow these steps:</p> <ol> <li>Go to NOMAD's upload page and create a new upload.</li> <li>Click on the <code>CREATE ENTRY</code> button.</li> <li>Select a name for your entry and pick <code>Openbis Project Import</code> from the <code>Built-in schema</code> dropdown menu.</li> <li>Click on <code>CREATE</code>. This creates an entry where you can insert your user information.</li> <li>Fill in the <code>project url</code>, <code>username</code>, and <code>password</code> fields.</li> <li>Once completed, click on the save icon in the top-right corner of the screen. This triggers NOMAD's parser to    populate the schema of the current ELN. Now, the metadata and all files of your Openbis project should be populated    in this entry.</li> </ol> <p>The normalizer will search for all entries in your Openbis project and attempt to import them one by one.</p>"},{"location":"howto/manage/eln.html#under-development","title":"Under Development","text":"<p>The integration of third-party ELNs suite is planned to be moved to a new plug-in mechanism to allow for a smoother interface for interacting with other ELN providers.</p>"},{"location":"howto/manage/explore.html","title":"How to explore data","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>While we are still working on this, please use our video tutorial as a starting point:</p>"},{"location":"howto/manage/north.html","title":"How to use NORTH","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"howto/manage/upload.html","title":"How to upload and publish data for supported formats","text":"<p>This guide describes how to upload data in NOMAD supported file formats. You find a list of supported formats on top of each upload page, see below.</p>"},{"location":"howto/manage/upload.html#preparing-files","title":"Preparing files","text":"<p>You can upload files one by one, but you can also provider larger <code>.zip</code> or <code>.tar.gz</code> archive files, if this is easier to you. Also the file upload via frp or command line with curl or with wget generates an archive files. The specific layout of these files is up to you. NOMAD will simply extract them and consider the whole directory structure within.</p>"},{"location":"howto/manage/upload.html#create-an-upload-and-add-files","title":"Create an upload and add files","text":"<p>Open NOMAD and log in; if you don't have a NOMAD account, please create one.</p> <p>Go to <code>PUBLISH</code> / <code>Uploads</code>. Here you can create an upload with the <code>CREATE A NEW UPLOAD</code> button. This will bring you to the upload page.</p> <p>Before you start, make sure that the size of your data does not exceed the upload limits. If it does, please contact us.</p> <p>You can drop your files on (or click) the <code>CLICK OR DROP FILES</code> button. On top you will see a list of supported file formats and details on the files to upload. You can also go to the <code>FILES</code> tab. Here you can create directories and drop files into directories.</p>"},{"location":"howto/manage/upload.html#processing-files","title":"Processing files","text":"<p>NOMAD interprets your files. It checks each file and recognizes the main output file of the supported codes. NOMAD creates an entry for this mainfile that represents the respective data of this code run, experiment, etc.</p> <p>While you can browse all files of an upload from its upload page, NOMAD only allows to search for such recognized mainfiles. As long as your upload does not contain any files that are recognized by NOMAD, you cannot publish the data.</p> <p>However, all files that are associated to a recognized mainfile by being in the same directory are displayed as auxiliary files next to the entry represented by the mainfile.</p> <p>Note</p> <p>A note for VASP users. On the handling of POTCAR files: NOMAD takes care of it; you don't need to worry about it. We understand that POTCAR files are not supposed to be visible to the public according to your VASP license. Thus, in agreement with Georg Kresse, NOMAD extracts the most important information of POTCAR files and stores it in the files named <code>POTCAR.stripped</code>. These files can be accessed and downloaded by anyone, while the original POTCAR files are automatically removed.</p>"},{"location":"howto/manage/upload.html#visibility-and-access","title":"Visibility and access","text":"<p>You can edit who has access to your upload. Click on the Edit upload members button to open the corresponding dialog. There you can search for users (or groups, see below) and add them to the list as reviewers, who can view your upload before it is published.</p> <p>The table below the search field shows who has been added and their role. In each row you can remove the member or change their role from Reviewer to Co-author, who may also edit the upload. You must confirm the dialog for the changes to take effect.</p> <p>You can also make the upload visible for everyone, even not logged in users, by using the checkbox in the same upload step. Note that you cannot use this feature if the upload is published with embargo and vice versa.</p> <p>Note</p> <p>Instead of single users you may also give access via user groups. Switch the search type from User to Group, search for the group name, and optionally change their role. This feature must be enabled in the config file <code>nomad.yaml</code>.</p> <p>See API on how to create and edit user groups.</p>"},{"location":"howto/manage/upload.html#add-user-metadata","title":"Add user metadata","text":"<p>NOMAD automatically extracts as much information as possible from your files but you can still specify additional metadata. This is what we call user metadata. This includes comments, additional web-references, and datasets (use the edit metadata function on the upload page).</p> <p>User metadata can also be provided in an uploaded file, either named <code>nomad.json</code> or <code>nomad.yaml</code>. Here is a JSON example:</p> <pre><code>{\n    \"comment\": \"Data from a cool research project\",\n    \"references\": [\"http://archivex.org/mypaper\"],\n    \"coauthors\": [\n        \"&lt;email-or-username&gt;\",\n        \"&lt;email-or-username&gt;\"\n    ],\n    \"datasets\": [\n        \"&lt;dataset-name&gt;\"\n    ],\n    \"entries\": {\n        \"path/to/entry_dir/vasp.xml\": {\n            \"comment\": \"An entry specific comment.\"\n        }\n    }\n}\n</code></pre> <p>This file is only applied during the initial processing of an entry. So make sure you either upload it first or with everything else as part of an archive file.</p>"},{"location":"howto/manage/upload.html#publish-and-get-a-doi","title":"Publish and get a DOI","text":"<p>After clicking the <code>PUBLISH</code> button, the uploaded files will become immutable, but you can still edit the metadata.</p> <p>As part of the edit metadata functionality, you can create and assign datasets. Go to <code>PUBLISH</code> / <code>Datasets</code> in the menu to see all your datasets. Here you can assign a DOI to created datasets. For a dataset with DOI, you can only add more entries, but not remove entries.</p>"},{"location":"howto/manage/upload.html#upload-limits","title":"Upload limits","text":"<ul> <li>One upload cannot exceed 32 GB in size.</li> <li>Only 10 non published uploads are allowed per user.</li> <li>Only uploads with at least one recognized entry can be published. See also supported codes/formats below.</li> </ul>"},{"location":"howto/manage/upload.html#strategies-for-large-amounts-of-data","title":"Strategies for large amounts of data","text":"<p>Before attempting to upload large amounts of data, run some experiments with a representative and small subset of your data. Use this to simulate a larger upload that you can review and edit in the normal way. You do not have to publish this test upload; simply delete it before publish, once you are satisfied with the results.</p> <p>Ask for assistance and Contact us in advance. This will allow us to react to your specific situation and eventually prepare additional measures. Allow enough time before you need your data to be published. Adding multiple hundreds of GBs to NOMAD isn't a trivial feat and will take some time and effort on all sides.</p> <p>The upload limits above are necessary to keep NOMAD data manageable and we cannot easily grant exceptions to these rules. This means you have to split your data into 32 GB uploads. Uploading these files, observing the processing, and publishing the data can be automatized through NOMAD APIs.</p> <p>When splitting your data, it is important to not split subdirectories that contain files of the same single entry. NOMAD can only bundle those related files to an entry if they are part of the same upload (and directory). Therefore, there is no single recipe to follow, and a script to split your data depends heavily on how your data is organized.</p> <p>If you provide data for a potentially large amount of entries, it might be advisable to provide user metadata via file. See user metadata above for details.</p> <p>To further automate, you can also upload and directly publish data. After performing some smaller test uploads, you should consider skipping our staging and publish the upload right away. This can save you some time and additional API calls. The upload endpoint has a parameter <code>publish_directly</code>. You can modify the upload command you get on the upload page as follows:</p> <pre><code>curl \"http://nomad-lab.eu/prod/v1/uploads/?token=&lt;your-token&gt;&amp;publish_directly=true\" -T &lt;local_file&gt;\n</code></pre> <p>HTTP makes it easy for you to upload files via browser and curl, but it is not an ideal protocol for the stable transfer of large and many files. Alternatively, we can organize a separate manual file transfer to our servers. We will put your prepared upload files (.zip or .tag.gz) on a predefined path on the NOMAD servers. NOMAD allows to \"upload\" files directly from its servers via an additional <code>local_path</code> parameter:</p> <pre><code>curl -X PUT \"http://nomad-lab.eu/prod/v1/api/uploads/?token=&lt;your-token&gt;&amp;local_path=&lt;path-to-upload-file&gt;\"\n</code></pre>"},{"location":"howto/oasis/admin.html","title":"How to perform admin tasks","text":""},{"location":"howto/oasis/admin.html#backups","title":"Backups","text":"<p>To backup your Oasis at least the file data and mongodb data needs to be saved. You determined the path to your file data (your uploads) during the installation. By default all data is stored in a directory called <code>.volumes</code> that is created in the current working directory of your installation/docker-compose. This directory can be backed up like any other file backup (e.g. rsync).</p> <p>To backup the mongodb, please refer to the official mongodb documentation. We suggest a simple mongodump export that is backed up alongside your files. The default configuration mounts <code>.volumes/mongo</code> into the mongodb container (as <code>/backup</code>) for this purpose. You can use this to export the NOMAD mongo database. Combine this with rsync on the <code>.volumes</code> directory and everything should be set. To create a new mongodump run:</p> <pre><code>docker exec nomad_oasis_mongo mongodump -d nomad_oasis_v1 -o /backup\n</code></pre> <p>The elasticsearch contents can be reproduced with the information in the files and the mongodb.</p> <p>To create a new oasis with the backup data, create the <code>.volumes</code> directory from your backup. Start the new oasis. Use mongorestore:</p> <pre><code>docker exec nomad_oasis_mongo mongorestore /backup\n</code></pre> <p>Now you still have to recreate the elasticsearch index:</p> <pre><code>docker exec nomad_oasis_app python -m nomad.cli admin uploads index\n</code></pre>"},{"location":"howto/oasis/admin.html#managing-data-with-the-cli","title":"Managing data with the CLI","text":"<p>The NOMAD command line interface (CLI) provides a few useful administrative functions. To use the NOMAD CLI, open a shell into the app container and run it from there:</p> <pre><code>docker exec -ti nomad_oasis_app bash\n</code></pre> <p>For example you can ls or remove uploads:</p> <pre><code>nomad admin uploads ls\nnomad admin uploads rm -- &lt;upload_id&gt; &lt;upload_id&gt;\n</code></pre> <p>You can also reset the processing (of \"stuck\") uploads and reprocess:</p> <pre><code>nomad admin uploads reset -- &lt;upload_id&gt;\nnomad admin uploads process -- &lt;upload_id&gt;\n</code></pre> <p>You can also use the CLI to wipe the whole installation:</p> <pre><code>nomad admin reset --i-am-really-sure\n</code></pre>"},{"location":"howto/oasis/admin.html#upload-commands","title":"Upload commands","text":"<p>The <code>nomad admin uploads</code> group of CLI commands allow you to inspect and modify all or some uploads in your installation. Sub-commands include <code>ls</code>, <code>rm</code>, <code>chown</code>, <code>process</code> (see below), <code>index</code> (see below).</p> <p>The command group takes many different parameters to target specific subsets of uploads. Here are a few examples:</p> <ul> <li><code>--unpublished</code></li> <li><code>--published</code></li> <li><code>--outdated</code> Select published uploads with older NOMAD versions than the current</li> <li><code>--processing-failure</code> Uploads with processing failures.</li> </ul> <p>For a complete list refer to the CLI reference documentation.</p> <p>Alternatively, you can use a list of upload ids at the end of the command, e.g.:</p> <pre><code>nomad admin uploads ls -- &lt;id1&gt; &lt;id2&gt;\n</code></pre> <p>If you have a list of ids (e.g. in a file), you could use <code>xargs</code>:</p> <pre><code>cat file_with_ids.txt | xargs nomad admin uploads ls --\n</code></pre>"},{"location":"howto/oasis/admin.html#re-processing","title":"Re-processing","text":"<p>Processing includes the conversion of raw files into NOMAD entries. Files are parsed, normalizers are called, the processing results are stored, and the search index is updated. In certain scenarios (failed processing, migration, changed plugins) might require that admins process certain uploads again.</p> <pre><code>nomad admin uploads process\n</code></pre>"},{"location":"howto/oasis/admin.html#re-indexing","title":"Re-Indexing","text":"<p>Each NOMAD entry is represented in NOMAD's search index. Only if an entry is in this index, you can find it via the search interface. Some changes between NOMAD versions (see also our update guide), might require that you re-index all uploads.</p> <pre><code>nomad admin uploads index\n</code></pre>"},{"location":"howto/oasis/admin.html#restricting-access-to-your-oasis","title":"Restricting access to your Oasis","text":"<p>An Oasis works exactly the same way the official NOMAD works. It is open and everybody can access published data. Everybody with an account can upload data. This might not be what you want.</p> <p>Currently there are two ways to restrict access to your Oasis. First, you do not expose the Oasis to the public internet, e.g. you only make it available on an intra-net or through a VPN.</p> <p>Second, we offer a simple white-list mechanism. As the Oasis administrator you provide a list of accounts as part of your Oasis configuration. To use the Oasis, all users have to be logged in and be on your white list of allowed users. To enable white-listing, you can provide a list of NOMAD account email addresses in your <code>nomad.yaml</code> like this:</p> <pre><code>oasis:\n    allowed_users:\n        - user1@gmail.com\n        - user2@gmail.com\n</code></pre>"},{"location":"howto/oasis/admin.html#configuring-for-performance","title":"Configuring for performance","text":"<p>If you run the OASIS on a single computer, like described here (either with docker or bare linux), you might run into problems with processing large uploads. If the NOMAD worker and app are run on the same computer, the app might become unresponsive, when the worker consumes all system resources.</p> <p>By default, the worker container might have as many worker processes as the system as CPU cores. In addition, each worker process might spawn additional threads and consume more than one CPU core.</p> <p>There are multiple ways to restrict the worker's resource consumption:</p> <ul> <li>limit the number of worker processes and thereby lower the number of used cores</li> <li>disable or restrict multithreading</li> <li>limit available CPU utilization of the worker's docker container with docker</li> </ul>"},{"location":"howto/oasis/admin.html#limit-the-number-of-worker-processes","title":"Limit the number of worker processes","text":"<p>The worker uses the Python package celery. Celery can be configured to use less than the default number of worker processes (which equals the number of available cores). To use only a single core only, you can alter the worker service command in the <code>docker-compose.yml</code> and add a <code>--concurrency</code> argument:</p> <pre><code>command: python -m celery -A nomad.processing worker -l info --concurrency=1 -Q celery\n</code></pre> <p>See also the celery documentation.</p>"},{"location":"howto/oasis/admin.html#limiting-the-use-of-threads","title":"Limiting the use of threads","text":"<p>You can also reduce the usable threads that Python packages based on OpenMP could use to reduce the threads that might be spawned by a single worker process. Simply set the <code>OMP_NUM_THREADS</code> environment variable in the worker container in your <code>docker-compose.yml</code>:</p> <pre><code>services:\n    worker:\n        ...\n        environment:\n            ...\n            OMP_NUM_THREADS: 1\n</code></pre>"},{"location":"howto/oasis/admin.html#limit-cpu-with-docker","title":"Limit CPU with docker","text":"<p>You can add a <code>deploy.resources.limits</code> section to the worker service in the <code>docker-compose.yml</code>:</p> <pre><code>services:\n    worker:\n        ...\n        deploy:\n            resources:\n                limits:\n                    cpus: '0.50'\n</code></pre> <p>The number refers to the percentage use of a single CPU core. See also the docker-compose documentation.</p>"},{"location":"howto/oasis/configure.html","title":"Configure an Oasis","text":"<p>Originally, the NOMAD Central Repository is a service that runs at the Max-Planck's computing facility in Garching, Germany. However, the NOMAD software is Open-Source, and everybody can run it. Any service that uses NOMAD software independently is called a NOMAD Oasis. A NOMAD Oasis does not need to be fully isolated. For example, you can publish uploads from your NOMAD Oasis to the central NOMAD installation.</p> <p>Note</p> <p>Register your Oasis If you installed (or even just plan to install) a NOMAD Oasis, please register your Oasis with FAIRmat and help us to assist you in the future.</p>"},{"location":"howto/oasis/configure.html#creating-a-nomad-distribution-for-your-oasis","title":"Creating a NOMAD distribution for your Oasis","text":"<p>The configuration for a NOMAD Oasis is defined in a NOMAD distribution project. We provide a template for these distribution projects. A NOMAD distribution project contains all necessary config files and will allow you to version your configuration, install and configure plugins, build custom images automatically, and much more.</p> <p>For a production installation, we recommend to create your own distribution project based on the template by pressing the \"use this template\" button on the top right of the template's GitHub page. If you wish to instead try out the default setup locally, follow the instructions in \"Try  NOMAD Oasis locally\".</p> Try NOMAD Oasis locally <p>This is an example of how you would deploy a simple, single-machine NOMAD Oasis on your computer. This is meant only as an example and you should see our documentation on Deploying an Oasis for more details on setting up a production deployment.</p> <ol> <li> <p>Make sure you have docker installed.     Docker nowadays comes with <code>docker compose</code> built in. Prior, you needed to     install the stand-alone docker-compose.</p> </li> <li> <p>Clone the <code>nomad-distro-template</code> repository or download the repository as a zip file.</p> <pre><code>git clone https://github.com/FAIRmat-NFDI/nomad-distro-template.git\ncd nomad-distro-template\n</code></pre> <p>or</p> <pre><code>curl-L -o nomad-distro-template.zip \"https://github.com/FAIRmat-NFDI/nomad-distro-template/archive/main.zip\"\nunzip nomad-distro-template.zip\ncd nomad-distro-template\n</code></pre> </li> <li> <p>On Linux only, recursively change the owner of the <code>.volumes</code> directory to the nomad user (1000)</p> <pre><code>sudo chown -R 1000 .volumes\n</code></pre> </li> <li> <p>Pull the images specified in the <code>docker-compose.yaml</code></p> <p>Note that the image needs to be public or you need to provide a PAT (see \"Important\" note above).</p> <pre><code>docker compose pull\n</code></pre> </li> <li> <p>And run it with docker compose in detached (--detach or -d) mode</p> <pre><code>docker compose up -d\n</code></pre> </li> <li> <p>Optionally you can now test that NOMAD is running with</p> <pre><code>curl localhost/nomad-oasis/alive\n</code></pre> </li> <li> <p>Finally, open http://localhost/nomad-oasis in your browser to start using your new NOMAD Oasis.</p> </li> </ol> <p>To run NORTH (the NOMAD Remote Tools Hub), the <code>hub</code> container needs to run docker and the container has to be run under the docker group. You need to replace the default group id <code>991</code> in the <code>docker-compose.yaml</code>'s <code>hub</code> section with your systems docker group id. Run <code>id</code> if you are a docker user, or <code>getent group | grep docker</code> to find your systems docker gid. The user id 1000 is used as the nomad user inside all containers.</p>"},{"location":"howto/oasis/configure.html#configuring-your-installation","title":"Configuring your installation","text":""},{"location":"howto/oasis/configure.html#sharing-data-through-log-transfer-and-data-privacy-notice","title":"Sharing data through log transfer and data privacy notice","text":"<p>NOMAD includes a log transfer functions. When enabled this it automatically collects and transfers non-personalized logging data to us. Currently, this functionality is experimental and requires opt-in. However, in upcoming versions of NOMAD Oasis, we might change to out-out.</p> <p>To enable this functionality add <code>logtransfer.enabled: true</code> to you <code>nomad.yaml</code>.</p> <p>The service collects log-data and aggregated statistics, such as the number of users or the number of uploaded datasets. In any case this data does not personally identify any users or contains any uploaded data. All data is in an aggregated and anonymized form.</p> <p>The data is solely used by the NOMAD developers and FAIRmat, including but not limited to:</p> <ul> <li>Analyzing and monitoring system performance to identify and resolve issues.</li> <li>Improving our NOMAD software based on usage patterns.</li> <li>Generating aggregated and anonymized reports.</li> </ul> <p>We do not share any collected data with any third parties.</p> <p>We may update this data privacy notice from time to time to reflect changes in our data practices. We encourage you to review this notice periodically for any updates.</p>"},{"location":"howto/oasis/configure.html#using-the-central-user-management","title":"Using the central user management","text":"<p>Our recommendation is to use the central user management provided by nomad-lab.eu. We simplified its use and you can use it out-of-the-box. You can even run your system from <code>localhost</code> (e.g. for initial testing). The central user management system is not communicating with your OASIS directly. Therefore, you can run your OASIS without exposing it to the public internet.</p> <p>There are two requirements. First, your users must be able to reach the OASIS. If a user is logging in, she/he is redirected to the central user management server and after login, she/he is redirected back to the OASIS. These redirects are executed by your user's browser and do not require direct communication.</p> <p>Second, your OASIS must be able to request (via HTTP) the central user management and central NOMAD installation. This is necessary for non JWT-based authentication methods and to retrieve existing users for data-sharing features.</p> <p>The central user management will make future synchronizing data between NOMAD installations easier and generally recommend to use the central system. But in principle, you can also run your own user management. See the section on your own user management.</p>"},{"location":"howto/oasis/configure.html#configuration-files","title":"Configuration files","text":"<p>The <code>nomad-distro-template</code> provides all the neccessary configuration files. We strongly recommend to create your own distribution project based on the template. This will allow you to version your configuration, build custom images with plugins, and much more.</p> <p>In this section, you can learn about settings that you might need to change. The config files are:</p> <ul> <li><code>docker-compose.yaml</code></li> <li><code>configs/nomad.yaml</code></li> <li><code>configs/nginx.conf</code></li> </ul> <p>All docker containers are configured via docker-compose and the respective <code>docker-compose.yaml</code> file. The other files are mounted into the docker containers.</p>"},{"location":"howto/oasis/configure.html#docker-composeyaml","title":"docker-compose.yaml","text":"<p>The most basic <code>docker-compose.yaml</code> to run an OASIS looks like this:</p> <pre><code>services:\n  # broker for celery\n  rabbitmq:\n    restart: unless-stopped\n    image: rabbitmq:4\n    container_name: nomad_oasis_rabbitmq\n    environment:\n      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG\n      - RABBITMQ_DEFAULT_USER=rabbitmq\n      - RABBITMQ_DEFAULT_PASS=rabbitmq\n      - RABBITMQ_DEFAULT_VHOST=/\n    volumes:\n      - rabbitmq:/var/lib/rabbitmq\n    healthcheck:\n      test: [ \"CMD\", \"rabbitmq-diagnostics\", \"--silent\", \"--quiet\", \"ping\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # the search engine\n  elastic:\n    restart: unless-stopped\n    image: elasticsearch:7.17.24\n    container_name: nomad_oasis_elastic\n    environment:\n      - ES_JAVA_OPTS=-Xms512m -Xmx512m\n      - discovery.type=single-node\n    volumes:\n      - elastic:/usr/share/elasticsearch/data\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"--fail\", \"--silent\", \"http://elastic:9200/_cat/health\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 60s\n\n  # the user data db\n  mongo:\n    restart: unless-stopped\n    image: mongo:5\n    container_name: nomad_oasis_mongo\n    environment:\n      - MONGO_DATA_DIR=/data/db\n      - MONGO_LOG_DIR=/dev/null\n    volumes:\n      - mongo:/data/db\n      - ./.volumes/mongo:/backup\n    command: mongod --logpath=/dev/null # --quiet\n    healthcheck:\n      test: [ \"CMD\", \"mongo\", \"mongo:27017/test\", \"--quiet\", \"--eval\", \"'db.runCommand({ping:1}).ok'\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad worker (processing)\n  worker:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_worker\n    environment:\n      NOMAD_SERVICE: nomad_oasis_worker\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run-worker.sh\n\n  # nomad app (api + proxy)\n  app:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_app\n    environment:\n      NOMAD_SERVICE: nomad_oasis_app\n      NOMAD_SERVICES_API_PORT: 80\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n      NOMAD_NORTH_HUB_HOST: north\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n      north:\n        condition: service_started\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run.sh\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"--fail\", \"--silent\", \"http://localhost:8000/-/health\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad remote tools hub (JupyterHUB, e.g. for AI Toolkit)\n  north:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_north\n    environment:\n      NOMAD_SERVICE: nomad_oasis_north\n      NOMAD_NORTH_DOCKER_NETWORK: nomad_oasis_network\n      NOMAD_NORTH_HUB_CONNECT_IP: north\n      NOMAD_NORTH_HUB_IP: \"0.0.0.0\"\n      NOMAD_NORTH_HUB_HOST: north\n      NOMAD_SERVICES_API_HOST: app\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n      - /var/run/docker.sock:/var/run/docker.sock\n    user: '1000:991'\n    command: python -m nomad.cli admin run hub\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"--fail\", \"--silent\", \"http://localhost:8081/nomad-oasis/north/hub/health\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad proxy (a reverse proxy for nomad)\n  proxy:\n    restart: unless-stopped\n    image: nginx:stable-alpine\n    container_name: nomad_oasis_proxy\n    command: nginx -g 'daemon off;'\n    volumes:\n      - ./configs/nginx.conf:/etc/nginx/conf.d/default.conf\n    depends_on:\n      app:\n        condition: service_healthy\n      worker:\n        condition: service_started # TODO: service_healthy\n      north:\n        condition: service_healthy\n    ports:\n      - \"80:80\"\n\nvolumes:\n  mongo:\n    name: \"nomad_oasis_mongo\"\n  elastic:\n    name: \"nomad_oasis_elastic\"\n  rabbitmq:\n    name: \"nomad_oasis_rabbitmq\"\n  keycloak:\n    name: \"nomad_oasis_keycloak\"\n\nnetworks:\n  default:\n    name: nomad_oasis_network\n</code></pre> <p>Changes necessary:</p> <ul> <li>The group in the value of the hub's user parameter needs to match the docker group   on the host. This should ensure that the user which runs the hub, has the rights to access the host's docker.</li> <li>On Windows or MacOS computers you have to run the <code>app</code> and <code>worker</code> container without <code>user: '1000:1000'</code> and the <code>north</code> container with <code>user: root</code>.</li> </ul> <p>A few things to notice:</p> <ul> <li>The app, worker, and north service use the NOMAD docker image. Here we use the <code>latest</code> tag, which   gives you the latest beta version of NOMAD. You might want to change this to <code>stable</code>,   a version tag (format is <code>vX.X.X</code>, you find all releases here), or a specific branch tag.</li> <li>All services use docker volumes for storage. This could be changed to host mounts.</li> <li>It mounts two configuration files that need to be provided (see below): <code>nomad.yaml</code>, <code>nginx.conf</code>.</li> <li>The only exposed port is <code>80</code> (proxy service). This could be changed to a desired port if necessary.</li> <li>The NOMAD images are pulled from our gitlab at MPCDF, the other services use images from a public registry (dockerhub).</li> <li>All containers will be named <code>nomad_oasis_*</code>. These names can be used later to reference the container with the <code>docker</code> cmd.</li> <li>The services are setup to restart <code>always</code>, you might want to change this to <code>no</code> while debugging errors to prevent indefinite restarts.</li> <li>Make sure that the <code>PWD</code> environment variable is set. NORTH needs to create bind mounts that require absolute paths and we need to pass the current working directory to the configuration from the PWD variable (see hub service in the <code>docker-compose.yaml</code>).</li> <li>The <code>north</code> service needs to run docker containers. We have to use the systems docker group as a group. You might need to replace <code>991</code> with your   systems docker group id.</li> </ul>"},{"location":"howto/oasis/configure.html#nomadyaml","title":"nomad.yaml","text":"<p>NOMAD app and worker read a <code>nomad.yaml</code> for configuration.</p> <pre><code>services:\n  api_host: 'localhost'\n  api_base_path: '/nomad-oasis'\n\noasis:\n  is_oasis: true\n  uses_central_user_management: true\n\nnorth:\n  jupyterhub_crypt_key: '978bfb2e13a8448a253c629d8dd84ff89587f30e635b753153960930cad9d36d'\n\nmeta:\n  deployment: 'oasis'\n  deployment_url: 'https://my-oasis.org/api'\n  maintainer_email: 'me@my-oasis.org'\n\nlogtransfer:\n  enabled: false\n\nmongo:\n    db_name: nomad_oasis_v1\n\nelastic:\n    entries_index: nomad_oasis_entries_v1\n    materials_index: nomad_oasis_materials_v1\n</code></pre> <p>You should change the following:</p> <ul> <li>Replace <code>localhost</code> with the hostname of your server. I user-management will redirect your   users back to this host. Make sure this is the hostname, your users can use.</li> <li>Replace <code>deployment</code>, <code>deployment_url</code>, and <code>maintainer_email</code> with representative values.   The <code>deployment_url</code> should be the url to the deployment's api (should end with <code>/api</code>).</li> <li>To enable the log transfer set <code>logtransfer.enable: true</code> (data privacy notice above).</li> <li>You can change <code>api_base_path</code> to run NOMAD under a different path prefix.</li> <li>You should generate your own <code>north.jupyterhub_crypt_key</code>. You can generate one   with <code>openssl rand -hex 32</code>.</li> <li>On Windows or MacOS, you have to add <code>hub_connect_ip: 'host.docker.internal'</code> to the <code>north</code> section.</li> </ul> <p>A few things to notice:</p> <ul> <li>Under <code>mongo</code> and <code>elastic</code> you can configure database and index names. This might   be useful, if you need to run multiple NOMADs with the same databases.</li> <li>All managed files are stored under <code>.volumes</code> of the current directory.</li> </ul>"},{"location":"howto/oasis/configure.html#nginxconf","title":"nginx.conf","text":"<p>The GUI container serves as a proxy that forwards requests to the app container. The proxy is an nginx server and needs a configuration similar to this:</p> <pre><code>map $http_upgrade $connection_upgrade {\n    default upgrade;\n    ''      close;\n}\n\nserver {\n    listen        80;\n    server_name   localhost;\n    proxy_set_header Host $host;\n\n    gzip_min_length     1000;\n    gzip_buffers        4 8k;\n    gzip_http_version   1.0;\n    gzip_disable        \"msie6\";\n    gzip_vary           on;\n    gzip on;\n    gzip_proxied any;\n    gzip_types\n        text/css\n        text/javascript\n        text/xml\n        text/plain\n        application/javascript\n        application/x-javascript\n        application/json;\n\n    location / {\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /nomad-oasis\\/?(gui)?$ {\n        rewrite ^ /nomad-oasis/gui/ permanent;\n    }\n\n    location /nomad-oasis/gui/ {\n        proxy_intercept_errors on;\n        error_page 404 = @redirect_to_index;\n        proxy_pass http://app:8000;\n    }\n\n    location @redirect_to_index {\n        rewrite ^ /nomad-oasis/gui/index.html break;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ \\/gui\\/(service-worker\\.js|meta\\.json)$ {\n        add_header Last-Modified $date_gmt;\n        add_header Cache-Control 'no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0';\n        if_modified_since off;\n        expires off;\n        etag off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/uploads(/?$|.*/raw|.*/bundle?$)  {\n        client_max_body_size 35g;\n        proxy_request_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/.*/download {\n        proxy_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location /nomad-oasis/north/ {\n        client_max_body_size 500m;\n        proxy_pass http://north:9000;\n\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        # websocket headers\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n        proxy_set_header X-Scheme $scheme;\n\n        proxy_buffering off;\n    }\n}\n</code></pre> <p>A few things to notice:</p> <ul> <li>It configures the base path (<code>nomad-oasis</code>). It needs to be changed, if you use a different base path.</li> <li>You can use the server for additional content if you like.</li> <li><code>client_max_body_size</code> sets a limit to the possible upload size.</li> </ul> <p>You can add an additional reverse proxy in front or modify the nginx in the docker-compose.yaml to support https. If you operate the GUI container behind another proxy, keep in mind that your proxy should not buffer requests/responses to allow streaming of large requests/responses for <code>api/v1/uploads</code> and <code>api/v1/.*/download</code>. An nginx reverse proxy location on an additional reverse proxy, could have these directives to ensure the correct http headers and allows the download and upload of large files:</p> <pre><code>client_max_body_size 35g;\nproxy_set_header Host $host;\nproxy_pass_request_headers on;\nproxy_buffering off;\nproxy_request_buffering off;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_pass http://&lt;your-oasis-host&gt;/nomad-oasis;\n</code></pre>"},{"location":"howto/oasis/configure.html#plugins","title":"Plugins","text":"<p>Plugins allow the customization of a NOMAD deployment in terms of which search apps, normalizers, parsers and schema packages are available. In order for these customization to be activated, they have to be configured and installed into an Oasis. The basic template comes with a core set of plugins. If you want to configure your own set of plugins, using the template and creating your own distro project is mandatory.</p> <p>Plugins are configured via the <code>pyproject.toml</code> file. Based on this file the distro project CI pipeline creates the NOMAD docker image that is used by your installation. Only plugins configured in the <code>pyproject.toml</code> files, will be installed into the docker image and only those plugins installed in the used docker image are available in your Oasis.</p> <p>Please refer to the template README to learn how to add your own plugins.</p>"},{"location":"howto/oasis/configure.html#starting-and-stopping-nomad-services","title":"Starting and stopping NOMAD services","text":"<p>If you prepared the above files, simply use the usual <code>docker compose</code> commands to start everything.</p> <p>To make sure you have the latest docker images for everything, run this first:</p> <pre><code>docker compose pull\n</code></pre> <p>In the beginning and to simplify debugging, it is recommended to start the services separately:</p> <pre><code>docker compose up -d mongo elastic rabbitmq\ndocker compose up app worker gui\n</code></pre> <p>The <code>-d</code> option runs container in the background as daemons. Later you can run all at once:</p> <pre><code>docker compose up -d\n</code></pre> <p>Running all services also contains NORTH. When you use a tool in NORTH for the first time, your docker needs to pull the image that contains this tool. Be aware that this might take longer than timeouts allow and starting a tool for the very first time might fail.</p> <p>You can also use docker to stop and remove faulty containers that run as daemons:</p> <pre><code>docker stop nomad_oasis_app\ndocker rm nomad_oasis_app\n</code></pre> <p>You can wait for the start-up with curl using the apps <code>alive</code> \"endpoint\":</p> <pre><code>curl http://&lt;your host&gt;/nomad-oasis/alive\n</code></pre> <p>If everything works, the gui should be available under:</p> <pre><code>http://&lt;your host&gt;/nomad-oasis/gui/\n</code></pre> <p>If you run into problems, use the dev-tools of your browser to check the javascript logs or monitor the network traffic for HTTP 500/400/404/401 responses.</p> <p>To see if at least the api works, check</p> <pre><code>http://&lt;your host&gt;/nomad-oasis/alive\nhttp://&lt;your host&gt;/nomad-oasis/api/info\n</code></pre> <p>To see logs or 'go into' a running container, you can access the individual containers with their names and the usual docker commands:</p> <pre><code>docker logs nomad_oasis_app\n</code></pre> <pre><code>docker exec -ti nomad_oasis_app /bin/bash\n</code></pre> <p>If you want to report problems with your OASIS. Please provide the logs for</p> <ul> <li>nomad_oasis_app</li> <li>nomad_oasis_worker</li> <li>nomad_oasis_gui</li> </ul>"},{"location":"howto/oasis/configure.html#provide-and-connect-your-own-user-management","title":"Provide and connect your own user management","text":"<p>NOMAD uses keycloak for its user management. NOMAD uses keycloak in two ways. First, the user authentication uses the OpenID Connect/OAuth interfaces provided by keycloak. Second, NOMAD uses the keycloak realm-management API to get a list of existing users. Keycloak is highly customizable and numerous options to connect keycloak to existing identity providers exist.</p> <p>This tutorial assumes that you have some understanding of what keycloak is and how it works.</p> <p>The NOMAD Oasis installation with your own keyloak is very similar to the regular docker-compose installation above. There are just a three changes.</p> <ul> <li>The <code>docker-compose.yaml</code> has an added keycloak service.</li> <li>The <code>nginx.conf</code> is also modified to add another location for keycloak.</li> <li>The <code>nomad.yaml</code> has modifications to tell nomad to use your and not the official NOMAD keycloak.</li> </ul> <p>You can start with the regular installation above and manually adopt the config or download the already updated configuration files: nomad-oasis-with-keycloak.zip. The download also contains an additional <code>configs/nomad-realm.json</code> that allows you to create an initial keycloak realm that is configured for NOMAD automatically.</p> <p>First, the <code>docker-compose.yaml</code>:</p> <pre><code>services:\n  # keycloak user management\n  keycloak:\n    restart: unless-stopped\n    image: quay.io/keycloak/keycloak:16.1.1\n    container_name: nomad_oasis_keycloak\n    environment:\n      - PROXY_ADDRESS_FORWARDING=true\n      - KEYCLOAK_USER=admin\n      - KEYCLOAK_PASSWORD=password\n      - KEYCLOAK_FRONTEND_URL=http://localhost/keycloak/auth\n      - KEYCLOAK_IMPORT=\"/tmp/nomad-realm.json\"\n    command:\n      - \"-Dkeycloak.import=/tmp/nomad-realm.json -Dkeycloak.migration.strategy=IGNORE_EXISTING\"\n    volumes:\n      - keycloak:/opt/jboss/keycloak/standalone/data\n      - ./configs/nomad-realm.json:/tmp/nomad-realm.json\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"--fail\", \"--silent\", \"http://127.0.0.1:9990/health/live\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 30s\n\n  # broker for celery\n  rabbitmq:\n    restart: unless-stopped\n    image: rabbitmq:4\n    container_name: nomad_oasis_rabbitmq\n    environment:\n      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG\n      - RABBITMQ_DEFAULT_USER=rabbitmq\n      - RABBITMQ_DEFAULT_PASS=rabbitmq\n      - RABBITMQ_DEFAULT_VHOST=/\n    volumes:\n      - rabbitmq:/var/lib/rabbitmq\n    healthcheck:\n      test: [ \"CMD\", \"rabbitmq-diagnostics\", \"--silent\", \"--quiet\", \"ping\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # the search engine\n  elastic:\n    restart: unless-stopped\n    image: elasticsearch:7.17.24\n    container_name: nomad_oasis_elastic\n    environment:\n      - ES_JAVA_OPTS=-Xms512m -Xmx512m\n      - discovery.type=single-node\n    volumes:\n      - elastic:/usr/share/elasticsearch/data\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"--fail\", \"--silent\", \"http://elastic:9200/_cat/health\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 60s\n\n  # the user data db\n  mongo:\n    restart: unless-stopped\n    image: mongo:5\n    container_name: nomad_oasis_mongo\n    environment:\n      - MONGO_DATA_DIR=/data/db\n      - MONGO_LOG_DIR=/dev/null\n    volumes:\n      - mongo:/data/db\n      - ./.volumes/mongo:/backup\n    command: mongod --logpath=/dev/null # --quiet\n    healthcheck:\n      test: [ \"CMD\", \"mongo\", \"mongo:27017/test\", \"--quiet\", \"--eval\", \"'db.runCommand({ping:1}).ok'\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad worker (processing)\n  worker:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_worker\n    environment:\n      NOMAD_SERVICE: nomad_oasis_worker\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run-worker.sh\n\n  # nomad app (api + proxy)\n  app:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_app\n    environment:\n      NOMAD_SERVICE: nomad_oasis_app\n      NOMAD_SERVICES_API_PORT: 80\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\n      elastic:\n        condition: service_healthy\n      mongo:\n        condition: service_healthy\n      keycloak:\n        condition: service_started\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n    command: ./run.sh\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"--fail\", \"--silent\", \"http://localhost:8000/-/health\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad remote tools hub (JupyterHUB, e.g. for AI Toolkit)\n  north:\n    restart: unless-stopped\n    image: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:latest\n    container_name: nomad_oasis_north\n    environment:\n      NOMAD_SERVICE: nomad_oasis_north\n      NOMAD_NORTH_DOCKER_NETWORK: nomad_oasis_network\n      NOMAD_NORTH_HUB_CONNECT_IP: north\n      NOMAD_NORTH_HUB_IP: \"0.0.0.0\"\n      NOMAD_NORTH_HUB_HOST: north\n      NOMAD_SERVICES_API_HOST: app\n      NOMAD_FS_EXTERNAL_WORKING_DIRECTORY: \"$PWD\"\n      NOMAD_RABBITMQ_HOST: rabbitmq\n      NOMAD_ELASTIC_HOST: elastic\n      NOMAD_MONGO_HOST: mongo\n    depends_on:\n      keycloak:\n        condition: service_started\n      app:\n        condition: service_started\n    volumes:\n      - ./configs/nomad.yaml:/app/nomad.yaml\n      - ./.volumes/fs:/app/.volumes/fs\n      - /var/run/docker.sock:/var/run/docker.sock\n    user: '1000:991'\n    command: python -m nomad.cli admin run hub\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"--fail\", \"--silent\", \"http://localhost:8081/nomad-oasis/north/hub/health\" ]\n      interval: 10s\n      timeout: 10s\n      retries: 30\n      start_period: 10s\n\n  # nomad proxy (a reverse proxy for nomad)\n  proxy:\n    restart: unless-stopped\n    image: nginx:stable-alpine\n    container_name: nomad_oasis_proxy\n    command: nginx -g 'daemon off;'\n    volumes:\n      - ./configs/nginx.conf:/etc/nginx/conf.d/default.conf\n    depends_on:\n      keycloak:\n        condition: service_healthy\n      app:\n        condition: service_healthy\n      worker:\n        condition: service_started # TODO: service_healthy\n      north:\n        condition: service_healthy\n    ports:\n      - \"80:80\"\n\nvolumes:\n  mongo:\n    name: \"nomad_oasis_mongo\"\n  elastic:\n    name: \"nomad_oasis_elastic\"\n  rabbitmq:\n    name: \"nomad_oasis_rabbitmq\"\n  keycloak:\n    name: \"nomad_oasis_keycloak\"\n\nnetworks:\n  default:\n    name: nomad_oasis_network\n</code></pre> <p>A few notes:</p> <ul> <li>You have to change the <code>KEYCLOAK_FRONTEND_URL</code> variable to match your host and set a path prefix.</li> <li>The environment variables on the keycloak service allow to use keycloak behind the nginx proxy with a path prefix, e.g. <code>keycloak</code>.</li> <li>By default, keycloak will use a simple H2 file database stored in the given volume. Keycloak offers many other options to connect SQL databases.</li> <li>We will use keycloak with our nginx proxy here, but you can also host-bind the port <code>8080</code> to access keycloak directly.</li> <li>We mount and use the downloaded <code>configs/nomad-realm.json</code> to configure a NOMAD compatible realm on the first startup of keycloak.</li> </ul> <p>Second, we add a keycloak location to the nginx config:</p> <pre><code>map $http_upgrade $connection_upgrade {\n    default upgrade;\n    ''      close;\n}\n\nserver {\n    listen        80;\n    server_name   localhost;\n    proxy_set_header Host $host;\n\n    location /keycloak {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        rewrite /keycloak/(.*) /$1 break;\n        proxy_pass http://keycloak:8080;\n    }\n\n    location / {\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /nomad-oasis\\/?(gui)?$ {\n        rewrite ^ /nomad-oasis/gui/ permanent;\n    }\n\n    location /nomad-oasis/gui/ {\n        proxy_intercept_errors on;\n        error_page 404 = @redirect_to_index;\n        proxy_pass http://app:8000;\n    }\n\n    location @redirect_to_index {\n        rewrite ^ /nomad-oasis/gui/index.html break;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ \\/gui\\/(service-worker\\.js|meta\\.json)$ {\n        add_header Last-Modified $date_gmt;\n        add_header Cache-Control 'no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0';\n        if_modified_since off;\n        expires off;\n        etag off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/uploads(/?$|.*/raw|.*/bundle?$)  {\n        client_max_body_size 35g;\n        proxy_request_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location ~ /api/v1/.*/download {\n        proxy_buffering off;\n        proxy_pass http://app:8000;\n    }\n\n    location /nomad-oasis/north/ {\n        client_max_body_size 500m;\n        proxy_pass http://north:9000;\n\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        # websocket headers\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n        proxy_set_header X-Scheme $scheme;\n\n        proxy_buffering off;\n    }\n}\n</code></pre> <p>A few notes:</p> <ul> <li>Again, we are using <code>keycloak</code> as a path prefix. We configure the headers to allow   keycloak to pick up the rewritten url.</li> </ul> <p>Third, we modify the keycloak configuration in the <code>nomad.yaml</code>:</p> <pre><code>services:\n  api_host: 'localhost'\n  api_base_path: '/nomad-oasis'\n\noasis:\n  is_oasis: true\n  uses_central_user_management: false\n\nnorth:\n  jupyterhub_crypt_key: '978bfb2e13a8448a253c629d8dd84ff89587f30e635b753153960930cad9d36d'\n\nkeycloak:\n  server_url: 'http://keycloak:8080/auth/'\n  public_server_url: 'http://localhost/keycloak/auth/'\n  realm_name: nomad\n  username: 'admin'\n  password: 'password'\n\nmeta:\n  deployment: 'oasis'\n  deployment_url: 'https://my-oasis.org/api'\n  maintainer_email: 'me@my-oasis.org'\n\nlogtransfer:\n  enabled: false\n\nmongo:\n    db_name: nomad_oasis_v1\n\nelastic:\n    entries_index: nomad_oasis_entries_v1\n    materials_index: nomad_oasis_materials_v1\n</code></pre> <p>You should change the following:</p> <ul> <li>There are two urls to configure for keycloak. The <code>server_url</code> is used by the nomad   services to directly communicate with keycloak within the docker network. The <code>public_server_url</code>   is used by the UI to perform the authentication flow. You need to replace <code>localhost</code>   in <code>public_server_url</code> with <code>&lt;yourhost&gt;</code>.</li> </ul> <p>A few notes:</p> <ul> <li>The particular <code>admin_user_id</code> is the Oasis admin user in the provided example realm   configuration. See below.</li> </ul> <p>If you open <code>http://&lt;yourhost&gt;/keycloak/auth</code> in a browser, you can access the admin console. The default user and password are <code>admin</code> and <code>password</code>.</p> <p>Keycloak uses <code>realms</code> to manage users and clients. A default NOMAD compatible realm is imported by default. The realm comes with a test user and password <code>test</code> and <code>password</code>.</p> <p>A few notes on the realm configuration:</p> <ul> <li>Realm and client settings are almost all default keycloak settings.</li> <li>You should change the password of the admin user in the nomad realm.</li> <li>The admin user in the nomad realm has the additional <code>view-users</code> client role for <code>realm-management</code>   assigned. This is important, because NOMAD will use this user to retrieve the list of possible   users for managing co-authors and reviewers on NOMAD uploads.</li> <li>The realm has one client <code>nomad_public</code>. This has a basic configuration. You might   want to adapt this to your own policies. In particular you can alter the valid redirect URIs to   your own host.</li> <li>We disabled the https requirement on the default realm for simplicity. You should change   this for a production system.</li> </ul>"},{"location":"howto/oasis/configure.html#further-steps","title":"Further steps","text":"<p>This is an incomplete list of potential things to customize your NOMAD experience.</p> <ul> <li>Learn how to develop plugins that can be installed in an Oasis</li> <li>Write .yaml based schemas and ELNs</li> <li>Learn how to use the tabular parser to manage data from .xls or .csv</li> <li>Add specialized NORTH tools</li> <li>Restricting user access</li> </ul>"},{"location":"howto/oasis/configure.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"howto/oasis/configure.html#time-offset-between-oasis-and-the-authentication-server","title":"Time offset between Oasis and the Authentication server","text":"<p>If during login you get an error like: <code>jwt.exceptions.ImmatureSignatureError: The token is not yet valid (iat)</code>, it most probably means that there is a time difference between the two machines: the one creating the JWT and the other that is validating it. This causes an error where the authentication server looking at the token thinks that it has not been issued yet.</p> <p>To fix this problem, you should ensure that the time on the servers is synchronized. It is possible that a network port on one of the servers may be closed, preventing it from synchronizing the time. Note that the servers do not need to be on the same timezone, as internally everything is converted to UTC+0. To check the time on a server, you can on a linux-based machine use the <code>timedatectl</code> command which will report both the harware clock and the system clock (see here for the difference). For authentication, the system clocks on the two machines need to be set correctly, but you might also need to correct the hardware clock since it initially sets the system clock upon rebooting the machine.</p>"},{"location":"howto/oasis/configure.html#nomad-in-networks-with-restricted-internet-access","title":"NOMAD in networks with restricted Internet access","text":"<p>Some network environments do not allow direct Internet connections, and require the use of an outbound proxy. However, NOMAD needs to connect to the central user management or elasticsearch thus requires an active Internet connection (at least on Windows) to work. In these cases you need to configure docker to use your proxy. See details via this link https://docs.docker.com/network/proxy/. An example file <code>~/.docker/config.json</code> could look like this.</p> <pre><code>{\n  \"proxies\": {\n    \"default\": {\n      \"httpProxy\": \"http://&lt;proxy&gt;:&lt;port&gt;\",\n      \"httpsProxy\": \"http://&lt;proxy&gt;:&lt;port&gt;\",\n      \"noProxy\": \"127.0.0.0/8,elastic,localhost\"\n    }\n  }\n}\n</code></pre> <p>Since not all used services respect proxy variables, one also has to change the docker compose config file <code>docker-compose.yaml</code> for elastic search to:</p> <pre><code>elastic:\n  restart: unless-stopped\n  image: elasticsearch:7.17.24\n  container_name: nomad_oasis_elastic\n  environment:\n    - ES_JAVA_OPTS=-Xms512m -Xmx512m\n    - ES_JAVA_OPTS=-Djava.net.useSystemProxies=true\n    - ES_JAVA_OPTS=-Dhttps.proxyHost=&lt;proxy&gt; -Dhttps.proxyPort=port -Dhttps.nonProxyHosts=localhost|127.0.0.1|elastic\n    - discovery.type=single-node\n  volumes:\n    - elastic:/usr/share/elasticsearch/data\n  healthcheck:\n    test:\n      - \"CMD\"\n      - \"curl\"\n      - \"--fail\"\n      - \"--silent\"\n      - \"http://elastic:9200/_cat/health\"\n    interval: 10s\n    timeout: 10s\n    retries: 30\n    start_period: 60s\n</code></pre> <p>Unfortunately there is no way yet to use the NORTH tools with the central user management, since the jupyterhub spawner does not respect proxy variables. It has not been tested yet when using an authentication which does not require the proxy, e.g. a local keycloak server.</p> <p>If you have issues please contact us on discord n the oasis channel.</p>"},{"location":"howto/oasis/configure.html#nomad-behind-a-firewall","title":"NOMAD behind a firewall","text":"<p>It is also possible that your docker container is not able to talk to each other. This could be due to restrictive settings on your server. The firewall shall allow both inbound and outbound HTTP and HTTPS traffic. The corresponding rules need to be added. Furthermore, inbound traffic needs to be enabled for the port used on the <code>nginx</code> service.</p> <p>In this case you should make sure this test runs through: https://docs.docker.com/network/network-tutorial-standalone/</p> <p>If not please contact your server provider for help.</p>"},{"location":"howto/oasis/configure.html#elasticsearch-and-open-files-limit","title":"Elasticsearch and open files limit","text":"<p>Even when run in docker elasticsearch might require you to change your systems resource limits as described in the elasticsearch documentation here.</p> <p>You can temporarely change the open files limit like this:</p> <pre><code>sudo ulimit -n 65535\n</code></pre>"},{"location":"howto/oasis/deploy.html","title":"Deploying an Oasis","text":"<p>Once you have configured your NOMAD Oasis through a distribution project as described in the configuration how-to, it is time to deploy it. An instance of a NOMAD distribution that is running on a particular machine is called a deployment. This document contains information about the basic requirements and available alternatives for deploying your NOMAD Oasis.</p>"},{"location":"howto/oasis/deploy.html#hardware-considerations","title":"Hardware considerations","text":"<p>The hardware requirements depend on the volume of data you need to manage and process, the number of concurrent users you have, and how many concurrent remote tools you are running. The following subsections go more into detail about the hardware choices but the minimum recommended hardware is:</p> <ul> <li>4 CPU cores</li> <li>8 GB RAM</li> <li>30 GB disk space</li> </ul>"},{"location":"howto/oasis/deploy.html#cpu","title":"CPU","text":"<p>The amount of compute resource (e.g. processor cores) is a matter of convenience (and amount of expected users). Four CPU cores are typically sufficient to support a research group and run application, processing, and databases in parallel. Smaller systems still work, e.g. for testing.</p>"},{"location":"howto/oasis/deploy.html#ram","title":"RAM","text":"<p>There should be enough RAM to run databases, application, and processing at the same time. The minimum requirements here can be quite low, but for processing the metadata for individual files is kept in memory. For large DFT geometry-optimizations this can add up quickly, especially if many CPU cores are available for processing entries in parallel. We recommend at least 2GB per core and a minimum of 8GB. You also need to consider RAM and CPU for running tools like Jupyter, if you opt to use NOMAD NORTH.</p>"},{"location":"howto/oasis/deploy.html#storage","title":"Storage","text":"<p>NOMAD keeps all files that it manages as they are. The files that NOMAD processes in addition (e.g. through parsing) are typically smaller than the original raw files. Therefore, you can base your storage requirements based on the size of the data files that you expect to manage. The additional MongoDB database and Elasticsearch index is comparatively small. A minimum storage size of 30GB is enough to host the required docker images and also to run the databases without hitting any disk-usage watermark errors.</p> <p>Storage speed is another consideration. NOMAD can work with NAS systems. All that NOMAD needs is a POSIX-compliant filesystem as an interface. So everything you can (e.g. Docker host) mount should be compatible. For processing data obviously relies on read/write speed, but this is just a matter of convenience. The processing is designed to run as managed asynchronous tasks. Local storage might be favorable for MongoDB and Elasticsearch operation, but it is not a must.</p>"},{"location":"howto/oasis/deploy.html#deployment-alternatives","title":"Deployment alternatives","text":"<p>NOMAD is designed so that it can be run either on a single machine using <code>docker-compose</code> or then can be scaled to using several virtual machines using <code>kubernetes</code>. The single machine setup with <code>docker-compose</code> is more typical for an Oasis as it is easier to get running and in many cases a single machine can deal with the computational load. The setup with <code>kubernetes</code> requires a bit more work but becomes important once you need to scale the service to deal with more processing, more simultaneous remote tools and so on.</p>"},{"location":"howto/oasis/deploy.html#docker-compose","title":"<code>docker-compose</code>","text":"<p>For the single-machine setup with <code>docker-compose</code>, the <code>nomad-distro-template</code> provides a basic <code>docker-compose.yaml</code> file and a set of instructions in <code>README.md</code> for booting up all of the service.</p>"},{"location":"howto/oasis/deploy.html#kubernetes","title":"kubernetes","text":"<p>Attention</p> <p>This is just preliminary documentation and many details are missing.</p> <p>There is a NOMAD Helm chart. First we need to add the NOMAD Helm chart repository:</p> <pre><code>helm repo add nomad https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/helm/latest\n</code></pre> <p>New we need a minimal <code>values.yaml</code> that configures the individual kubernetes resources created by our Helm chart:</p> <pre><code>\n</code></pre> <p>The <code>jupyterhub</code>, <code>mongodb</code>, <code>elasticsearch</code>, <code>rabbitmq</code> follow the respective official Helm charts configuration.</p> <p>Run the Helm chart and install NOMAD:</p> <pre><code>helm update --install nomad nomad/nomad -f values.yaml\n</code></pre>"},{"location":"howto/oasis/deploy.html#base-linux-without-docker","title":"Base Linux (without Docker)","text":"<p>Not recommended. We do not provide official support for this type of installation, but it is possible to run NOMAD without Docker. You can infer the necessary steps from the provided <code>docker-compose.yaml</code>.</p>"},{"location":"howto/oasis/deploy.html#deploying-nomad-on-a-cloud-provider","title":"Deploying NOMAD on a cloud provider","text":"<p>Note</p> <p>Disclaimer: This guide is an independent tutorial for deploying NOMAD on various cloud providers. We are not affiliated with, endorsed by, or funded by any cloud providers mentioned in this document.</p> <p>Regardless of the cloud provider, the deployment typically follows these steps:</p> <ol> <li> <p>Choose the cloud provider and and set up an account</p> </li> <li> <p>Provision compute instances</p> </li> <li> <p>Configure network and security</p> </li> <li> <p>Deploy NOMAD</p> </li> <li> <p>Access and test deployment</p> </li> </ol>"},{"location":"howto/oasis/deploy.html#single-node-deployment-with-docker-compose","title":"Single node deployment with <code>docker-compose</code>","text":""},{"location":"howto/oasis/deploy.html#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ol> <li> <p>Create an AWS account</p> <p>You can do it here. You will need a credit card for creating an account.</p> </li> <li> <p>Create an EC2 instance</p> <p>EC2 (Elastic Compute Cloud) is Amazon's platform for creating and running virtual machines. To create a new EC2 instance, you need to login to the AWS console and start the process of creating a new EC2 instance. In the EC2 instance settings, pay attention to the following settings:</p> <ul> <li>Choose a Linux-based operating system (OS). (e.g. Ubuntu, Amazon Linux, Red Hat, SUSE Linux). This tutorial is based on using Ubuntu.</li> <li>Select an instance type based on your workload (see appropriate hardware resources). If you are unsure, you could start with a <code>c5.xlarge</code> instance.</li> <li>In network settings, ensure that \"Auto-assign public IP\" is enabled</li> <li>In network settings, ensure that \"Allow HTTPS traffic from the internet\" is enabled.</li> <li>In network settings, ensure that \"Allow HTTP traffic from the internet\" is enabled.</li> <li>In the storage settings, add persistent storage for databases and files stored by NOMAD. The default EBS (Elastic Block Store) is a recommended option, as it provides durable and scalable storage. Learn more in the AWS Storage Guide. We recommend starting with at least 30 GiB of storage to have space for the docker images and databases.</li> <li>Launch the instance</li> </ul> </li> <li> <p>Configure Network &amp; Security</p> <ul> <li> <p>Check that inbound traffic is allowed in the Network &amp; Security/Security Groups settings. Inbound traffic should be allowed for:</p> <ul> <li>HTTP: Protocol TCP, port 80, source 0.0.0.0/0</li> <li>HTTPS: Protocol TCP, port 443, source 0.0.0.0/0</li> <li>(Optional) SSH: Protocol TCP, port 22, source 0.0.0.0/0</li> </ul> <p>These rules should have been added during the previous step if you selected to allow HTTP/HTTPS traffic from the internet.</p> </li> <li> <p>Check that the OS firewall (e.g. <code>ufw</code> for Ubuntu) is also allowing this traffic.</p> </li> <li> <p>Assign an Elastic (static) IP address, as by default AWS assigns a dynamic public IP that changes upon instance restart.</p> </li> <li> <p>To enable secure communication with the server, read the guide on setting up secured connection using HTTPS. For testing you can skip this step, but HTTPS communication must be enforced in the final production setup.</p> </li> </ul> </li> <li> <p>Deploy NOMAD</p> <ul> <li> <p>Connect to the EC2 instance. The easiest way is to use the browser based connection directly from the AWS console. You can alternatively also connect through SSH if you have generated a key pair and have SSH access enabled in the instance settings.</p> </li> <li> <p>Install docker and docker compose on the virtual machine: you can read more about the installation here.</p> </li> <li> <p>Ensure that Git is installed to be able to easily sync the distribution configuration. You can check this by running <code>git --version</code>. Generic installations instructions are found here.</p> </li> <li> <p>Create a NOMAD Oasis distribution using our template <code>nomad-distro-template</code>. We recommend creating a new repository by presssing the \"Use this template button\", but for testing it is also possible to use the existing template repository directly.</p> </li> <li> <p>Follow the deployment instructions in the <code>README.md</code> file under Deploying the distribution/For a new Oasis. This typically consists of cloning the repository, setting up file priviledges and then running <code>docker compose pull</code> + <code>docker compose up -d</code>.</p> </li> </ul> </li> <li> <p>Access and test deployment</p> <p>You should now be able to access the Oasis installation from anywhere using the static IP address or domain name you have configured: <code>http://&lt;IP-or-domain&gt;/nomad-oasis</code>. If you have not yet set up secure connections with HTTPS, read about it here.</p> </li> </ol>"},{"location":"howto/oasis/deploy.html#installing-docker","title":"Installing Docker","text":"<p>You can find generic installation instructions here. On Ubuntu, you can install docker and docker compose with:</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> <p>You will also want to configure docker to be run as a non-root user using these steps. On Ubuntu, this can be done with:</p> <pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\n</code></pre> <p>Note that you may need to reboot to get the docker daemon running and the user priviledges to work.</p>"},{"location":"howto/oasis/deploy.html#secured-connections-using-https","title":"Secured connections using HTTPS","text":"<p>Before entering production, you must set up secure connections through HTTPS. Without it any communication with the server is compromised and e.g. credentials and other sensitive data can be stolen. To set up secure connections, follow these steps (the steps focus on the single-node <code>docker-compose</code> setup):</p> <ol> <li>Ensure that you have a static IP address.</li> <li> <p>Get a TLS certificate</p> <p>HTTPS connections require a TLS certificate which also needs to be renewed periodically. Depending on your setup, you have several alternatives for setting up a certificate:</p> <ol> <li> <p>You already have a certificate.</p> <p>In this case you just need the certificate and key files.</p> </li> <li> <p>Self-signed certificate</p> <p>For testing, you can create a self-signed certificate. These are not viable for a production setup, as they are not trusted e.g. by browsers.</p> <p>For detailed instructions, see the \"Deploy Oasis with HTTPS\" section in the <code>nomad-distro-template</code> documentation</p> </li> <li> <p>Free certificate from Let's Encrypt</p> <p>Let's Encrypt is a non-profit organization that provides free TLS certificats. To create a free certificate you must have a domain name. You can follow their tutorials on creating free certificates.</p> </li> </ol> </li> <li> <p>Setup your server to accept HTTPS traffic.     To enable HTTPS, you need to mount your TLS certificate and ensure that port 443 is open. A template nginx configuration file is available, see the \"Deploy Oasis with HTTPS\" section of <code>nomad-distro-template</code> documentation.</p> </li> </ol>"},{"location":"howto/oasis/update.html","title":"How to Update Oasis versions","text":""},{"location":"howto/oasis/update.html#software-versions","title":"Software versions","text":"<p>We distribute NOMAD as docker images that are available in our public docker registry. The a NOMAD image names looks like this:</p> <pre><code>gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair:v1.2.0\n</code></pre> <p>The version tag (e.g. <code>v1.2.0</code>) follows semantic versioning (<code>major.minor.patch</code>). Images released under a version tag do not change. There are also variable tags like <code>stable</code>, <code>latest</code>, and <code>develop</code>. The image tag for recent feature branches use a encoded variant of the respective merge request name. It is generally the safes to use version tags.</p> <p>Our semantic interpretation of \"minor\" is the following:</p> <ul> <li>there are only additions to programming interfaces and config options</li> <li>NOMAD will still operate on existing data, but the structure of newly processed data might change</li> <li>minor version might introduce new features that are only available after certain   actions migration steps.</li> </ul> <p>A road-map for major features can be found on our homepage here. You'll find a detailed change log in the source code here.</p>"},{"location":"howto/oasis/update.html#configuration-versions","title":"Configuration versions","text":"<p>Depending on the versions you need to update your docker-compose or NOMAD configuration. This might be necessary for breaking changes, or advisable to activate new features. Therefore, it is important to understand that the installations files that run NOMAD (e.g. <code>docker-compose.yaml</code> or <code>nomad.yaml</code>) are independent of the NOMAD image and they won't automatically change just because you use a new image.</p> <p>We will list respective changes and guides under migration steps below.</p>"},{"location":"howto/oasis/update.html#data-versions","title":"Data versions","text":"<p>Different version of NOMAD might represent data differently. For example, the definition of the search index, database schema, file system layout, or internal file formats might change. For such changes, we might offer a migration period, where NOMAD supports two different data representations at the same time. However, eventually this requires some migration of the existing data. This might be necessary for breaking changes (or because an old representation is deprecated) or advisable because a new representation offers new features. Therefore, it is important to understand that the used NOMAD software and data are independent things. Using a new image, does not change the data in your installation.</p> <p>We will list respective changes and guides under migration steps below.</p>"},{"location":"howto/oasis/update.html#using-a-new-version","title":"Using a new version","text":"<p>To use a different version of NOMAD, it can be as simple as swapping the tag in the docker-compose services that use the nomad images (i.e. <code>app</code> and <code>worker</code>). Depending on the version change, further steps might be necessary:</p> <ul> <li>for patch releases no further actions should be necessary</li> <li>for minor releases some additional actions might be required to unlock new features</li> <li>for major releases breaking changes are likely and further actions will be required</li> </ul> <p>For changing the minor or major version, please check the migration steps below.</p>"},{"location":"howto/oasis/update.html#migration-steps","title":"Migration steps","text":""},{"location":"howto/oasis/update.html#to-122","title":"to 1.2.2","text":"<p>We upgraded the Jupyterhub version used for NORTH from 1.0.2 to 4.0.2. By default the Jupyterhub database is persisted in the <code>nomad_oasis_north</code> container. If you want to keep the database (e.g. to not loose any open tools), you will have to upgrade the database. Update the NOMAD docker image version and restart the Oasis like this:</p> <pre><code>docker compose down\ndocker compose pull\ndocker compose run north python -m jupyterhub upgrade-db\ndocker compose up -d\n</code></pre> <p>Alternatively, you can delete the <code>nomad_oasis_north</code> container and start with a fresh database. Make sure that all north tools are stopped and removed.</p> <pre><code>docker compose down\ndocker rm nomad_oasis_north\ndocker compose pull\ndocker compose up -d\n</code></pre>"},{"location":"howto/oasis/update.html#to-120","title":"to 1.2.0","text":"<ul> <li> <p>We introduced the plugin mechanism. There are now more options to control which schemas   and parsers are available in your installation. By default all the existing and shipped   schemas and parsers are enabled. See also here.</p> </li> <li> <p>We changed the archive file format. Re-processing might yield better performance.</p> </li> <li> <p>Parsers are now using a different workflow model and the UI now includes a   workflow card on the overview page of entries with workflows for the new model.   Re-processing all data will enable this feature for old data. Any analysis build on   the old workflow model, might not work for new data.</p> </li> <li> <p>We introduce the log-transfer service. This is currently an opt-in feature.</p> </li> </ul>"},{"location":"howto/oasis/update.html#from-08x-to-1x","title":"from 0.8.x to 1.x","text":"<p>Between versions 0.10.x and 1.x we needed to change how archive and metadata data is stored internally in files and databases. This means you cannot simply start a new version of NOMAD on top of the old data. But there is a strategy to adapt the data. This should work for data based on NOMAD &gt;0.8.0 and &lt;= 0.10.x.</p> <p>The overall strategy is to create a new mongo database, copy all information, and then reprocess all data for the new version.</p> <p>First, shutdown the OASIS and remove all old containers.</p> <pre><code>docker compose stop\ndocker compose rm -f\n</code></pre> <p>Update your config files (<code>docker-compose.yaml</code>, <code>nomad.yaml</code>, <code>nginx.conf</code>) according to the latest documentation (see above). Make sure to use index and database names that are different. The default values contain a version number in those names, if you don't overwrite those defaults, you should be safe.</p> <p>Make sure you get the latest images and start the OASIS with the new version of NOMAD:</p> <pre><code>docker compose pull\ndocker compose up -d\n</code></pre> <p>If you go to the GUI of your OASIS, it should now show the new version and appear empty, because we are using a different database and search index now.</p> <p>To migrate the data, we created a command that you can run within your OASIS' NOMAD application container. This command takes the old database name as an argument, it will copy all data from the old mongodb to the current one. The default v8.x database name was <code>nomad_fairdi</code>, but you might have changed this to <code>nomad_v0_8</code> as recommended by our old Oasis documentation.</p> <pre><code>docker exec -ti nomad_oasis_app bash -c 'nomad admin upgrade migrate-mongo --src-db-name nomad_v0_8'\ndocker exec -ti nomad_oasis_app bash -c 'nomad admin uploads reprocess'\n</code></pre> <p>Now all your data should appear in your OASIS again. If you like, you can remove the old index and database:</p> <pre><code>docker exec nomad_oasis_elastic bash -c 'curl -X DELETE http://elastic:9200/nomad_fairdi'\ndocker exec nomad_oasis_mongo bash -c 'mongo nomad_fairdi --eval \"printjson(db.dropDatabase())\"'\n</code></pre>"},{"location":"howto/plugins/apis.html","title":"How to write an API","text":"<p>APIs allow you to add more APIs to the NOMAD app. More specifically you can create a FastAPI apps that can be mounted into the main NOMAD app alongside other apis such as <code>/api/v1</code>, <code>/optimade</code>, etc.</p> <p>This documentation shows you how to write a plugin entry point for an API. You should read the introduction to plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/apis.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing an API. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 apis\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 myapi.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/apis.html#api-entry-point","title":"API entry point","text":"<p>The entry point defines basic information about your API and is used to automatically load it into a NOMAD distribution. It is an instance of a <code>APIEntryPoint</code> or its subclass and it contains a <code>load</code> method which returns a <code>fastapi.FastAPI</code> app instance. Furthermore, it allows you to define a path prefix for your API. The entry point should be defined in <code>*/apis/__init__.py</code> like this:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import APIEntryPoint\n\n\nclass MyAPIEntryPoint(APIEntryPoint):\n\n    def load(self):\n        from nomad_example.apis.myapi import app\n\n        return app\n\n\nmyapi = MyAPIEntryPoint(\n    prefix = 'myapi',\n    name = 'MyAPI',\n    description = 'My custom API.',\n)\n</code></pre> <p>Here you can see that a new subclass of <code>APIEntryPoint</code> was defined. In this new class you have to override the <code>load</code> method to determine the FastAPI app that makes your API. In the reference you can see all of the available configuration options for a <code>APIEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for it to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyapi = \"nomad_example.apis:myapi\"\n</code></pre>"},{"location":"howto/plugins/apis.html#the-fastapi-app","title":"The FastAPI app","text":"<p>The <code>load</code>-method of an API entry point has to return an instance of a <code>fastapi.FastAPI</code>. This app should be implemented in a separate file (e.g. <code>*/apis/myapi.py</code>) and could look like this:</p> <pre><code>from fastapi import FastAPI\nfrom nomad.config import config\n\nmyapi_entry_point = config.get_plugin_entry_point('nomad_example.apis:myapi')\n\napp = FastAPI(\n    root_path=f'{config.services.api_base}/{myapi_entry_points.prefix}'\n)\n\napp.get('/')\nasync def root():\n    return {\"message\": \"Hello World\"}\n</code></pre> <p>Read the official FastAPI documentation to learn how to build apps and APIs with FastAPI.</p> <p>If you run NOMAD with this plugin following our Oasis configuration documentation you can curl this API and should receive the message:</p> <pre><code>curl localhost/nomad-oasis/myapi\n</code></pre>"},{"location":"howto/plugins/apps.html","title":"How to write an app","text":"<p>Apps provide customized views of data in the GUI, making it easier for the users to navigate and understand the data related to a specific domain. This typically means that certain domain-specific properties are highlighted, different units may be used for physical properties, and specialized dashboards may be presented. This becomes crucial for NOMAD installations to be able to scale with data that contains a mixture of experiments and simulations, different techniques, and physical properties spanning different time and length scales.</p> <p>Apps only affect the way data is displayed for the user: if you wish to affect the underlying data structure, you will need to write a Python schema package or a YAML schema package.</p> <p>This documentation shows you how to write an plugin entry point for an app. You should read the introduction to plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/apps.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing an app. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 apps\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/apps.html#app-entry-point","title":"App entry point","text":"<p>The entry point defines basic information about your app and is used to automatically load the app into a NOMAD distribution. It is an instance of an <code>AppEntryPoint</code> and unlike many other plugin entry points, it does not have a separate resource that needs to be lazy-loaded as the entire app is defined in the configuration as an instance of <code>nomad.config.models.ui.App</code>. You will learn more about the <code>App</code> class in the next sections. The entry point should be defined in <code>*/apps/__init__.py</code> like this:</p> <pre><code>from nomad.config.models.plugins import AppEntryPoint\nfrom nomad.config.models.ui import App\n\nmyapp = AppEntryPoint(\n    name = 'MyApp',\n    description = 'My custom app.',\n    app = App(...)\n)\n</code></pre> <p>Here we have instantiated an object <code>myapp</code> in which you specify the default parameterization and other details about the app. In the reference you can see all of the available configuration options for an <code>AppEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the app to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyapp = \"nomad_example.apps:myapp\"\n</code></pre>"},{"location":"howto/plugins/apps.html#creating-an-app","title":"Creating an <code>App</code>","text":"<p>The definition fo the actual app is given as an instance of the <code>App</code> class specified as part of the entry point. A full breakdown of the model is given below in the app reference, but here is a small example:</p> <pre><code>from nomad.config.models.plugins import AppEntryPoint\nfrom nomad.config.models.ui import (\n    App,\n    Axis,\n    Column,\n    Dashboard,\n    Layout,\n    Menu,\n    MenuItemPeriodicTable,\n    MenuItemHistogram,\n    MenuItemTerms,\n    SearchQuantities,\n    WidgetHistogram,\n)\n\nschema = 'nomad_example.schema_packages.mypackage.MySchema'\nmyapp = AppEntryPoint(\n    name='MyApp',\n    description='App defined using the new plugin mechanism.',\n    app=App(\n        # Label of the App\n        label='My App',\n        # Path used in the URL, must be unique\n        path='myapp',\n        # Used to categorize apps in the explore menu\n        category='Theory',\n        # Brief description used in the app menu\n        description='An app customized for me.',\n        # Longer description that can also use markdown\n        readme='Here is a much longer description of this app.',\n        # If you want to use quantities from a custom schema, you need to load\n        # the search quantities from it first here. Note that you can use a glob\n        # syntax to load the entire package, or just a single schema from a\n        # package.\n        search_quantities=SearchQuantities(\n            include=['*#nomad_example.schema_packages.mypackage.MySchema'],\n        ),\n        # Controls which columns are shown in the results table\n        columns=[\n            Column(quantity='entry_id', selected=True),\n            Column(quantity=f'data.section.myquantity#{schema}', selected=True),\n            Column(\n                quantity=f'data.my_repeated_section[*].myquantity#{schema}',\n                selected=True,\n            ),\n            Column(quantity='upload_create_time'),\n        ],\n        # Dictionary of search filters that are always enabled for queries made\n        # within this app. This is especially important to narrow down the\n        # results to the wanted subset. Any available search filter can be\n        # targeted here. This example makes sure that only entries that use\n        # MySchema are included.\n        filters_locked={'section_defs.definition_qualified_name': [schema]},\n        # Controls the menu shown on the left\n        menu=Menu(\n            title='Material',\n            items=[\n                Menu(\n                    title='elements',\n                    items=[\n                        MenuItemPeriodicTable(\n                            quantity='results.material.elements',\n                        ),\n                        MenuItemTerms(\n                            quantity='results.material.chemical_formula_hill',\n                            width=6,\n                            options=0,\n                        ),\n                        MenuItemTerms(\n                            quantity='results.material.chemical_formula_iupac',\n                            width=6,\n                            options=0,\n                        ),\n                        MenuItemHistogram(\n                            x='results.material.n_elements',\n                        ),\n                    ],\n                )\n            ],\n        ),\n        # Controls the default dashboard shown in the search interface\n        dashboard=Dashboard(\n            widgets=[\n                WidgetHistogram(\n                    title='Histogram Title',\n                    show_input=False,\n                    autorange=True,\n                    nbins=30,\n                    scale='linear',\n                    x=Axis(search_quantity=f'data.mysection.myquantity#{schema}'),\n                    layout={'lg': Layout(minH=3, minW=3, h=4, w=12, y=0, x=0)},\n                )\n            ]\n        ),\n    ),\n)\n</code></pre> <p>Tip</p> <p>If you want to load an app definition from a YAML file, this can be easily done with the pydantic <code>parse_obj</code> function:</p> <pre><code>    import yaml\n    from nomad.config.models.plugins import AppEntryPoint\n    from nomad.config.models.ui import App\n\n    yaml_data = \"\"\"\n        label: My App\n        path: myapp\n        category: Theory\n    \"\"\"\n    myapp = AppEntryPoint(\n        name='MyApp',\n        description='App defined using the new plugin mechanism.',\n        app=App.parse_obj(\n            yaml.safe_load(yaml_data)\n        ),\n    )\n</code></pre>"},{"location":"howto/plugins/apps.html#loading-quantity-definitions-into-an-app","title":"Loading quantity definitions into an app","text":"<p>By default, quantities from custom schemas are not available in an app, and they need to be explicitly added. Each app may define the quantities to load by using the search_quantities field in the app config. Once loaded, these search quantities can be queried in the search interface, but also targeted in the rest of the app configuration as explained below.</p> <p>Important</p> <p>Note that not all of the quantities from a custom schema can be loaded into the search. At the moment, we only support loading scalar quantities from custom schemas.</p> <p>Each schema has a unique name within the NOMAD ecosystem, which is needed to target them in the configuration. The name depends on the resource in which the schema is defined in:</p> <ul> <li>Python schemas are identified by the python path for the class that inherits from <code>Schema</code>. For example, if you have a python package called <code>nomad_example</code>, which has a subpackage called <code>schema_packages</code>, containing a module called <code>mypackage.py</code>, which contains the class <code>MySchema</code>, then the schema name will be <code>nomad_example.schema_packages.mypackage.MySchema</code>.</li> <li>YAML schemas are identified by the entry id of the schema file together with the name of the section defined in the YAML schema. For example if you have uploaded a schema YAML file containing a section definition called <code>MySchema</code>, and it has been assigned an <code>entry_id</code>, the schema name will be <code>entry_id:&lt;entry_id&gt;.MySchema</code>.</li> </ul> <p>The quantities from schemas may be included or excluded by using the <code>SearchQuantities</code> field in the app config. This option supports a wildcard/glob syntax for including/excluding certain search quantities. For example, to include all search quantities from the Python schema defined in the class <code>nomad_example.schema_packages.mypackage.MySchema</code>, you could use:</p> <pre><code>search_quantities=SearchQuantities(\n    include=['*#nomad_example.schema_packages.mypackage.MySchema']\n)\n</code></pre> <p>The same thing for a YAML schema could be achieved with:</p> <pre><code>search_quantities=SearchQuantities(\n    include=['*#entry_id:&lt;entry_id&gt;.MySchema']\n)\n</code></pre>"},{"location":"howto/plugins/apps.html#using-loaded-search-quantity-definitions","title":"Using loaded search quantity definitions","text":"<p>Once search quantities are loaded, they can be targeted in the rest of the app. The app configuration often refers to specific search quantities to configure parts of the user interface.</p> <p>The syntax for targeting quantities depends on the resource:</p> <ul> <li>For python schemas, you need to provide the path and the python schema name separated by a hashtag (#), for example <code>data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema</code>.</li> <li>For YAML schemas, you need to provide the path and the YAML schema name separated by a hashtag (#), for example <code>data.mysection.myquantity#entry_id:&lt;entry_id&gt;.MySchema</code>.</li> <li>Quantities that are common for all NOMAD entries can be targeted by using only the path without the need for specifying a schema, e.g. <code>results.material.symmetry.space_group</code>.</li> </ul> <p>For example, one could configure the results table to show a new column using one of the search quantities with:</p> <pre><code>from nomad.config.models.ui import Column\n\ncolumns = [\n    Column(quantity='entry_id', selected=True),\n    Column(\n        quantity='data.mysection.myquantity#nomad_example.schema_packages.mypackage.MySchema',\n        label='My Quantity Name',\n        selected=True,\n    ),\n    Column(quantity='upload_create_time'),\n]\n</code></pre>"},{"location":"howto/plugins/apps.html#narrowing-down-search-results-in-the-app","title":"Narrowing down search results in the app","text":"<p>The search results that will show up in the app can be narrowed down by passing a dictionary to the <code>filters_locked</code> option. In the example app, only entries that use <code>MySchema</code> are included.</p> <pre><code>filters_locked={\n    \"section_defs.definition_qualified_name\": [schema]\n}\n</code></pre> <p>It is also possible to filter by quantities defined in the <code>results</code> section. For example, if you want to limit your app to entries that have the property <code>catalytic</code> filled in the <code>results</code> section:</p> <pre><code>filters_locked={\n    \"quantities\": [\"results.properties.catalytic\"]\n}\n</code></pre>"},{"location":"howto/plugins/apps.html#menu","title":"Menu","text":"<p>The <code>menu</code> field controls the structure of the menu shown on the left side of the search interface. Menus have a controllable width, and they contain items that are displayed on a 12-based grid. You can also nest menus within each other. For example, this defines a menu with two levels:</p> <pre><code>from nomad.config.models.ui import Axis, Menu, MenuItemTerms, MenuItemHistogram\n\n# This is a top level menu that is always visible. It shows two items: a terms\n# item and a submenu beneath it.\nmenu = Menu(\n    size='sm',\n    items=[\n        MenuItemTerms(search_quantity='authors.name', options=5),\n        # This is a submenu whose items become visible once selected. It\n        # contains three items: one full-width histogram and two terms items\n        # which are displayed side-by-side.\n        Menu(\n            title='Submenu',\n            size='md',\n            items=[\n                MenuItemHistogram(x=Axis(search_quantity='upload_create_time')),\n                # These items target data from a custom schema\n                MenuItemTerms(\n                    width=6,\n                    search_quantity='data.quantity1#nomad_example.schema_packages.mypackage.MySchema',\n                ),\n                MenuItemTerms(\n                    width=6,\n                    search_quantity='data.quantity2#nomad_example.schema_packages.mypackage.MySchema',\n                ),\n            ],\n        ),\n    ],\n)\n</code></pre> <p>The following items are supported in menus, and you can read more about them in the App reference:</p> <ul> <li><code>Menu</code>: Defines a nested submenu.</li> <li><code>MenuItemTerms</code>: Used to display a set of possible text options.</li> <li><code>MenuItemHistogram</code>: Histogram of numerical values.</li> <li><code>MenuItemPeriodictable</code>: Displays a periodic table.</li> <li><code>MenuItemOptimade</code>: Field for entering OPTIMADE queries.</li> <li><code>MenuItemVisibility</code>: Controls for the query visibility.</li> <li><code>MenuItemDefinitions</code>: Shows a tree of available definitions from which items can be selected for the query.</li> <li><code>MenuItemCustomQuantities</code>: Form for querying custom quantities coming from any schema.</li> <li><code>MenuItemNestedObject</code>: Used to group together menu items so that their query is performed using an Elasticsearch nested query. Note that you cannot yet use nested queries for search quantities originating from custom schemas.</li> </ul>"},{"location":"howto/plugins/apps.html#dashboard","title":"Dashboard","text":"<p>The Dashboard field controls the content of the main search interface window. Different widgets can be added which contain terms or numerical information and can be controlled in size and position. There are 4 different types of Widgets:  <code>WidgetTerms</code>,  <code>WidgetHistogram</code>,  <code>WidgetScatterplot</code> and  <code>WidgetPeriodicTable</code></p> <pre><code>dashboard = Dashboard(\n    widgets=[\n        WidgetPeriodicTable(\n            title='Elements of the material',\n            layout={\n                'lg': Layout(h=8, minH=8, minW=12, w=12, x=0, y=0),\n            },\n            search_quantity='results.material.elements',\n            scale='linear',\n        ),\n        WidgetTerms(\n            title='Widget Terms Title',\n            layout={\n                'lg': Layout(h=8, minH=3, minW=3, w=6, x=12, y=0),\n            },\n            search_quantity=f'data.mysection.myquantity#{schema}',\n            showinput=True,\n            scale='linear',\n        ),\n        WidgetHistogram(\n            title='Histogram Title',\n            show_input=False,\n            autorange=True,\n            nbins=30,\n            scale='linear',\n            x=Axis(search_quantity=f'data.mysection.myquantity#{schema}'),\n            layout={'lg': Layout(minH=3, minW=3, h=8, w=12, y=8, x=0)},\n        ),\n        WidgetScatterPlot(\n            title='Scatterplot title',\n            autorange=True,\n            layout={\n                'lg': Layout(h=8, minH=3, minW=8, w=12, x=12, y=8),\n            },\n            x=Axis(\n                search_quantity=f'data.mysection.mynumericalquantity#{schema}',\n                title='quantity x',\n            ),\n            y=Axis(search_quantity=f'data.mysection.myothernumericalquantity#{schema}'),\n            color=f'data.mysection.myquantity#{schema}',  # optional, if set has to be scalar value\n            size=1000,  # maximum number of entries loaded\n        ),\n    ]\n)\n</code></pre>"},{"location":"howto/plugins/apps.html#app-reference","title":"App reference","text":""},{"location":"howto/plugins/apps.html#app","title":"App","text":"<p>Defines the layout and functionality for an App.</p> name type label <code>str</code> Name of the App.default: <code>PydanticUndefined</code> path <code>str</code> Path used in the browser address bar.default: <code>PydanticUndefined</code> resource <code>str</code> Targeted resource.default: <code>entries</code>options: - <code>entries</code> - <code>materials</code> category <code>str</code> Category used to organize Apps in the explore menu.default: <code>PydanticUndefined</code> pagination <code>Pagination</code> Default result pagination.default: Complex object, default value not displayed. breadcrumb <code>str | None</code> Name displayed in the breadcrumb, by default the label will be used. description <code>str | None</code> Short description of the App. readme <code>str | None</code> Longer description of the App that can also use markdown. columns <code>list[Column] | None</code> List of columns for the results table. rows <code>Rows | None</code> Controls the display of entry rows in the results table.default: Complex object, default value not displayed. menu <code>Menu | None</code> Filter menu displayed on the left side of the screen. filter_menus <code>FilterMenus | None</code> deprecated filters <code>Filters | None</code> deprecated search_quantities <code>SearchQuantities | None</code> Controls the quantities that are available for search in this app.default: Complex object, default value not displayed. dashboard <code>Dashboard | None</code> Default dashboard layout. filters_locked <code>dict | None</code> Fixed query object that is applied for this search context. This filter will always be active for this context and will not be displayed to the user by default. search_syntaxes <code>SearchSyntaxes | None</code> Controls which types of search syntax are available."},{"location":"howto/plugins/apps.html#rows","title":"Rows","text":"<p>Controls the visualization of rows in the search results.</p> name type actions <code>RowActions</code> default: Complex object, default value not displayed. details <code>RowDetails</code> default: Complex object, default value not displayed. selection <code>RowSelection</code> default: Complex object, default value not displayed."},{"location":"howto/plugins/apps.html#rowactions","title":"RowActions","text":"<p>Controls the visualization of row actions that are shown at the end of each row.</p> name type enabled <code>bool</code> Whether to enable row actions.default: <code>True</code> include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include. options <code>dict[str, RowActionNorth | RowActionURL] | None</code> deprecated items <code>list[RowActionNorth | RowActionURL] | None</code> List of actions to show for each row."},{"location":"howto/plugins/apps.html#rowactionurl","title":"RowActionURL","text":"<p>Action that will open an external link read from the archive.</p> name type icon <code>str</code> The icon to show for the action. You can browse the available icons at: https://fonts.google.com/icons?icon.set=Material+Icons. Note that you have to give the icon name in snake_case. In addition, the following extra icons are available: - <code>github</code>: @material-ui/icons/GitHubdefault: <code>launch</code> path <code>str</code> JMESPath pointing to a path in the archive that contains the URL.default: <code>PydanticUndefined</code> description <code>str | None</code> Description of the action shown to the user."},{"location":"howto/plugins/apps.html#rowactionnorth","title":"RowActionNorth","text":"<p>Action that will open a NORTH tool given its name in the archive.</p> name type icon <code>str</code> The icon to show for the action. You can browse the available icons at: https://fonts.google.com/icons?icon.set=Material+Icons. Note that you have to give the icon name in snake_case. In addition, the following extra icons are available: - <code>github</code>: @material-ui/icons/GitHubdefault: <code>launch</code> filepath <code>str</code> JMESPath pointing to a path in the archive that contains the filepath.default: <code>PydanticUndefined</code> tool_name <code>str</code> Name of the NORTH tool to open.default: <code>PydanticUndefined</code> description <code>str | None</code> Description of the action shown to the user."},{"location":"howto/plugins/apps.html#rowselection","title":"RowSelection","text":"<p>Controls the selection of rows. If enabled, rows can be selected and additional actions performed on them.</p> name type enabled <code>bool</code> Whether to show the row selection.default: <code>True</code>"},{"location":"howto/plugins/apps.html#rowdetails","title":"RowDetails","text":"<p>Controls the visualization of row details that are shown upon pressing the row and contain basic details about the entry.</p> name type enabled <code>bool</code> Whether to show row details.default: <code>True</code>"},{"location":"howto/plugins/apps.html#searchsyntaxes","title":"SearchSyntaxes","text":"<p>Controls the availability of different search syntaxes. These syntaxes determine how raw user input in e.g. the search bar is parsed into queries supported by the API.</p> <p>Currently you can only exclude items. By default, the following options are included:</p> <ul> <li><code>existence</code>: Used to query for the existence of a specific metainfo field in the data.</li> <li><code>equality</code>: Used to query for a specific value with exact match.</li> <li><code>range_bounded</code>: Queries values that are between two numerical limits, inclusive or exclusive.</li> <li><code>range_half_bounded</code>: Queries values that are above/below a numerical limit, inclusive or exclusive.</li> <li><code>free_text</code>: For inexact, free-text queries. Requires that a set of keywords has been filled in the entry.</li> </ul> name type exclude <code>list[str] | None</code> List of excluded options."},{"location":"howto/plugins/apps.html#dashboard_1","title":"Dashboard","text":"<p>Dashboard configuration.</p> name type widgets <code>list[WidgetTerms | WidgetHistogram | WidgetScatterPlot | WidgetScatterPlotDeprecated | WidgetPeriodicTable | WidgetPeriodicTableDeprecated]</code> List of widgets contained in the dashboard.default: <code>PydanticUndefined</code>"},{"location":"howto/plugins/apps.html#widgetterms","title":"WidgetTerms","text":"<p>Terms widget configuration.</p> name type search_quantity <code>str</code> The targeted search quantity.default: <code>PydanticUndefined</code> scale <code>str</code> Statistics scaling.default: <code>ScaleEnum.LINEAR</code>options: - <code>linear</code> - <code>log</code> - <code>1/2</code> - <code>1/4</code> - <code>1/8</code> show_input <code>bool</code> Whether to show text input field.default: <code>True</code> layout <code>dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>.default: <code>PydanticUndefined</code> quantity <code>str | None</code> deprecated showinput <code>bool | None</code> deprecated query_mode <code>str | None</code> The query mode to use when multiple terms are selected. title <code>str | None</code> Custom widget title. If not specified, a widget-specific default title is used."},{"location":"howto/plugins/apps.html#layout","title":"Layout","text":"<p>Defines widget size and grid positioning for different breakpoints.</p> name type h <code>int</code> Height in grid unitsdefault: <code>PydanticUndefined</code> w <code>int</code> Width in grid units.default: <code>PydanticUndefined</code> x <code>int</code> Horizontal start location in the grid.default: <code>PydanticUndefined</code> y <code>int</code> Vertical start location in the grid.default: <code>PydanticUndefined</code> minH <code>int | None</code> Minimum height in grid units.default: <code>3</code> minW <code>int | None</code> Minimum width in grid units.default: <code>3</code>"},{"location":"howto/plugins/apps.html#widgetscatterplotdeprecated","title":"WidgetScatterPlotDeprecated","text":"<p>Deprecated copy of WidgetScatterPlot with a misspelled type.</p> name type layout <code>dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>.default: <code>PydanticUndefined</code> x <code>AxisLimitedScale | str</code> Configures the information source and display options for the x-axis.default: <code>PydanticUndefined</code> y <code>AxisLimitedScale | str</code> Configures the information source and display options for the y-axis.default: <code>PydanticUndefined</code> size <code>int</code> Maximum number of entries to fetch. Notice that the actual number may be more or less, depending on how many entries exist and how many of the requested values each entry contains.default: <code>1000</code> drag_mode <code>str</code> Action to perform on mouse drag.default: <code>DragModeEnum.ZOOM</code> autorange <code>bool</code> Whether to automatically set the range according to the data limits.default: <code>True</code> title <code>str | None</code> Custom widget title. If not specified, a widget-specific default title is used. markers <code>Markers | None</code> Configures the information source and display options for the markers. color <code>str | None</code> Quantity used for coloring points. Note that this field is deprecated and <code>markers</code> should be used instead."},{"location":"howto/plugins/apps.html#markers","title":"Markers","text":"<p>Configuration for plot markers.</p> name type color <code>Axis | None</code> Configures the information source and display options for the marker colors."},{"location":"howto/plugins/apps.html#axis","title":"Axis","text":"<p>Configuration for a plot axis with limited scaling options.</p> name type search_quantity <code>str</code> Path of the targeted search quantity. Note that you can most of the features JMESPath syntax here to further specify a selection of values. This becomes especially useful when dealing with repeated sections or statistical values.default: <code>PydanticUndefined</code> title <code>str | None</code> Custom title to show for the axis. unit <code>str | None</code> Custom unit used for displaying the values. quantity <code>str | None</code> deprecated scale <code>str | None</code> Defines the axis scaling. Defaults to linear scaling.default: <code>ScaleEnum.LINEAR</code>"},{"location":"howto/plugins/apps.html#axislimitedscale","title":"AxisLimitedScale","text":"<p>Configuration for a plot axis with limited scaling options.</p> name type search_quantity <code>str</code> Path of the targeted search quantity. Note that you can most of the features JMESPath syntax here to further specify a selection of values. This becomes especially useful when dealing with repeated sections or statistical values.default: <code>PydanticUndefined</code> title <code>str | None</code> Custom title to show for the axis. unit <code>str | None</code> Custom unit used for displaying the values. quantity <code>str | None</code> deprecated scale <code>str | None</code> Defines the axis scaling. Defaults to linear scaling.default: <code>ScaleEnumPlot.LINEAR</code>"},{"location":"howto/plugins/apps.html#widgetperiodictabledeprecated","title":"WidgetPeriodicTableDeprecated","text":"<p>Deprecated copy of WidgetPeriodicTable with a misspelled type.</p> name type search_quantity <code>str</code> The targeted search quantity.default: <code>PydanticUndefined</code> layout <code>dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>.default: <code>PydanticUndefined</code> quantity <code>str | None</code> deprecated scale <code>str | None</code> Statistics scaling.default: <code>ScaleEnum.LINEAR</code> title <code>str | None</code> Custom widget title. If not specified, a widget-specific default title is used."},{"location":"howto/plugins/apps.html#widgethistogram","title":"WidgetHistogram","text":"<p>Histogram widget configuration.</p> name type show_input <code>bool</code> Whether to show text input field.default: <code>True</code> x <code>Axis | str</code> Configures the information source and display options for the x-axis.default: <code>PydanticUndefined</code> y <code>AxisScale | str</code> Configures the information source and display options for the y-axis.default: <code>PydanticUndefined</code> autorange <code>bool</code> Whether to automatically set the range according to the data limits.default: <code>False</code> layout <code>dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>.default: <code>PydanticUndefined</code> quantity <code>str | None</code> deprecated scale <code>str | None</code> deprecated showinput <code>bool | None</code> deprecated n_bins <code>int | None</code> Maximum number of histogram bins. Notice that the actual number of bins may be smaller if there are fewer data items available. nbins <code>int | None</code> deprecated title <code>str | None</code> Custom widget title. If not specified, a widget-specific default title is used."},{"location":"howto/plugins/apps.html#axisscale","title":"AxisScale","text":"<p>Basic configuration for a plot axis.</p> name type scale <code>str | None</code> Defines the axis scaling. Defaults to linear scaling.default: <code>ScaleEnum.LINEAR</code>"},{"location":"howto/plugins/apps.html#widgetperiodictable","title":"WidgetPeriodicTable","text":"<p>Periodic table widget configuration.</p> name type search_quantity <code>str</code> The targeted search quantity.default: <code>PydanticUndefined</code> layout <code>dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>.default: <code>PydanticUndefined</code> quantity <code>str | None</code> deprecated scale <code>str | None</code> Statistics scaling.default: <code>ScaleEnum.LINEAR</code> title <code>str | None</code> Custom widget title. If not specified, a widget-specific default title is used."},{"location":"howto/plugins/apps.html#widgetscatterplot","title":"WidgetScatterPlot","text":"<p>Scatter plot widget configuration.</p> name type layout <code>dict[str, Layout]</code> Defines widget size and grid positioning for different breakpoints. The following breakpoints are supported: <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code> and <code>xxl</code>.default: <code>PydanticUndefined</code> x <code>AxisLimitedScale | str</code> Configures the information source and display options for the x-axis.default: <code>PydanticUndefined</code> y <code>AxisLimitedScale | str</code> Configures the information source and display options for the y-axis.default: <code>PydanticUndefined</code> size <code>int</code> Maximum number of entries to fetch. Notice that the actual number may be more or less, depending on how many entries exist and how many of the requested values each entry contains.default: <code>1000</code> drag_mode <code>str</code> Action to perform on mouse drag.default: <code>DragModeEnum.ZOOM</code> autorange <code>bool</code> Whether to automatically set the range according to the data limits.default: <code>True</code> title <code>str | None</code> Custom widget title. If not specified, a widget-specific default title is used. markers <code>Markers | None</code> Configures the information source and display options for the markers. color <code>str | None</code> Quantity used for coloring points. Note that this field is deprecated and <code>markers</code> should be used instead."},{"location":"howto/plugins/apps.html#filters","title":"Filters","text":"<p>Alias for SearchQuantities.</p> name type include <code>list[str] | None</code> List of included options. Supports glob/wildcard syntax. exclude <code>list[str] | None</code> List of excluded options. Supports glob/wildcard syntax. Has higher precedence than include."},{"location":"howto/plugins/apps.html#pagination","title":"Pagination","text":"name type order_by <code>str</code> Field used for sorting.default: <code>upload_create_time</code> order <code>str</code> Sorting order.default: <code>desc</code> page_size <code>int</code> Number of results on each page.default: <code>20</code>"},{"location":"howto/plugins/apps.html#menu_1","title":"Menu","text":"<p>Defines a menu that is shown on the left side of the search interface. Menus have a controllable width, and contains items. Items in the menu are displayed on a 12-based grid and you can control the width of each item by using the <code>width</code> field. You can also nest menus within each other.</p> name type width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> title <code>str | None</code> Custom item title. size <code>str | str | None</code> Size of the menu. Either use presets as defined by MenuSizeEnum, or then provide valid CSS widths.default: <code>MenuSizeEnum.SM</code> indentation <code>int | None</code> Indentation level for the menu.default: <code>0</code> items <code>list[MenuItemTerms | MenuItemHistogram | MenuItemPeriodicTable | MenuItemNestedObject | MenuItemVisibility | MenuItemDefinitions | MenuItemOptimade | MenuItemCustomQuantities | Menu] | None</code> List of items in the menu."},{"location":"howto/plugins/apps.html#menuitemterms","title":"MenuItemTerms","text":"<p>Menu item that shows a list of text values from e.g. <code>str</code> or <code>MEnum</code> quantities.</p> name type search_quantity <code>str</code> The targeted search quantity.default: <code>PydanticUndefined</code> scale <code>str</code> Statistics scaling.default: <code>ScaleEnum.LINEAR</code>options: - <code>linear</code> - <code>log</code> - <code>1/2</code> - <code>1/4</code> - <code>1/8</code> show_input <code>bool</code> Whether to show text input field.default: <code>True</code> width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> n_columns <code>int</code> The number of columns to use when displaying the options.default: <code>1</code> sort_static <code>bool</code> Whether to sort static options by their occurrence in the data. Options are static if they are read from the enum options of the field or if they are explicitly given as a dictionary in 'options'.default: <code>True</code> show_statistics <code>bool</code> Whether to show statistics for the options.default: <code>True</code> quantity <code>str | None</code> deprecated showinput <code>bool | None</code> deprecated query_mode <code>str | None</code> The query mode to use when multiple terms are selected. title <code>str | None</code> Custom item title. options <code>int | bool | dict[str, MenuItemOption] | None</code> Used to control the displayed options: - If not specified, sensible default options are shown based on the    definition. For enum fields all of the defined options are shown,    whereas for generic string fields the top 5 options are shown. - If a number is specified, that many options are dynamically fetched    in order of occurrence. Set to 0 to completely disable options. - If a dictionary of str + MenuItemOption pairs is given, only these    options will be shown."},{"location":"howto/plugins/apps.html#menuitemoption","title":"MenuItemOption","text":"<p>Represents an option shown for a filter.</p> name type label <code>str | None</code> The label to show for this option. description <code>str | None</code> Detailed description for this option."},{"location":"howto/plugins/apps.html#menuitemvisibility","title":"MenuItemVisibility","text":"<p>Menu item that shows a radio button that can be used to change the visiblity.</p> name type width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> title <code>str | None</code> Custom item title."},{"location":"howto/plugins/apps.html#menuitemcustomquantities","title":"MenuItemCustomQuantities","text":"<p>Menu item that shows a search dialog for filtering by custom quantities coming from all different custom schemas, including YAML and Python schemas. Will only show quantities that have been populated in the data.</p> name type width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> title <code>str | None</code> Custom item title."},{"location":"howto/plugins/apps.html#menuitemoptimade","title":"MenuItemOptimade","text":"<p>Menu item that shows a dialog for entering OPTIMADE queries.</p> name type width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> title <code>str | None</code> Custom item title."},{"location":"howto/plugins/apps.html#menuitemperiodictable","title":"MenuItemPeriodicTable","text":"<p>Menu item that shows a periodic table built from values stored into a text quantity.</p> name type search_quantity <code>str</code> The targeted search quantity.default: <code>PydanticUndefined</code> width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> show_statistics <code>bool</code> Whether to show statistics for the options.default: <code>True</code> quantity <code>str | None</code> deprecated scale <code>str | None</code> Statistics scaling.default: <code>ScaleEnum.LINEAR</code> title <code>str | None</code> Custom item title."},{"location":"howto/plugins/apps.html#menuitemhistogram","title":"MenuItemHistogram","text":"<p>Menu item that shows a histogram for numerical or timestamp quantities.</p> name type show_input <code>bool</code> Whether to show text input field.default: <code>True</code> x <code>Axis | str</code> Configures the information source and display options for the x-axis.default: <code>PydanticUndefined</code> y <code>AxisScale | str</code> Configures the information source and display options for the y-axis.default: <code>PydanticUndefined</code> autorange <code>bool</code> Whether to automatically set the range according to the data limits.default: <code>False</code> width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> show_statistics <code>bool</code> Whether to show the full histogram, or just a range slider.default: <code>True</code> quantity <code>str | None</code> deprecated scale <code>str | None</code> deprecated showinput <code>bool | None</code> deprecated n_bins <code>int | None</code> Maximum number of histogram bins. Notice that the actual number of bins may be smaller if there are fewer data items available. nbins <code>int | None</code> deprecated title <code>str | None</code> Custom item title."},{"location":"howto/plugins/apps.html#menuitemnestedobject","title":"MenuItemNestedObject","text":"<p>Menu item that can be used to wrap several subitems into a nested object. By wrapping items with this class the query for them is performed as an Elasticsearch nested query: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html. Note that you cannot yet use nested queries for search quantities originating from custom schemas.</p> name type width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> path <code>str</code> Path of the nested object. Typically a section name.default: <code>PydanticUndefined</code> title <code>str | None</code> Custom item title. items <code>list[MenuItemTerms | MenuItemHistogram | MenuItemPeriodicTable | MenuItemVisibility | MenuItemDefinitions | MenuItemOptimade | MenuItemCustomQuantities] | None</code> Items that are grouped by this nested object."},{"location":"howto/plugins/apps.html#menuitemdefinitions","title":"MenuItemDefinitions","text":"<p>Menu item that shows a tree for filtering data by the presence of definitions.</p> name type width <code>int</code> Width of the item, 12 means maximum width. Note that the menu size can be changed.default: <code>12</code> show_header <code>bool</code> Whether to show the header.default: <code>True</code> title <code>str | None</code> Custom item title."},{"location":"howto/plugins/apps.html#searchquantities","title":"SearchQuantities","text":"<p>Controls the quantities that are available in the search interface. Search quantities correspond to pieces of information that can be queried in the search interface of the app, but also targeted in the rest of the app configuration. You can load quantities from custom schemas as search quantities, but note that not all quantities will be loaded: only scalar values are supported at the moment. The <code>include</code> and <code>exlude</code> attributes can use glob syntax to target metainfo, e.g. <code>results.*</code> or <code>*.#myschema.schema.MySchema</code>.</p> name type include <code>list[str] | None</code> List of included options. Supports glob/wildcard syntax. exclude <code>list[str] | None</code> List of excluded options. Supports glob/wildcard syntax. Has higher precedence than include."},{"location":"howto/plugins/apps.html#filtermenus","title":"FilterMenus","text":"<p>Contains filter menu definitions and controls their availability.</p> name type include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include. options <code>dict[str, FilterMenu] | None</code> Contains the available filter menu options."},{"location":"howto/plugins/apps.html#filtermenu","title":"FilterMenu","text":"<p>Defines the layout and functionality for a filter menu.</p> name type label <code>str | None</code> Menu label to show in the UI. level <code>int | None</code> Indentation level of the menu.default: <code>0</code> size <code>str | None</code> Width of the menu.default: <code>FilterMenuSizeEnum.S</code> actions <code>FilterMenuActions | None</code>"},{"location":"howto/plugins/apps.html#filtermenuactions","title":"FilterMenuActions","text":"<p>Contains filter menu action definitions and controls their availability.</p> name type include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include. options <code>dict[str, FilterMenuActionCheckbox] | None</code> Contains options for filter menu actions."},{"location":"howto/plugins/apps.html#filtermenuactioncheckbox","title":"FilterMenuActionCheckbox","text":"<p>Contains definition for checkbox action in the filter menu.</p> name type type <code>str</code> Action type.default: <code>PydanticUndefined</code>options: - <code>checkbox</code> label <code>str</code> Label to show.default: <code>PydanticUndefined</code> quantity <code>str</code> Targeted quantitydefault: <code>PydanticUndefined</code>"},{"location":"howto/plugins/apps.html#column","title":"Column","text":"<p>Column show in the search results table. With <code>quantity</code> you may target a specific part of the data to be shown. Note that the use of JMESPath is supported here, and you can e.g. do the following:</p> <ul> <li>Show first value from a repeating subsection: <code>repeating_section[0].quantity</code></li> <li>Show slice of values from a repeating subsection: <code>repeating_section[1:2].quantity</code></li> <li>Show all values from a repeating subsection: <code>repeating_section[*].quantity</code></li> <li>Show minimum value from a repeating section: <code>min(repeating_section[*].quantity)</code></li> <li>Show instance that matches a criterion: <code>repeating_section[?label=='target'].quantity</code></li> </ul> name type selected <code>bool</code> Is this column initially selected to be shown.default: <code>False</code> align <code>str</code> Alignment in the table.default: <code>AlignEnum.LEFT</code>options: - <code>left</code> - <code>right</code> - <code>center</code> search_quantity <code>str | None</code> Path of the targeted quantity. Note that you can most of the features JMESPath syntax here to further specify a selection of values. This becomes especially useful when dealing with repeated sections or statistical values. quantity <code>str | None</code> deprecated title <code>str | None</code> Label shown in the header. Defaults to the quantity name. label <code>str | None</code> Alias for title. unit <code>str | None</code> Unit to convert to when displaying. If not given will be displayed in using the default unit in the active unit system. format <code>Format | None</code> Controls the formatting of the values."},{"location":"howto/plugins/apps.html#format","title":"Format","text":"<p>Value formatting options.</p> name type decimals <code>int</code> Number of decimals to show for numbers.default: <code>3</code> mode <code>str</code> Display mode for numbers.default: <code>ModeEnum.SCIENTIFIC</code>options: - <code>standard</code> - <code>scientific</code> - <code>separators</code> - <code>date</code> - <code>time</code>"},{"location":"howto/plugins/example_uploads.html","title":"How to write an example upload","text":"<p>Example uploads can be used to add representative collections of data for your plugin. Example uploads are available for end-users in the Uploads-page under the Add example uploads-button. There, users can instantiate an example upload with a click. This can be very useful for educational or demonstration purposes but also for testing.</p> <p>This documentation shows you how to write a plugin entry point for an example upload. You should read the introduction to plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/example_uploads.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing an example upload. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 example_uploads\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 getting_started\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u251c\u2500\u2500 MANIFEST.in\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/example_uploads.html#example-upload-entry-point","title":"Example upload entry point","text":"<p>The entry point is an instance of an <code>ExampleUploadEntryPoint</code> or its subclass. It defines basic information about your example upload and is used to automatically load the associated data into a NOMAD distribution. The entry point should be defined in <code>*/example_uploads/__init__.py</code> like this:</p> <pre><code>from nomad.config.models.plugins import ExampleUploadEntryPoint\n\nmyexampleupload = ExampleUploadEntryPoint(\n    title = 'My Example Upload',\n    category = 'Examples',\n    description = 'Description of this example upload.',\n    resources=['example_uploads/getting_started/*']\n)\n</code></pre> <p>The <code>resources</code> field can contain one or several data resources that can be provided directly in the Python package or stored online. You can learn more about different data loading options in the next section. In the reference you can also see all of the available configuration options for an <code>ExampleUploadEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the example upload to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyexampleupload = \"nomad_example.example_uploads:myexampleupload\"\n</code></pre>"},{"location":"howto/plugins/example_uploads.html#including-data-in-an-example-upload","title":"Including data in an example upload","text":"<p>There are three main ways to include data in an example upload, and you can also combine these different methods:</p> <ol> <li> <p>Data stored directly in the plugin package:</p> <p>You can include data from the Python package by specifying file or folder names. Here are some examples:</p> <pre><code># Include a file into the upload\nresources = 'example_uploads/getting_started/README.md'\n\n# Include the entire folder recursively\nresources = 'example_uploads/getting_started'\n\n# Include contents of a folder recursively\nresources = 'example_uploads/getting_started/*'\n\n# Include a file/folder into a specific location within the upload\nresources = UploadResource(\n    path='example_uploads/getting_started',\n    target='upload_subfolder'\n)\n\n# Include multiple files/folders. The resources will be added in the given order.\nresources = [\n    'example_uploads/getting_started/README.md',\n    'example_uploads/getting_started/data.txt'\n]\n</code></pre> <p>This is convenient if you have relative small example data that can be tracked in version control. The path should be given relative to the package installation location (<code>src/&lt;package-name&gt;</code>), and you should ensure that the data is distributed with your Python package. Distribution of additional data files in Python packages is controlled with the <code>MANIFEST.in</code> file. If you create a plugin with our template, the <code>src/&lt;package-name&gt;/example_uploads</code> folder is included automatically in <code>MANIFEST.in</code>. If you later add an example upload entry point to your plugin, remember to include the folder by adding the following line to <code>MANIFEST.in</code>:</p> <pre><code>graft src/&lt;package-name&gt;/&lt;path&gt;\n</code></pre> </li> <li> <p>Data retrieved online:</p> <p>If your example uploads are very large (&gt;100MB), storing them in Git may become unpractical. In order to deal with larger uploads, they can be stored in a separate online service. For example, Zenodo is an open and free platform for hosting scientific data. To load such external resources, you can specify one or multiple URLs as resources:</p> <pre><code># Include single ZIP file (note that ZIP contents are not automatically extracted)\nresources = 'http://my_large_file_address.zip'\n\n# Include a file into a specific location within the upload\nresources = UploadResource(\n    path='http://my_large_file_address.zip',\n    target='upload_subfolder'\n)\n\n# Include multiple online files\nresources = ['http://my_large_file_address.zip', 'http://data.txt']\n</code></pre> <p>Note that by default online files are downloaded only when the user requests the creation of an example upload, and that the downloaded files are not cached.</p> </li> <li> <p>Data retrieved with a custom method:</p> <p>If the above options do not suite your use case, you can also override the <code>load</code>-method of <code>ExampleUploadEntryPoint</code> to perform completely custom data loading logic. Note that the <code>load</code> function receives the root upload folder as an argument, and you should store all files in this location. Below is an example of a custom <code>load</code> function that generates a data file on the fly:</p> <pre><code>import numpy as np\nfrom nomad.config.models.plugins import ExampleUploadEntryPoint, UploadPath\n\n\nclass MyExampleUploadEntryPoint(ExampleUploadEntryPoint):\n\n    def load(self, upload_path: str):\n        \"\"\"Custom load function that generates a data file on the fly.\"\"\"\n        filepath = os.path.join(upload_path, 'my_large_data.npy')\n        np.save(filepath, np.ones((1000, 1000)))\n</code></pre> <p>The utility function <code>ExampleUploadEntryPoint.resolve_resource</code> can be used for downloading files into the correct location, see the default <code>load</code> function for reference.</p> </li> </ol>"},{"location":"howto/plugins/normalizers.html","title":"How to write a normalizer","text":"<p>A normalizer takes the archive of an entry as input and manipulates (usually expands) the given archive. This way, a normalizer can add additional sections and quantities based on the information already available in the archive. All normalizers are executed in the order determined by their <code>level</code> after parsing, but the normalizer may decide to not do anything based on the entry contents.</p> <p>This documentation shows you how to write a plugin entry point for a normaliser. You should read the introduction to plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/normalizers.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing a normalizer. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 normalizers\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mynormalizer.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/normalizers.html#normalizer-entry-point","title":"Normalizer entry point","text":"<p>The entry point defines basic information about your normalizer and is used to automatically load the normalizer code into a NOMAD distribution. It is an instance of a <code>NormalizerEntryPoint</code> or its subclass and it contains a <code>load</code> method which returns a <code>nomad.normalizing.Normalizer</code> instance that will perform the actual normalization. You will learn more about the <code>Normalizer</code> class in the next sections. The entry point should be defined in <code>*/normalizers/__init__.py</code> like this:</p> <pre><code>from nomad.config.models.plugins import NormalizerEntryPoint\n\n\nclass MyNormalizerEntryPoint(NormalizerEntryPoint):\n\n    def load(self):\n        from nomad_example.normalizers.mynormalizer import MyNormalizer\n\n        return MyNormalizer(**self.dict())\n\n\nmynormalizer = MyNormalizerEntryPoint(\n    name = 'MyNormalizer',\n    description = 'My custom normalizer.',\n)\n</code></pre> <p>Here you can see that a new subclass of <code>NormalizerEntryPoint</code> was defined. In this new class you can override the <code>load</code> method to determine how the <code>Normalizer</code> class is instantiated, but you can also extend the <code>NormalizerEntryPoint</code> model to add new configurable parameters for this normalizer as explained in Explanation &gt; Plugins &gt; Plugin Configuration.</p> <p>We also instantiate an object <code>mynormalizer</code> from the new subclass. This is the final entry point instance in which you specify the default parameterization and other details about the normalizer. In the reference you can see all of the available configuration options for a <code>NormalizerEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the normalizer to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmynormalizer = \"nomad_example.normalizers:mynormalizer\"\n</code></pre>"},{"location":"howto/plugins/normalizers.html#normalizer-class","title":"<code>Normalizer</code> class","text":"<p>The resource returned by a normalizer entry point must be an instance of a <code>nomad.normalizing.Normalizer</code> class. This normalizer definition should be contained in a separate file (e.g. <code>*/normalizer/mynormalizer.py</code>) and could look like this:</p> <pre><code>from typing import TYPE_CHECKING\n\nfrom nomad.normalizing import Normalizer\n\nif TYPE_CHECKING:\n    from nomad.datamodel import EntryArchive\n\n\nclass MyNormalizer(Normalizer):\n    def normalize(\n        self,\n        archive: EntryArchive,\n        logger=None,\n    ) -&gt; None:\n        logger.info('MyNormalizer called')\n</code></pre> <p>The minimal requirement is that your class has a <code>normalize</code> function, which as input takes:</p> <ul> <li><code>archive</code>: The <code>EntryArchive</code> object in which the normalization results will be stored</li> <li><code>logger</code>: Logger that you can use to log normalization events into</li> </ul>"},{"location":"howto/plugins/normalizers.html#systembasednormalizer-class","title":"<code>SystemBasedNormalizer</code> class","text":"<p><code>SystemBasedNormalizer</code> is a special base class for normalizing systems that allows to run the normalization on all (or only the resulting) <code>representative</code> systems:</p> <pre><code>from nomad.atomutils import get_volume\nfrom nomad.normalizing import SystemBasedNormalizer\n\nclass UnitCellVolumeNormalizer(SystemBasedNormalizer):\n    def _normalize_system(self, system, is_representative):\n        system.unit_cell_volume = get_volume(system.lattice_vectors.magnitude)\n</code></pre> <p>For <code>SystemBasedNormalizer</code>, we implement the <code>_normalize_system</code> method. The parameter <code>is_representative</code> will be true for the <code>representative</code> systems. The representative system refers to the system that corresponds to the calculation result. It is determined by scanning the archive sections starting with <code>workflow2</code> until the system fitting the criterion is found. For example, it refers to the final step in a geometry optimization or other workflow.</p> <p>Of course, if you add new information to the archive, this also needs to be defined in the schema (see How-to extend the schema). For example you could extend the section system with a special system definition that extends the existing section system definition:</p> <pre><code>import numpy as np\nfrom nomad.datamodel.metainfo import runschema\nfrom nomad.metainfo import Section, Quantity\n\nclass UnitCellVolumeSystem(runschema.system.System):\n    m_def = Section(extends_base_section=True)\n    unit_cell_volume = Quantity(np.dtype(np.float64), unit='m^3')\n</code></pre> <p>Here, we used the schema definition for the <code>run</code> section defined in this plugin.</p>"},{"location":"howto/plugins/normalizers.html#control-normalizer-execution-order","title":"Control normalizer execution order","text":"<p><code>NormalizerEntryPoints</code> have an attribute <code>level</code>, which you can use to control their execution order. Normalizers are executed in order from lowest level to highest level. The default level for normalizers is <code>0</code>, but this can be changed per installation using <code>nomad.yaml</code>:</p> <pre><code>plugins:\n  entry_points:\n    options:\n      \"nomad_example.normalizers:mynormalizer1\":\n        level: 1\n      \"nomad_example.normalizers:mynormalizer2\":\n        level: 2\n</code></pre>"},{"location":"howto/plugins/normalizers.html#running-the-normalizer","title":"Running the normalizer","text":"<p>If you have the plugin package and <code>nomad-lab</code> installed in your Python environment, you can run the normalization as a part of the parsing process using the NOMAD CLI:</p> <pre><code>nomad parse &lt;filepath&gt;\n</code></pre> <p>The output will return the final archive in JSON format.</p> <p>Normalization can also be run within a python script (or Jupyter notebook), e.g., to facilate debugging, with the following code:</p> <pre><code>from nomad.datamodel import EntryArchive\nfrom nomad_example.normalizers.mynormalizer import MyNormalizer\nimport logging\n\nnormalizer = MyNormalizer()\nentry_archive = EntryArchive()\nnormalizer.normalize(entry_archive, logger=logging.getLogger())\n\nprint(entry_archive.m_to_dict())\n</code></pre>"},{"location":"howto/plugins/parsers.html","title":"How to write a parser","text":"<p>NOMAD uses parsers to automatically extract information from raw files and output that information into structured archives. Parsers can decide which files act upon based on the filename, mime type or file contents and can also decide into which schema the information should be populated into.</p> <p>This documentation shows you how to write a plugin entry point for a parser. You should read the introduction to plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/parsers.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing a parser. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 parsers\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 myparser.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugin, including linting, testing and documenting.</p>"},{"location":"howto/plugins/parsers.html#parser-entry-point","title":"Parser entry point","text":"<p>The entry point defines basic information about your parser and is used to automatically load the parser code into a NOMAD distribution. It is an instance of a <code>ParserEntryPoint</code> or its subclass and it contains a <code>load</code> method which returns a <code>nomad.parsing.Parser</code> instance that will perform the actual parsing. You will learn more about the <code>Parser</code> class in the next sections. The entry point should be defined in <code>*/parsers/__init__.py</code> like this:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import ParserEntryPoint\n\n\nclass MyParserEntryPoint(ParserEntryPoint):\n\n    def load(self):\n        from nomad_example.parsers.myparser import MyParser\n\n        return MyParser(**self.dict())\n\n\nmyparser = MyParserEntryPoint(\n    name = 'MyParser',\n    description = 'My custom parser.',\n    mainfile_name_re = '.*\\.myparser',\n)\n</code></pre> <p>Here you can see that a new subclass of <code>ParserEntryPoint</code> was defined. In this new class you can override the <code>load</code> method to determine how the <code>Parser</code> class is instantiated, but you can also extend the <code>ParserEntryPoint</code> model to add new configurable parameters for this parser as explained in Explanation &gt; Plugins &gt; Plugin Configuration.</p> <p>We also instantiate an object <code>myparser</code> from the new subclass. This is the final entry point instance in which you specify the default parameterization and other details about the parser. In the reference you can see all of the available configuration options for a <code>ParserEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for the parser to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyparser = \"nomad_example.parsers:myparser\"\n</code></pre>"},{"location":"howto/plugins/parsers.html#parser-class","title":"<code>Parser</code> class","text":"<p>The resource returned by a parser entry point must be an instance of a <code>nomad.parsing.Parser</code> class. In many cases you will, however, want to use the already existing <code>nomad.parsing.MatchingParser</code> subclass that takes care of the file matching process for you. This parser definition should be contained in a separate file (e.g. <code>*/parsers/myparser.py</code>) and could look like this:</p> <pre><code>from typing import Dict\n\nfrom nomad.datamodel import EntryArchive\nfrom nomad.parsing import MatchingParser\n\n\nclass MyParser(MatchingParser):\n    def parse(\n        self,\n        mainfile: str,\n        archive: EntryArchive,\n        logger=None,\n        child_archives: Dict[str, EntryArchive] = None,\n    ) -&gt; None:\n        logger.info('MyParser called')\n</code></pre> <p>If you are using the <code>MatchingParser</code> interface, the minimal requirement is that your class has a <code>parse</code> function, which will take as input:</p> <ul> <li><code>mainfile</code>: Filepath to a raw file that the parser should open and run on</li> <li><code>archive</code>: The <code>EntryArchive</code> object in which the parsing results will be stored</li> <li><code>logger</code>: Logger that you can use to log parsing events into</li> </ul> <p>Note here that if using <code>MatchingParser</code>, the process of identifying which files the <code>parse</code> method is run against is take care of by passing in the required parameters to the instance in the <code>load</code> mehod. In the previous section, the <code>load</code> method looked something like this:</p> <pre><code>    def load(self):\n        from nomad_example.parsers.myparser import MyParser\n\n        return MyParser(**self.dict())\n</code></pre> <p>There we are passing all of the entry configuration options to the parser instance, including things like <code>mainfile_name_re</code> and <code>mainfile_contents_re</code>. The <code>MatchingParser</code> constructor uses these parameters to set up the file matching appropriately. If you wish to take full control of the file matching process, you can use the <code>nomad.parsing.Parser</code> class and override the <code>is_mainfile</code> function.</p>"},{"location":"howto/plugins/parsers.html#match-your-raw-file","title":"Match your raw file","text":"<p>If you are using the <code>MatchingParser</code> interface you can configure which files are matched directly in the <code>ParserEntryPoint</code>. For example to match only certain file extensions and file contents, you can use the <code>mainfile_name_re</code> and <code>mainfile_contents_re</code> fields:</p> <pre><code>myparser = MyParserEntryPoint(\n    name = 'MyParser',\n    description = 'My custom parser.',\n    mainfile_name_re = '.*\\.myparser',\n    mainfile_contents_re = '\\s*\\n\\s*HELLO WORLD',\n)\n</code></pre> <p>You can find all of the available matching criteria in the <code>ParserEntryPoint</code> reference</p>"},{"location":"howto/plugins/parsers.html#running-a-parser","title":"Running a parser","text":"<p>Parsers automatically run for the matched files within a NOMAD distribution, but it is also possible to run the manually for specific files. This can be useful for testing and for connecting them into external software.</p>"},{"location":"howto/plugins/parsers.html#using-the-cli","title":"Using the CLI","text":"<p>If you have installed a NOMAD plugin into a Python virtual environment, you can run a parser from that plugin with the <code>nomad</code> command line interface. The following command will uses the CLI to parse a given input file, and store the resulting JSON output into an output file:</p> <pre><code>nomad parse &lt;input-file&gt; &gt; &lt;output-file&gt;\n</code></pre> <p>The parse command will automatically match the right parser to your file and run the parser. To skip the parser matching, i.e. the process that determined which parser fits to the given file, you can use the <code>--parser</code> argument to provide a parser entry point id:</p> <pre><code>nomad parse --parser &lt;parser_entry_point_id&gt; &lt;input-file&gt;\n</code></pre> <p>You can check the CLI reference for <code>nomad parse</code> for the full list of arguments, but the following should get you started:</p> <ul> <li><code>--show-metadata</code>: Return json representation of the basic metadata</li> <li><code>--skip-normalizers</code>: Skip any normalizers</li> <li><code>--preview-plots</code>: Optionally previews the generated plots.</li> <li><code>--save-plot-dir &lt;directory&gt;</code>: Specifies a directory to save the plot images.</li> </ul>"},{"location":"howto/plugins/parsers.html#within-python-code","title":"Within python code","text":"<p>You can also invoke the NOMAD parsers using Python. This will give you the parse results as metainfo objects to conveniently analyze the results in Python. You can either run the parser class directly:</p> <pre><code>from nomad.datamodel import EntryArchive\nfrom nomad_example.parsers.myparser import MyParser\nimport logging\n\np = ExampleParser()\na = EntryArchive()\np.parse('tests/data/example.out', a, logger=logging.getLogger())\n\nprint(a.m_to_dict())\n</code></pre> <p>Or alternatively through the <code>parse</code> function that is also used internally by the CLI:</p> <pre><code>import sys\nfrom nomad.client import parse, normalize_all\n\n# Match and run the parser\narchives = parse('path/to/your/file')\n# Run all normalizers\nfor archive in archives:\n    normalize_all(archive)\n\n    # Get the 'main section' section_run as a metainfo object\n    section_run = archive.run[0]\n\n    # Get the same data as JSON serializable Python dict\n    python_dict = section_run.m_to_dict()\n</code></pre>"},{"location":"howto/plugins/parsers.html#parsing-text-files","title":"Parsing text files","text":"<p>ASCII text files are amongst the most common files used. Here, we show you how to parse the text by matching specific regular expressions in these files. For the following example, we will use the project file <code>tests/data/example.out</code>:</p> <p>Check out the <code>master</code> branch of the <code>exampleparser</code> project,</p> <pre><code>git checkout master\n</code></pre> <p>and examine the file to be parsed in <code>tests/data/example.out</code>:</p> <pre><code>2020/05/15\n               *** super_code v2 ***\n\nsystem 1\n--------\nsites: H(1.23, 0, 0), H(-1.23, 0, 0), O(0, 0.33, 0)\nlatice: (0, 0, 0), (1, 0, 0), (1, 1, 0)\nenergy: 1.29372\n\n*** This was done with magic source                                ***\n***                                x\u00b042                            ***\n\n\nsystem 2\n--------\nsites: H(1.23, 0, 0), H(-1.23, 0, 0), O(0, 0.33, 0)\ncell: (0, 0, 0), (1, 0, 0), (1, 1, 0)\nenergy: 1.29372\n</code></pre> <p>At the top there is some general information such as date, name of the code (<code>super_code</code>) and its version (<code>v2</code>). Then is information for two systems (<code>system 1</code> and <code>system 2</code>), separated with a string containing a code-specific value <code>magic source</code>. Both system sections contain the quantities <code>sites</code> and <code>energy</code>, but each have a unique quantity as well, <code>latice</code> and <code>cell</code>, respectively.</p> <p>In order to convert the information from this file into the NOMAD archive, we first have to parse the necessary quantities. The <code>nomad-lab</code> Python package provides a <code>text_parser</code> module for declarative (i.e., semi-automated) parsing of text files. You can define text file parsers as follows:</p> <pre><code>def str_to_sites(string):\n    sym, pos = string.split('(')\n    pos = np.array(pos.split(')')[0].split(',')[:3], dtype=float)\n    return sym, pos\n\n\ncalculation_parser = TextParser(\n    quantities=[\n        Quantity(\n            'sites',\n            r'([A-Z]\\([\\d\\.\\, \\-]+\\))',\n            str_operation=str_to_sites,\n            repeats=True,\n        ),\n        Quantity(\n            Model.lattice,\n            r'(?:latice|cell): \\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*\\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*\\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*',\n            repeats=False,\n        ),\n        Quantity('energy', r'energy: (\\d\\.\\d+)'),\n        Quantity(\n            'magic_source',\n            r'done with magic source\\s*\\*{3}\\s*\\*{3}\\s*[^\\d]*(\\d+)',\n            repeats=False,\n        ),\n    ]\n)\n\nmainfile_parser = TextParser(\n    quantities=[\n        Quantity('date', r'(\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d)', repeats=False),\n        Quantity('program_version', r'super\\_code\\s*v(\\d+)\\s*', repeats=False),\n        Quantity(\n            'calculation',\n            r'\\s*system \\d+([\\s\\S]+?energy: [\\d\\.]+)([\\s\\S]+\\*\\*\\*)*',\n            sub_parser=calculation_parser,\n            repeats=True,\n        ),\n    ]\n)\n</code></pre> <p>The quantities to be parsed can be specified as a list of <code>Quantity</code> objects in <code>TextParser</code>. Each quantity should have a name and a regular expression (re) pattern to match the value. The matched value should be enclosed in a group(s) denoted by <code>(...)</code>. In addition, we can specify the following arguments:</p> <ul> <li><code>findall (default=True)</code>: Switches simultaneous matching of all quantities using <code>re.findall</code>. In this case, overlap between matches is not tolerated, i.e. two quantities cannot share the same block in the file. If this cannot be avoided, set <code>findall=False</code> switching to<code>re.finditer</code>. This will perform matching one quantity at a time which is slower but with the benefit that matching is done independently of other quantities.</li> <li><code>repeats (default=False)</code>: Switches finding multiple matches for a quantity. By default, only the first match is returned.</li> <li><code>str_operation (default=None)</code>: An external function to be applied on the matched value to perform more specific string operations. In the above example, we defined <code>str_to_sites</code> to convert the parsed value of the atomic sites.</li> <li><code>sub_parser (default=None)</code>: A nested parser to be applied on the matched block. This can also be a <code>TextParser</code> object with a list of quantities to be parsed or other <code>FileParser</code> objects.</li> <li><code>dtype (default=None)</code>: The data type of the parsed value.</li> <li><code>shape (default=None)</code>: The shape of the parsed data.</li> <li><code>unit (default=None)</code>: The pint unit of the parsed data.</li> <li><code>flatten (default=True)</code>: Switches splitting the parsed string into a flat list.</li> <li><code>convert (default=True)</code>: Switches automatic conversion of parsed value.</li> <li><code>comment (default=None)</code>: String preceding a line to ignore.</li> </ul> <p>A <code>metainfo.Quantity</code> object can also be passed as first argument in place of name in order to define the data type, shape, and unit for the quantity. <code>TextParser</code> returns a dictionary of key-value pairs, where the key is defined by the name of the quantities and the value is based on the matched re pattern.</p> <p>To parse a file, simply do: To parse a file, specify the path to such file and call the <code>parse()</code> function of <code>TextParser</code>:</p> <pre><code>mainfile_parser.mainfile = mainfile\nmainfile_parser.parse()\n</code></pre> <p>This will populate the <code>mainfile_parser</code> object with parsed data and it can be accessed like a Python dict with quantity names as keys or directly as attributes:</p> <pre><code>mainfile_parser.get('date')\n'2020/05/15'\n\nmainfile_parser.calculation\n[TextParser(example.out) --&gt; 4 parsed quantities (sites, lattice_vectors, energy, magic_source), TextParser(example.out) --&gt; 3 parsed quantities (sites, lattice_vectors, energy)]\n</code></pre> <p>The next step is to write the parsed data into the NOMAD archive. We can use one of the predefined plugins containing schema packages in NOMAD. However, to better illustrate the connection between a parser and a schema we will define our own schema in this example (See How to write a schema in python for additional information on this topic). We define a root section called <code>Simulation</code> containing two subsections, <code>Model</code> and <code>Output</code>. The definitions are found in <code>exampleparser/metainfo/example.py</code>:</p> <p><pre><code>class Model(ArchiveSection):\n    m_def = Section()\n\n    n_atoms = Quantity(\n        type=np.int32, description=\"\"\"Number of atoms in the model system.\"\"\"\n    )\n\n    labels = Quantity(\n        type=str, shape=['n_atoms'], description=\"\"\"Labels of the atoms.\"\"\"\n    )\n\n    positions = Quantity(\n        type=np.float64, shape=['n_atoms'], description=\"\"\"Positions of the atoms.\"\"\"\n    )\n\n    lattice = Quantity(\n        type=np.float64,\n        shape=[3, 3],\n        description=\"\"\"Lattice vectors of the model system.\"\"\",\n    )\n\nclass Output(ArchiveSection):\n    m_def = Section()\n\n    model = Quantity(\n        type=Reference(Model), description=\"\"\"Reference to the model system.\"\"\"\n    )\n\n    energy = Quantity(\n        type=np.float64,\n        unit='eV',\n        description=\"\"\"Value of the total energy of the system.\"\"\",\n    )\n\n\nclass Simulation(ArchiveSection):\n    m_def = Section()\n\n    code_name = Quantity(\n        type=str, description=\"\"\"Name of the code used for the simulation.\"\"\"\n    )\n\n    code_version = Quantity(type=str, description=\"\"\"Version of the code.\"\"\")\n\n    date = Quantity(type=Datetime, description=\"\"\"Execution date of the simulation.\"\"\")\n\n    model = SubSection(sub_section=Model, repeats=True)\n\n    output = SubSection(sub_section=Output, repeats=True)\n</code></pre> Each of the classes inherit from the base class <code>ArchiveSection</code>. This is the abstract class used in NOMAD to define sections and subsections in a schema. The <code>Model</code> section is used to store the <code>sites</code> and <code>lattice/cell</code> information, while the <code>Output</code> section is used to store the <code>energy</code> quantity. Each of the classes that we defined is a sub-class of <code>ArchiveSection</code>. This is required in order to assign these sections to the <code>data</code> section of the NOMAD archive.</p> <p>The following is the implementation of the <code>parse</code> function of <code>ExampleParser</code> to write the parsed quantities from our mainfile parser into the archive:</p> <p><pre><code>def parse(self, mainfile: str, archive: EntryArchive, logger):\n    simulation = Simulation(\n        code_name='super_code', code_version=mainfile_parser.get('program_version')\n    )\n    date = datetime.datetime.strptime(mainfile_parser.date, '%Y/%m/%d')\n    simulation.date = date\n\n    for calculation in mainfile_parser.get('calculation', []):\n        model = Model()\n        model.lattice = calculation.get('lattice_vectors')\n        sites = calculation.get('sites')\n        model.labels = [site[0] for site in sites]\n        model.positions = [site[1] for site in sites]\n        simulation.model.append(model)\n\n        output = Output()\n        output.model = model\n        output.energy = calculation.get('energy') * units.eV\n        magic_source = calculation.get('magic_source')\n        if magic_source is not None:\n            archive.workflow2 = Workflow(x_example_magic_value=magic_source)\n        simulation.output.append(output)\n    # put the simulation section into archive data\n    archive.data = simulation\n</code></pre> We first assign the code name and version as well as the date that the simulation was performed. For each of the parsed calculations, we create a model and an output section to which we write the corresponding parsed quantities. Finally, we assign the simulation section to the archive data subsection.</p> <p>Now, run the parser again and check that the new archive stores the intended quantities from <code>tests/data/example.out</code>.</p> <p>Additionally, the standard normalizers will be applied as well. This is run automatically during parsing, one can skip these by passing the argument <code>skip-normalizers</code>.</p>"},{"location":"howto/plugins/parsers.html#extending-the-metainfo","title":"Extending the Metainfo","text":"<p>There are several built-in schemas NOMAD (<code>nomad.datamodel.metainfo</code>).</p> <p>In the example below, we have made use of the base section for workflow and extended it to include a code-specific quantity <code>x_example_magic_value</code>. <pre><code># We extend the existing common definition of section Workflow\nclass ExampleWorkflow(Workflow):\n    # We alter the default base class behavior to add all definitions to the existing\n    # base class instead of inheriting from the base class\n    m_def = Section(extends_base_section=True)\n\n    # We define an additional example quantity. Use the prefix x_&lt;parsername&gt;_ to denote\n    # non common quantities.\n    x_example_magic_value = Quantity(\n        type=int, description='The magic value from a magic source.'\n    )\n</code></pre></p> <p>This is the approach for domain-specific schemas such as for simulation workflows. Refer to how to extend schemas.</p>"},{"location":"howto/plugins/parsers.html#other-fileparser-classes","title":"Other FileParser classes","text":"<p>Aside from <code>TextParser</code>, other <code>FileParser</code> classes are also defined. These include:</p> <ul> <li> <p><code>DataTextParser</code>: in addition to matching strings as in <code>TextParser</code>, this parser uses the <code>numpy.loadtxt</code> function to load structured data files. The loaded <code>numpy.array</code> data can then be accessed from the property data.</p> </li> <li> <p><code>XMLParser</code>: uses the ElementTree module to parse an XML file. The <code>parse</code> method of the parser takes in an XPath-style key to access individual quantities. By default, automatic data type conversion is performed, which can be switched off by setting <code>convert=False</code>.</p> </li> </ul>"},{"location":"howto/plugins/plugins.html","title":"Introduction to plugins","text":"<p>The main way to customize a NOMAD installation is through the use of plugins. A NOMAD plugin is a Python package that an administrator can install into a NOMAD distribution to add custom features. This page helps you develop and publish a NOMAD plugin. For a high-level overview of the plugin mechanism, see the NOMAD plugin system -page.</p> <p>A single Python plugin package can contain multiple plugin entry points. These entry points represent different types of customizations including:</p> <ul> <li>APIs</li> <li>Apps</li> <li>Example uploads</li> <li>Normalizers</li> <li>Parsers</li> <li>Schema packages</li> </ul> <p>See the FAIRmat-NFDI GitHub organization page for a list of plugins developed by FAIRmat. You can also see the list of activated plugins and plugin entry points at the bottom of the Information page (<code>about/information</code>) of any NOMAD installation, for example check out the central NOMAD installation.</p>"},{"location":"howto/plugins/plugins.html#plugin-anatomy","title":"Plugin anatomy","text":"<p>We provide a template repository which you can use to create the initial plugin repository layout for you. The repository layout as generated by the template looks like this:</p> <pre><code>\u251c\u2500\u2500 nomad-example\n\u2502   \u251c\u2500\u2500 src\n|   \u2502   \u251c\u2500\u2500 nomad_example\n|   |   \u2502   \u251c\u2500\u2500 apps\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n|   |   \u2502   \u251c\u2500\u2500 normalizers\n|   |   \u2502   \u2502   \u251c\u2500\u2500 mynormalizer.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n|   |   \u2502   \u251c\u2500\u2500 schema_packages\n|   |   \u2502   \u2502   \u251c\u2500\u2500 mypackage.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n|   |   \u2502   \u251c\u2500\u2500 parsers\n|   |   \u2502   \u2502   \u251c\u2500\u2500 myparser.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u251c\u2500\u2500 LICENSE.txt\n\u2502   \u251c\u2500\u2500 README.md\n</code></pre> <p>We suggest using the following convention for naming the repository name and the plugin package:</p> <ul> <li>repository name: <code>nomad-&lt;plugin name&gt;</code></li> <li>package name: <code>nomad_&lt;plugin name&gt;</code></li> </ul>"},{"location":"howto/plugins/plugins.html#controlling-loading-of-plugin-entry-points","title":"Controlling loading of plugin entry points","text":"<p>By default, plugin entry points are automatically loaded, and as an administrator you only need to install the Python package. You can, however, control which entry points to load by explicitly including/excluding them in your <code>nomad.yaml</code>. For example, if a plugin has the following <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmyparser = \"nomad_example.parsers:myparser\"\n</code></pre> <p>You could disable the parser entry point in your <code>nomad.yaml</code> with:</p> <pre><code>plugins:\n  entry_points:\n    exclude: [\"nomad_plugin.parsers:myparser\"]\n</code></pre>"},{"location":"howto/plugins/plugins.html#plugin-development-guidelines","title":"Plugin development guidelines","text":""},{"location":"howto/plugins/plugins.html#linting-and-formatting","title":"Linting and formatting","text":"<p>While developing NOMAD plugins, we highly recommend using a Python linter, such as Ruff, to analyze and enforce coding standards in your plugin projects. This also ensures smoother integration and collaboration. If you have used our template repository, you will automatically have <code>ruff</code> defined as a development dependency with suitable defaults set in <code>pyproject.toml</code> together with a GitHub actions that runs the linting and formatting checks on each push to the Git repository.</p>"},{"location":"howto/plugins/plugins.html#testing","title":"Testing","text":"<p>For testing, you should use pytest, and a folder structure that mimics the package layout with test modules named after the tested module. For example, if you are developing a parser in <code>myparser.py</code>, the test folder structure should look like this:</p> <pre><code>\u251c\u2500\u2500 nomad-example-plugin\n\u2502   \u251c\u2500\u2500 src\n|   \u2502   \u251c\u2500\u2500 nomad_example\n|   |   \u2502   \u251c\u2500\u2500 parsers\n|   |   \u2502   \u2502   \u251c\u2500\u2500 myparser.py\n|   |   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 tests\n|   \u2502   \u251c\u2500\u2500 parsers\n|   |   \u2502   \u251c\u2500\u2500 test_myparser.py\n|   |   \u2502   \u251c\u2500\u2500 conftest.py\n|   \u2502   \u251c\u2500\u2500 conftest.py\n</code></pre> <p>Any shared test utilities (such as <code>pytest</code> fixtures) should live in <code>conftest.py</code> modules placed at the appropriate level in the folder hierarchy, i.e. utilities dealing with parsers would live in <code>tests/parsers/conftest.py</code>, while root level utilities would live in <code>tests/conftest.py</code>. If you have used our template repository, you will automatically have an initial test folder structure, <code>pytest</code> defined as a development dependency in <code>pyproject.toml</code> and a GitHub action that runs the test suite on each push to the Git repository.</p> <p>In the <code>pytest</code> framework, test cases are created by defining functions with the <code>test_</code> prefix, which perform assertions. A typical test case could look like this:</p> <pre><code>def test_parse_file():\n    parser = MyParser()\n    archive = EntryArchive()\n    parser.parse('tests/data/example.out', archive, logging)\n\n    sim = archive.data\n    assert len(sim.model) == 2\n    assert len(sim.output) == 2\n    assert archive.workflow2.x_example_magic_value == 42\n</code></pre> <p>You can run all the tests in the <code>tests/</code> directory with:</p> <pre><code>python -m pytest -svx tests\n</code></pre>"},{"location":"howto/plugins/plugins.html#documentation","title":"Documentation","text":"<p>As your plugin matures, you should also think about documenting its usage. We recommend using <code>mkdocs</code> to create your documentation as a set of markdown files. If you have used our template repository, you will automatically have an initial documentation folder structure, <code>mkdocs</code> defined as a development dependency in <code>pyproject.toml</code> and a GitHub action that builds the docs to a separate <code>gh-pages</code> branch each push to the Git repository. Note that if you wish to host the documentation using GitHub pages, you need to enable this in the repository settings.</p>"},{"location":"howto/plugins/plugins.html#publishing-a-plugin","title":"Publishing a plugin","text":"<p>Attention</p> <p>The standard processes for publishing plugins and using plugins from other developers are still being worked out. The \"best\" practices mentioned in the following are preliminary. We aim to set up a dedicated plugin registry that allows you to publish your plugin and find plugins from others.</p>"},{"location":"howto/plugins/plugins.html#github-repository","title":"GitHub repository","text":"<p>The simplest way to publish a plugin is to have it live in a publicly shared Git repository. The package can then be installed with:</p> <pre><code>pip install git+https://&lt;repository_url&gt;\n</code></pre> <p>Note</p> <p>If you develop a plugin in the context of FAIRmat or the NOMAD CoE, put your plugin repositories in the corresponding GitHub organization.</p>"},{"location":"howto/plugins/plugins.html#pypipip-package","title":"PyPI/pip package","text":"<p>You may additionally publish the plugin package in PyPI. Learn from the PyPI documentation how to create a package for PyPI. We recommend to use the <code>pyproject.toml</code>-based approach.</p> <p>The PyPI documentation provides further information about how to publish a package to PyPI. If you have access to the MPCDF GitLab and NOMAD's presence there, you can also use the <code>nomad-FAIR</code> package registry:</p> <pre><code>pip install twine\ntwine upload \\\n    -u &lt;username&gt; -p &lt;password&gt; \\\n    --repository-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi \\\n    dist/nomad-example-plugin-*.tar.gz\n</code></pre>"},{"location":"howto/plugins/plugins.html#installing-a-plugin","title":"Installing a plugin","text":"<p>See our documentation on How to install plugins into a NOMAD Oasis.</p>"},{"location":"howto/plugins/schema_packages.html","title":"How to write a schema package","text":"<p>Schema packages are used to define and distribute custom data definitions that can be used within NOMAD. These schema packages typically contain schemas that users can select to instantiate manually filled entries using our ELN functionality, or that parsers select when organizing data they extract from files. Schema packages may also contain more abstract base classes that other schema packages use.</p> <p>This documentation shows you how to write a plugin entry point for a schema package. You should read the introduction to plugins to have a basic understanding of how plugins and plugin entry points work in the NOMAD ecosystem.</p>"},{"location":"howto/plugins/schema_packages.html#getting-started","title":"Getting started","text":"<p>You can use our template repository to create an initial structure for a plugin containing a schema package. The relevant part of the repository layout will look something like this:</p> <pre><code>nomad-example\n   \u251c\u2500\u2500 src\n   \u2502   \u251c\u2500\u2500 nomad_example\n   \u2502   \u2502   \u251c\u2500\u2500 schema_packages\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mypackage.py\n   \u251c\u2500\u2500 LICENSE.txt\n   \u251c\u2500\u2500 README.md\n   \u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>See the documentation on plugin development guidelines for more details on the best development practices for plugins, including linting, testing and documenting.</p>"},{"location":"howto/plugins/schema_packages.html#schema-package-entry-point","title":"Schema package entry point","text":"<p>The entry point defines basic information about your schema package and is used to automatically load it into a NOMAD distribution. It is an instance of a <code>SchemaPackageEntryPoint</code> or its subclass and it contains a <code>load</code> method which returns a <code>nomad.metainfo.SchemaPackage</code> instance that contains section and schema definitions. You will learn more about the <code>SchemaPackage</code> class in the next sections. The entry point should be defined in <code>*/schema_packages/__init__.py</code> like this:</p> <pre><code>from pydantic import Field\nfrom nomad.config.models.plugins import SchemaPackageEntryPoint\n\n\nclass MySchemaPackageEntryPoint(SchemaPackageEntryPoint):\n\n    def load(self):\n        from nomad_example.schema_packages.mypackage import m_package\n\n        return m_package\n\n\nmypackage = MySchemaPackageEntryPoint(\n    name = 'MyPackage',\n    description = 'My custom schema package.',\n)\n</code></pre> <p>Here you can see that a new subclass of <code>SchemaPackageEntryPoint</code> was defined. In this new class you can override the <code>load</code> method to determine how the <code>SchemaPackage</code> class is loaded, but you can also extend the <code>SchemaPackageEntryPoint</code> model to add new configurable parameters for this schema package as explained here.</p> <p>We also instantiate an object <code>mypackage</code> from the new subclass. This is the final entry point instance in which you specify the default parameterization and other details about the schema package. In the reference you can see all of the available configuration options for a <code>SchemaPackageEntryPoint</code>.</p> <p>The entry point instance should then be added to the <code>[project.entry-points.'nomad.plugin']</code> table in <code>pyproject.toml</code> in order for it to be automatically detected:</p> <pre><code>[project.entry-points.'nomad.plugin']\nmypackage = \"nomad_example.schema_packages:mypackage\"\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#schemapackage-class","title":"<code>SchemaPackage</code> class","text":"<p>The <code>load</code>-method of a schema package entry point returns an instance of a <code>nomad.metainfo.SchemaPackage</code> class. This definition should be contained in a separate file (e.g. <code>*/schema_packages/mypackage.py</code>) and could look like this:</p> <pre><code>import ase\n\nfrom nomad.datamodel.data import Schema\nfrom nomad.metainfo import MEnum, MSection, Quantity, SchemaPackage, SubSection\n\nm_package = SchemaPackage()\n\n\nclass System(MSection):\n    '''\n    A system section includes all quantities that describe a single simulated\n    system (a.k.a. geometry).\n    '''\n\n    n_atoms = Quantity(\n        type=int, description='''\n        Defines the number of atoms in the system.\n        ''')\n\n    atom_labels = Quantity(\n        type=MEnum(ase.data.chemical_symbols), shape=['n_atoms'])\n    atom_positions = Quantity(type=float, shape=['n_atoms', 3], unit='angstrom')\n    simulation_cell = Quantity(type=float, shape=[3, 3], unit='angstrom')\n    pbc = Quantity(type=bool, shape=[3])\n\n\nclass Simulation(Schema):\n    system = SubSection(sub_section=System, repeats=True)\n\nm_package.__init_metainfo__()\n</code></pre> <p>Schema packages typically contain one or several schema definitions, that can the be used to manually create new entries through the ELN functionality, or also by parsers to create instances of this schema fully automatically. All of the definitions contained in the package should be placed between the contructor call (<code>m_package = SchemaPackage()</code>) and the initialization (<code>m_package.__init_metainfo__()</code>).</p> <p>In this basic example we defined two sections: <code>System</code> and <code>Simulation</code>. <code>System</code> inherits from most primitive type of section - <code>MSection</code> - whereas <code>Simulation</code> is defined as a subclass of <code>Schema</code> which makes it possible to use this as the root section of an entry. Each section can have two types of properties: quantities and subsections. Sections and their properties are defined with Python classes and their attributes. Each quantity defines a piece of data. Basic quantity attributes are <code>type</code>, <code>shape</code>, <code>unit</code>, and <code>description</code>.</p> <p>Subsections allow the placement of sections within each other, forming containment hierarchies. Basic subsection attributes are <code>sub_section</code>\u2014a reference to the section definition of the subsection\u2014and <code>repeats</code>\u2014determines whether a subsection can be included once or multiple times.</p> <p>To use the above-defined schema and create actual data, we have to instantiate the classes:</p> <pre><code>simulation = Simulation()\nsystem = System()\nsystem.n_atoms = 3\nsystem.atom_labels = ['H', 'H', 'O']\nsimulation.system.append(system)\n</code></pre> <p>Section instances can be used like regular Python objects: quantities and subsections can be set and accessed like any other Python attribute. Special metainfo methods, starting with <code>m_</code> allow us to realize more complex semantics. For example <code>m_create</code> will instantiate a subsection and add it to the parent section in one step.</p> <p>Another example for an <code>m_</code>-method is:</p> <pre><code>simulation.m_to_json(indent=2)\n</code></pre> <p>This will convert the data into JSON:</p> <pre><code>{\n  \"system\": [\n    {\n      \"n_atoms\": 3,\n      \"atom_labels\": [\n        \"H\",\n        \"H\",\n        \"O\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#schema-packages-python-vs-yaml","title":"Schema packages: Python vs. YAML","text":"<p>In this guide, we explain how to write and upload schema packages in the <code>.archive.yaml</code> format. Writing and uploading such YAML schema packages is a good way for NOMAD users to start exploring schemas, but it has limitations. As a NOMAD developer or Oasis administrator you can add Python schema packages to NOMAD. All built-in NOMAD schemas (e.g. for electronic structure code data) are written in Python and are part of the NOMAD sources (<code>nomad.datamodel.metainfo.*</code>).</p> <p>There is a 1-1 translation between the structure in Python schema packages (written in classes) and YAML (or JSON) schema packages (written in objects). Both use the same fundamental concepts, like section, quantity, or subsection, introduced in YAML schemas. The main benefit of Python schema packages is the ability to define custom <code>normalize</code>-functions.</p> <p><code>normalize</code>-functions are attached to sections and are are called when instances of these sections are processed. All files are processed when they are uploaded or changed. To add a <code>normalize</code> function, your section has to inherit from <code>Schema</code> or <code>ArchiveSection</code> which provides the base for this functionality. Here is an example:</p> <pre><code>from nomad.datamodel import Schema, ArchiveSection\nfrom nomad.metainfo.metainfo import Quantity, Datetime, SubSection\n\n\nclass Sample(ArchiveSection):\n    added_date = Quantity(type=Datetime)\n    formula = Quantity(type=str)\n\n    sample_id = Quantity(type=str)\n\n    def normalize(self, archive, logger):\n        super(Sample, self).normalize(archive, logger)\n\n        if self.sample_id is None:\n            self.sample_id = f'{self.added_date}--{self.formula}'\n\n\nclass SampleDatabase(Schema):\n    samples = SubSection(section=Sample, repeats=True)\n</code></pre> <p>Make sure to call the <code>super</code> implementation properly to support multiple inheritance. In order to control the order by which the <code>normalize</code> calls are executed, one can define <code>normalizer_level</code> which is set to 0 by default. The normalize functions are always called for any sub section before the parent section. However, the order for any sections on the same level will be from low values of <code>normalizer_level</code> to high.</p> <p>If we parse an archive like this:</p> <pre><code>data:\n  m_def: 'examples.archive.custom_schema.SampleDatabase'\n  samples:\n    - formula: NaCl\n      added_date: '2022-06-18'\n</code></pre> <p>we will get a final normalized archive that contains our data like this:</p> <pre><code>{\n  \"data\": {\n    \"m_def\": \"examples.archive.custom_schema.SampleDatabase\",\n    \"samples\": [\n      {\n        \"added_date\": \"2022-06-18T00:00:00+00:00\",\n        \"formula\": \"NaCl\",\n        \"sample_id\": \"2022-06-18 00:00:00+00:00--NaCl\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#migration-guide","title":"Migration guide","text":"<p>By default, schema packages are identified by the full qualified path to the Python module that contains the definitions. An example of a full qualified path could be <code>nomad_example.schema_packages.mypackage</code>, where the first part is the Python package name, second part is a subpackage, and the last part is a Python module containing the definitions. This is the easiest way to prevent conflicts between different schema packages: python package names are unique (prevents clashes between packages) and paths inside a package must point to a single python module (prevents clashes within package). This does, however, mean that if you move your schema definition in the plugin source code, any references to the old definition will break. This becomes problematic in installations that have lot of old data processed with the old definition location, as those entries will still refer to the old location and will not work correctly.</p> <p>As it might not be possible, or even wise to prevent changes in the source code layout, and reprocessing all old entries might be impractical, we do provide an alias mechanism to help with migration tasks. Imagine your schema package was contained in <code>nomad_example.schema_packages.mypackage</code>, and in a newer version of your plugin you want to move it to <code>nomad_example.schema_packages.mynewpackage</code>. The way to do this without completely breaking the old entries is to add an alias in the schema package definition:</p> <pre><code>m_package = SchemaPackage(aliases=['nomad_example.schema_packages.mypackage'])\n</code></pre> <p>Note that this will only help in scenarious where you have moved the definition and not removed or modified any of them.</p>"},{"location":"howto/plugins/schema_packages.html#definitions","title":"Definitions","text":"<p>The following describes in detail the schema language for the NOMAD Metainfo and how it is expressed in Python.</p>"},{"location":"howto/plugins/schema_packages.html#common-attributes-of-metainfo-definitions","title":"Common attributes of Metainfo Definitions","text":"<p>In the example, you have already seen the basic Python interface to the Metainfo. Sections are represented in Python as objects. To define a section, you write a Python class that inherits from <code>MSection</code>. To define subsections and quantities you use Python properties. The definitions themselves are also objects derived from classes. For subsections and quantities, you directly instantiate <code>:class:SubSection</code> and <code>:class:Quantity</code>. For sections there is a generated object derived from <code>:class:Section</code> and available via <code>m_def</code> from each section class and section instance.</p> <p>These Python classes, used to represent metainfo definitions, form an inheritance hierarchy to share common properties</p> <ul> <li><code>name</code>: each definition has a name. This is typically defined by the corresponding   Python property. For example, a section class name becomes the section name; a quantity gets the name   from the variable name used in its Python definition, etc.</li> <li><code>description</code>: each definition should have one. Either set it directly or use doc strings</li> <li><code>links</code>: a list of useful internet references.</li> <li><code>more</code>: a dictionary of custom information. Any additional <code>kwargs</code> set when creating a definition   are added to <code>more</code>.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#sections","title":"Sections","text":"<p>Sections are defined with Python classes that extend <code>MSection</code> (or other section classes).</p> <ul> <li><code>base_sections</code>: automatically taken from the base classes of the Python class.</li> <li><code>extends_base_section</code>: a boolean that determines the inheritance. If this is <code>False</code>,   normal Python inheritance implies and this section will inherit all properties (subsections,   quantities) from all base classes. If <code>True</code>, all definitions in this section   will be added to the properties of the base class section. This allows the extension of existing   sections with additional properties.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#quantities","title":"Quantities","text":"<p>Quantity definitions are the main building block of metainfo schemas. Each quantity represents a single piece of data. Quantities can be defined with the following attributes:</p> <ul> <li><code>type</code>: can be a primitive Python type (<code>str</code>, <code>int</code>, <code>bool</code>), a numpy   data type (<code>np.dtype('float64')</code>), an <code>MEnum('item1', ..., 'itemN')</code>, a predefined   metainfo type (<code>Datetime</code>, <code>JSON</code>, <code>File</code>, ...), or another section or quantity to define   a reference type.</li> <li><code>shape</code>: defines the dimensionality of the quantity. Examples are: <code>[]</code> (number),   <code>['*']</code> (list), <code>[3, 3]</code> (3 by 3 matrix), <code>['n_elements']</code> (a vector of length defined by   another quantity <code>n_elements</code>).</li> <li><code>unit</code>: a physical unit. We use Pint here. You can   use unit strings that are parsed by Pint, e.g. <code>meter</code>, <code>m</code>, <code>m/s^2</code>. As a convention the   NOMAD Metainfo uses only SI units.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#subsection","title":"SubSection","text":"<p>A subsection defines a named property of a section that refers to another section. It allows to define that a section that contains another section.</p> <ul> <li><code>sub_section</code>: (aliases <code>section_def</code>, <code>sub_section_def</code>) defines the section that can   be contained.</li> <li><code>repeats</code>: a boolean that determines whether the subsection relationship allows multiple sections   or only one.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#references-and-proxies","title":"References and Proxies","text":"<p>Besides creating hierarchies with subsections (e.g. tree structures), the metainfo also allows one to create a reference within a section that points to either another section or a quantity value:</p> <pre><code>class Calculation(MSection):\n    system = Quantity(type=System.m_def)\n    atom_labels = Quantity(type=System.atom_labels)\n\ncalc = Calculation()\ncalc.system = run.systems[-1]\ncalc.atom_labels = run.systems[-1]\n</code></pre> <p>To define a reference, define a normal quantity and simply use the section or quantity you want to refer to as type. Then you can assign respective section instances as values.</p> <p>In Python memory, quantity values that reference other sections simply contain a Python reference to the respective section instance. However, upon serializing/storing metainfo data, these references have to be represented differently.</p> <p>Value references work a little differently. When you read a value reference, it behaves like the reference value. Internally, we do not store the values, but instead a reference to the section that holds the referenced quantity is stored. Therefore, when you want to assign a value reference, use the section with the quantity and not the value itself.</p> <p>References are serialized as URLs. There are different types of reference URLs:</p> <ul> <li><code>#/run/0/calculation/1</code>: a reference in the same Archive</li> <li><code>/run/0/calculation/1</code>: a reference in the same archive (legacy version)</li> <li><code>../upload/archive/mainfile/{mainfile}#/run/0</code>: a reference into an Archive of the same upload</li> <li><code>/entries/{entry_id}/archive#/run/0/calculation/1</code>: a reference into the Archive of a different entry on the same NOMAD installation</li> <li><code>/uploads/{upload_id}/archive/{entry_id}#/run/0/calculation/1</code>: similar to the previous one but based on uploads</li> <li><code>https://myoasis.de/api/v1/uploads/{upload_id}/archive/{entry_id}#/run/0/calculation/1</code>: a global reference towards a different NOMAD installation (Oasis)</li> </ul> <p>The host and path parts of URLs correspond with the NOMAD API. The anchors are paths from the root section of an Archive, over its subsections, to the referenced section or quantity value. Each path segment is the name of the subsection or an index in a repeatable subsection: <code>/system/0</code> or <code>/system/0/atom_labels</code>.</p> <p>References are automatically serialized by <code>:py:meth:MSection.m_to_dict</code>. When de-serializing data with <code>:py:meth:MSection.m_from_dict</code> these references are not resolved right away, because the reference section might not yet be available. Instead references are stored as <code>:class:MProxy</code> instances. These objects are automatically replaced by the referenced object when a respective quantity is accessed.</p> <p>If you want to define references, it might not be possible to define the referenced section or quantity beforehand, due to the way Python definitions and imports work. In these cases, you can use a proxy to reference the reference type. There is a special proxy implementation for sections:</p> <pre><code>class Calculation(MSection):\n    system = Quantity(type=SectionProxy('System')\n</code></pre> <p>The strings given to <code>SectionProxy</code> are paths within the available definitions. The above example works, if <code>System</code> is eventually defined in the same package.</p>"},{"location":"howto/plugins/schema_packages.html#categories","title":"Categories","text":"<p>Warning</p> <p>Categories are now deprecated. Their previous occurrences should be replaced with respective annotations.</p> <p>In the old metainfo this was known as abstract types.</p> <p>Categories are defined with Python classes that have <code>:class:MCategory</code> as base class. Their name and description are taken from the name and docstring of the class. An example category looks like this:</p> <pre><code>class CategoryName(MCategory):\n    ''' Category description '''\n    m_def = Category(links=['http://further.explanation.eu'], categories=[ParentCategory])\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#data-frames","title":"Data frames","text":"<p>On top of the core Metainfo concepts like <code>Sections</code>, <code>Quantities</code>, and <code>SubSection</code>, we provide a mechanism for modeling data frames.</p> <p>A NOMAD data frame is a multi-index table with named indices (variables) and columns (fields). All columns should match in length, as they are all parametrized by the same indices. Both variables and fields are defined standalone using Values. A DataFrame may contain any number of Values, though a bare minimum can be defined via the <code>mandatory_variables</code> and <code>mandatory_fields</code> respectively.</p> <p>The mechanism is based on a concept called <code>Values</code> for storing arrays of numeric data to represent a field or variable (or axis, dimension, etc.) and a concept called <code>DataFrame</code> that combines fields and variables with matching dimensions into a data frame. Our <code>DataFrame</code> is conceptually close to xarray datasets, pandas data frames, or the NeXus NXData group.</p> <p><code>Values</code> and <code>DataFrame</code> are usually not used directly, instead you will create re-usable templates that allow you to use the same type of <code>Values</code> (e.g. describing physical properties like energies, temperatures, pressures, ...) and the same type of <code>DataFrame</code> (e.g. describing material properties at different variables like density of states or band gap).</p>"},{"location":"howto/plugins/schema_packages.html#illustrating-example","title":"Illustrating example","text":"<pre><code>Energy = ValuesTemplate(\n    name='Energy',\n    type=np.float64,\n    shape=[],\n    unit='J',\n    iri='https://www.wikidata.org/wiki/Q11379',\n)\n\nTemperature = ValuesTemplate(\n    name='Temperature',\n    type=np.float64,\n    shape=[],\n    unit='K',\n    iri='https://www.wikidata.org/wiki/Q11466',\n)\n\nPressure = ValuesTemplate(\n    name='Pressure',\n    type=np.float64,\n    shape=[],\n    unit='Pa',\n    iri='https://www.wikidata.org/wiki/Q39552',\n)\n\nBandGap = DataFrameTemplate(\n    name='BandGap',\n    mandatory_fields=[Energy],\n)\n\n\nclass MySection(MSection):\n    band_gaps = BandGap()\n\n\nmy_section = MySection()\nmy_section.band_gaps = BandGap.create()\nmy_section.band_gaps.fields = [Energy.create(1.0, 1.1)]\nmy_section.band_gaps.variables = [Temperature.create(200, 220)]\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#fields-vs-variables-and-dimensions","title":"Fields vs variables (and dimensions)","text":"<p>Both fields and variables hold values (i.e. columns) in your data frame. While fields hold the actual data, variables span the data space and its dimensions (i.e. column indices).</p> <p>Variables and dimensions are conceptually slightly different. First, variables provide the values on a certain dimension (via shared indices). Second, the number of Variables often, but not necessarily, are equal to the number of dimensions. If some variables depend on each other, they might span shared dimensions. Fields on the other hand always provide values for all dimensions.</p> <p>Let's compare two datasets; one dataset that you could plot in a heatmap and one that you would plot in a scatter plot. In both cases, we have two variables <code>Temperature</code> and <code>Pressure</code>, as well as one field <code>Energy</code>.</p> <p>In the heatmap scenario, we vary <code>Temperature</code> and <code>Pressure</code> independently and have a <code>Energy</code> value (i.e. heatmap color/intensity) for each <code>Temperature</code> reading at every <code>Pressure</code> reading. For two values on each variable, we respectively we have 4 (2x2) field values:</p> <pre><code>my_section = MySection()\nmy_section.band_gaps = BandGap.create()\nmy_section.band_gaps.fields = [\n    Energy.create(np.array([[1.0, 1.1], [1.3, 1.4], [1.6, 1.7]]))\n]\nmy_section.band_gaps.variables = [\n    Temperature.create(200, 220),\n    Pressure.create(1e5, 1.2e5, 1.4e5),\n]\n</code></pre> <p>In the scatter plot scenario, we vary <code>Temperature</code> and <code>Pressure</code> together. We only have one field value (y-axis) for each pair of temperature and pressure (two x-axes) values. With two combined temperature and pressure readings, we respectively only have two field values:</p> <pre><code>my_section = MySection()\nmy_section.band_gaps = BandGap.create()\nmy_section.band_gaps.fields = [Energy.create(1.0, 1.1, 1.2)]\nmy_section.band_gaps.variables = [\n    Temperature.create(200, 220, 240, spanned_dimensions=[0]),\n    Pressure.create(1e5, 1.2e5, 1.4e5, spanned_dimensions=[0]),\n]\n</code></pre> <p>We can use the <code>ValueTemplate</code> kwarg <code>spanned_dimenions</code> to define how <code>Temperature</code> and <code>Pressure</code> are related. The given indices refer to the indices of the field values and represent the logical dimension of the data space.</p> <p>The first example without the <code>spanned_dimensions</code> is equivalent to this example with <code>spanned_dimensions</code>. Here we span two independent dimensions:</p> <pre><code>my_section = MySection()\nmy_section.band_gaps = BandGap.create()\nmy_section.band_gaps.fields = [\n    Energy.create(np.array([[1.0, 1.1], [1.3, 1.4], [1.6, 1.7]]))\n]\nmy_section.band_gaps.variables = [\n    Temperature.create(200, 220, spanned_dimensions=[0]),\n    Pressure.create(1e5, 1.2e5, 1.4e5, spanned_dimensions=[1]),\n]\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#field-and-variables-in-the-schema-vs-parsing","title":"Field and variables in the schema vs parsing","text":"<p>The templates allow you to define mandatory fields and variables in the schema. These fields and variables have to be provided by the parser when instantiating the respective dataset. However, parser can provide additional fields and variables. This allows to extend what is defined in the template without requiering new definitions.</p>"},{"location":"howto/plugins/schema_packages.html#data-representation","title":"Data representation","text":"<p>Each call to <code>ValueTemplate</code> and <code>DatasetTemplate</code> produces a section definition inheriting from <code>Values</code> and <code>DataFrame</code> respectively.</p> <p><code>Values</code> sections define a single quantity <code>values</code>. The <code>values</code> quantity always holds a numpy array based on the type and shape given in the template. The shape of the <code>values</code> quantity is the shape given in the template plus one dimension of arbitrary length. Variable values are always a flat list of values anyways (the values themselves can have a higher shape). Field values are always flattened. You might provide them in a higher dimensional array according to the dimensionality of the Variables, but they are always flattened as the <code>value</code> quantity only provides one additional dimension, because the real number of dimensions is only available at runtime. The original (runtime) shape of fields is stored int the <code>original_shape</code> <code>Values</code> quantity.</p> <p><code>DataFrame</code> sections define repeating sub-sections for <code>fields</code> and <code>variables</code>. The specific <code>DataFrame</code> section defined by the template, will also hold an annotation <code>DatasetAnnotation</code> that keeps the <code>mandatory_fields</code> and <code>mandatory_variables</code> for runtime validation. The <code>fields</code> and <code>variables</code> sub-sections provide a <code>Values</code> instances for each field in <code>mandatory_fields</code> and each variable in <code>mandatory_variables</code>, but they can also hold additional fields and variables to accommodate more fields and variables determined during parsing.</p> <p>When a <code>ValuesTemplate</code> is used (e.g. <code>some_property = Energy()</code>), a quantity is created. This quantity is a copy of the <code>values</code> quantity created by the template. This allows to reuse templated value quantities. When a <code>DatasetTemplate</code> is used (e.g. <code>some_property = BandGap()</code>), a sub-section is created. This sub-section targets the <code>DataFrame</code> section defined by the template.</p>"},{"location":"howto/plugins/schema_packages.html#working-with-xarrays-and-pandas","title":"Working with xarrays and pandas","text":"<p>We provide utility function on <code>DataFrame</code> that you can use to translate into respective xarray datasets and pandas data frames.</p> <p>Warning</p> <p>The documentation on this is still pending.</p>"},{"location":"howto/plugins/schema_packages.html#adding-python-schemas-to-nomad","title":"Adding Python schemas to NOMAD","text":"<p>The following describes how to integrate new schema modules into the existing code according to best practices.</p>"},{"location":"howto/plugins/schema_packages.html#schema-super-structure","title":"Schema super structure","text":"<p>You should follow the basic developer's getting started to setup a development environment. This will give you all the necessary libraries and allows you to place your modules into the NOMAD code.</p> <p>The <code>EntryArchive</code> section definition sets the root of the archive for each entry in NOMAD. It therefore defines the top level sections:</p> <ul> <li><code>metadata</code>: all \"administrative\" metadata (ids, permissions, publish state, uploads, user metadata, etc.)</li> <li><code>results</code>: a summary with copies and references to data from method specific sections. This also   presents the searchable metadata.</li> <li><code>workflows</code>: all workflow metadata</li> <li><code>data</code>: contains all data from method specific sections by default.</li> <li>Method-specific subsections: e.g. <code>run</code>. This is were all parsers are supposed to add the parsed data.</li> </ul> <p>The main NOMAD Python project includes Metainfo definitions in the following modules:</p> <ul> <li><code>nomad.metainfo</code>: defines the Metainfo itself. This includes a self-referencing schema. E.g. there is a section <code>Section</code>, etc.</li> <li><code>nomad.datamodel</code>: defines the section <code>metadata</code> that contains all \"administrative\"   metadata. It also contains the root section <code>EntryArchive</code>.</li> <li><code>nomad.datamodel.metainfo</code>: defines all the central, method specific (but not parser specific) definitions.   For example the section <code>run</code> with all the simulation definitions (computational material science definitions)   that are shared among the respective parsers.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#extending-existing-sections","title":"Extending existing sections","text":"<p>Parsers can provide their own definitions. By convention, these are placed into a <code>metainfo</code> sub-module of the parser Python module. The definitions here can add properties to existing sections (e.g. from <code>nomad.datamodel.metainfo</code>). By convention, use a <code>x_mycode_</code> prefix. This is done with the <code>extends_base_section</code> Section property. Here is an example:</p> <pre><code>from nomad.metainfo import Section\nfrom nomad.datamodel.metainfo.workflow import Workflow\n\nclass MyCodeRun(Workflow)\n    m_def = Section(extends_base_section=True)\n    x_mycode_execution_mode = Quantity(\n        type=MEnum('hpc', 'parallel', 'single'), description='...')\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#schema-conventions","title":"Schema conventions","text":"<ul> <li>Use lower snake case for section properties; use upper camel case for section definitions.</li> <li>Use a <code>_ref</code> suffix for references.</li> <li>Use subsections rather than inheritance to add specific quantities to a general section.   E.g. the section <code>workflow</code> contains a section <code>geometry_optimization</code> for all geometry optimization specific   workflow quantities.</li> <li>Prefix parser-specific and user-defined definitions with <code>x_name_</code>, where <code>name</code> is the short handle of a code name or other special method prefix.</li> </ul>"},{"location":"howto/plugins/schema_packages.html#use-python-schemas-to-work-with-data","title":"Use Python schemas to work with data","text":""},{"location":"howto/plugins/schema_packages.html#access-structured-data-via-api","title":"Access structured data via API","text":"<p>The API section demonstrates how to access an Archive, i.e. retrieve the processed data from a NOMAD entry. This API will give you JSON data likes this:</p> https://nomad-lab.eu/prod/v1/api/v1/entries/--dLZstNvL_x05wDg2djQmlU_oKn/archive<pre><code>{\n    \"run\": [\n        {\n            \"program\": {...},\n            \"method\": [...],\n            \"system\": [\n                {...},\n                {...},\n                {...},\n                {...},\n                {\n                    \"type\": \"bulk\",\n                    \"configuration_raw_gid\": \"-ZnDK8gT9P3_xtArfKlCrDOt9gba\",\n                    \"is_representative\": true,\n                    \"chemical_composition\": \"KKKGaGaGaGaGaGaGaGaGa\",\n                    \"chemical_composition_hill\": \"Ga9K3\",\n                    \"chemical_composition_reduced\": \"K3Ga9\",\n                    \"atoms\": {...},\n                    \"springer_material\": [...],\n                    \"symmetry\": [...]\n                }\n            ]\n            \"calculation\": [...],\n        }\n    ],\n    \"workflow\": [...],\n    \"metadata\": {...},\n    \"results\":{\n        \"material\": {...},\n        \"method\": {...},\n        \"properties\": {...},\n    }\n}\n</code></pre> <p>This will show you the Archive as a hierarchy of JSON objects (each object is a section), where each key is a property (e.g. a quantity or subsection). Of course you can use this data in this JSON form. You can expect that the same keys (each item has a formal definition) always provides the same type of data. However, not all keys are present in every archive, and not all lists might have the same number of objects. This depends on the data. For example, some runs contain many systems (e.g. geometry optimizations), others don't; typically bulk systems will have symmetry data, non bulk systems might not. To learn what each key means, you need to look up its definition in the Metainfo.</p> <p>You can browse the NOMAD metainfo schema or the archive of each entry (e.g. a VASP example) in the web-interface.</p>"},{"location":"howto/plugins/schema_packages.html#wrap-data-with-python-schema-classes","title":"Wrap data with Python schema classes","text":"<p>In Python, JSON data is typically represented as nested combinations of dictionaries and lists. Of course, you could work with this right away. To make it easier for Python programmers, the NOMAD Python package allows you to use this JSON data with a higher level interface, which provides the following advantages:</p> <ul> <li>code completion in dynamic coding environments like Jupyter notebooks</li> <li>a cleaner syntax that uses attributes instead of dictionary access</li> <li>all higher dimensional numerical data is represented as numpy arrays</li> <li>allows to navigate through references</li> <li>numerical data has a Pint unit attached to it</li> </ul> <p>For each section the Python package contains a Python class that corresponds to its definition in the metainfo. You can use these classes to access <code>json_data</code> downloaded via API:</p> <pre><code>from nomad.datamodel import EntryArchive\n\narchive = EntryArchive.m_from_dict(json_data)\ncalc = archive.run[0].calculation[-1]\ntotal_energy_in_ev = calc.energy.total.value.to(units.eV).m\nformula = calc.system_ref.chemical_formula_reduced\n</code></pre> <p>Archive data can also be serialized into JSON again:</p> <pre><code>import json\n\nprint(json.dumps(calc.m_to_dict(), indent=2))\n</code></pre>"},{"location":"howto/plugins/schema_packages.html#access-structured-data-via-the-nomad-python-package","title":"Access structured data via the NOMAD Python package","text":"<p>The NOMAD Python package provides utilities to query large amounts of archive data. This uses the built-in Python schema classes as an interface to the data.</p>"},{"location":"howto/plugins/schema_packages.html#versioning","title":"Versioning","text":"<p>Eventually you will change a schema. While we might think about schemas before and after a change as version 1 and version 2 of the same schema, those are technically two different schemas. We replace an existing schema for existing data with a new schema. The intention is that the new schema also describes the existing data. But, whether this is always the case depends on the type of changes we make and some changes will \"break\" existing data.</p> <p>What does it mean to \"break an entry\"? Either, your <code>*.archive.[json|yaml]</code> raw files cannot be parsed anymore, or processed data (i.e. \"archive\" data) can not be opened anymore. In both cases, NOMAD tries to convert JSON-style data into section definition instances, validating the data against the current schema. If items in the data do not match a given definitions this process will throw errors.</p> <p>If every potential section instance that followed a section definition in the old schema still follows the same section definition in the new schema, we can safely replace the schema. If not, we will need to migrate data to implement the logical transition we intended. Otherwise, we will break existing data.</p>"},{"location":"howto/plugins/schema_packages.html#save-changes-and-unsafe-changes","title":"Save changes and unsafe changes","text":"<p>Generally, adding to a schema is safe. Adding section definition, adding new quantities, new sub-sections. All existing data only uses definitions that still exist.</p> <p>Removing is generally not safe. Removing a quantity, makes NOMAD ignore values in existing entries. Removing section definitions, \"breaks\" entries.</p> <p>Changing a definition might or might not be safe. In many cases changing is like removing and adding something. Some examples:</p> <ul> <li>Names determine the identity of definitions and name changes are therefore literally removing and adding   a definition. They are not safe, unless you add an <code>alias</code>.</li> <li>Adding <code>base_sections</code> is safe, removing is not. It is as if you add and remove properties.</li> <li>Hoisting a property from a section into a base section is safe (you only add properties).   The reverse is not safe (you remove some properties from some definitions).</li> <li>Making the type of a sub-section more generic (you add properties to the sub-section) is safe,   making it more specific (you remove properties from the sub-section) is not.</li> <li>Changing quantity <code>type</code>, <code>shape</code>, or sub section <code>repeats</code> is not safe.</li> </ul> <p>There are some changes that do not \"break\" entries, but change their semantics. If you change a <code>unit</code> for example. NOMAD will still be able to open entires, but you will interpret the values wrong.</p> <p>Changing annotations will not \"break\" entries, but might have similar effects to your users when old entries might be treated unexpectedly in tools that use these annotations. The GUI is probably the most important \"tool\" here. Changing the ELN annotations for example might prohibit users to manipulate old data in the same way they used to do.</p>"},{"location":"howto/plugins/schema_packages.html#python-module-conventions","title":"Python module conventions","text":"<p>Eventually, you reach the point were \"breaking\" changes are unavoidable and you will need to create a new major version of your schema.</p> <p>First, you need to keep a schema version as long as there is data following that schema. That means two versions have to exist at the same time. Each section instance refers to a schema by its qualified python name that includes the respective python module with the schema package.</p> <p>Therefore, breaking changes should only be introduced in new schema packages. As a convention:</p> <ul> <li>schema packages cary their major version in the name, e.g. <code>nomad_example.schema_packages.my_package_v2</code></li> <li>schema packages are maintained in version sub-modules, e.g. <code>nomad_example.schema_packages.my_package.v2</code></li> </ul> <p>Note</p> <p>By convention and assuming the inevitability of breaking changes, you should already start with a <code>v1</code> module when developing a new schema package.</p> <p>Note that only the major version should result in separate python modules, i.e. schema packages. Minor and patch versions should not introduce breaking changes by definition and the module for the respective major version can be changed safely.</p>"},{"location":"howto/plugins/schema_packages.html#migration-strategies","title":"Migration strategies","text":"<p>Warning</p> <p>This is preliminary information.</p> <p>What are strategies to introduce \"breaking\" changes and migrate existing data to the new version of the schema.</p>"},{"location":"howto/plugins/schema_packages.html#schema-package-aliases","title":"Schema package aliases","text":"<p>In scenarious where you have moved the definition to a new module and did not create other breaking changes you can use schema package aliases.</p> <p>By default, schema packages are identified by the full qualified path to the Python module that contains the definitions. An example of a full qualified path could be <code>nomad_example.schema_packages.mypackage</code>, where the first part is the Python package name, second part is a subpackage, and the last part is a Python module containing the definitions. This is the easiest way to prevent conflicts between different schema packages: python package names are unique (prevents clashes between packages) and paths inside a package must point to a single python module (prevents clashes within package). This does, however, mean that if you move your schema definition to a new (version) module, any references to the old definition will break. To move the same schema to a new module, e.g. <code>nomad_example.schema_packages.mypackage.v2</code>, and still let existing entries use it with the old module name, you can use a schema package alias:</p> <p>You schema package definition:</p> <pre><code>m_package = SchemaPackage(aliases=['nomad_example.schema_packages.mypackage'])\n</code></pre> <p>Note</p> <p>This can also be used, if you need to move a schema package for other reasons, e.g. to move schemas from one plugin to another.</p>"},{"location":"howto/plugins/schema_packages.html#offer-migration-functionality","title":"Offer migration functionality","text":"<p>Warning</p> <p>This appoach is still tested and we might provide more dedicated functionality in the future.</p> <p>You will not want to maintain the old version indefinitely. Eventually, you deprecate and even remove schema package versions from future plugin releases. Here, you should offer functionality that lets users migrate their old data following the old version to data following the new version.</p>"},{"location":"howto/plugins/schema_packages.html#processed-data","title":"Processed data","text":"<p>In cases where your schema is instantiated via parser, processed data (i.e. entry \"archives\"), can be migrated by re-processing respective uploads with a new version of the parser following the new schema. Users \"just\" have to be made aware and perform the reprocessing.</p>"},{"location":"howto/plugins/schema_packages.html#raw-files","title":"Raw files","text":"<p>In cases of <code>*.archive.json</code> raw files (i.e. you use NOMAD ELNs), the raw files themselves have to change. This is harder to achieve as mainfile immutability is baked into NOMAD. To apply the same parser-based strategy (see section before), you can add a normalize function to the <code>EntryData</code> section definition(s) of the old schema version. This normalize function</p> <ul> <li>performs a transformation, i.e. creates and instance of the new   version from the given instance of the old version</li> <li>writes the <code>m_to_dict</code> of the new version back into the mainfile</li> <li>replaces the <code>data</code> section (old version instance) with the transformed (instance of the new version)</li> </ul> <p>Here is some pseudo code:</p> <pre><code>from ..v2 import MyData as MyDataV2\n\ndef normalize(self, archive, logger):\n  transformed = MyDataV2()\n\n  # code that fills transformed from self\n\n  with archive.context.raw_file(archive.metadata.mainfile, 'wt') as f:\n    f.write(json.dumps(dict(data=transformed.m_to_dict())))\n\n  archive.data = transformed\n</code></pre> <p>With such a <code>normalize</code> function in place, you can apply the re-processing strategy to migrate.</p> <p>Warning</p> <p>There might be issues with this approach depending on how the <code>normalize</code> functions are executed. This only works well, if after replacing data the <code>normalize</code> functions of the transformed instance are called and no <code>normalize</code> functions of the original instance have been called yet.</p> <p>Also this approach has risks. If the process fails in unexpected ways there might be raw file data being lost.</p>"},{"location":"howto/plugins/schema_packages.html#schema-packages-developed-by-fairmat","title":"Schema packages developed by FAIRmat","text":"<p>The following is a list of plugins containing schema packages developed by FAIRmat:</p> Description Project url simulation run https://github.com/nomad-coe/nomad-schema-plugin-run.git simulation data https://github.com/nomad-coe/nomad-schema-plugin-simulation-data.git simulation workflow https://github.com/nomad-coe/nomad-schema-plugin-simulation-workflow.git NEXUS https://github.com/FAIRmat-NFDI/pynxtools.git synthesis https://github.com/FAIRmat-NFDI/AreaA-data_modeling_and_schemas.git material processing https://github.com/FAIRmat-NFDI/nomad-material-processing.git measurements https://github.com/FAIRmat-NFDI/nomad-measurements.git catalysis https://github.com/FAIRmat-NFDI/nomad-catalysis-plugin.git"},{"location":"howto/programmatic/api.html","title":"How to use the API","text":"<p>This guide is about using NOMAD's REST APIs directly, e.g. via Python's <code>requests</code> library.</p> <p>To access the processed data with our client library <code>nomad-lab</code> follow How to access processed data. You can also watch our video tutorial on the API.</p>"},{"location":"howto/programmatic/api.html#different-options-to-use-the-api","title":"Different options to use the API","text":"<p>NOMAD offers all its functionality through application programming interfaces (APIs). More specifically RESTful HTTP APIs that allows you to use NOMAD as a set of resources (think data) that can be uploaded, accessed, downloaded, searched for, etc. via HTTP requests.</p> <p>You can get an overview on all NOMAD APIs on the API page. We will focus here on NOMAD's main API (v1). In fact, this API is also used by the web interface and should provide everything you need.</p> <p>There are different tools and libraries to use the NOMAD API that come with different trade-offs between expressiveness, learning curve, and convenience.</p>"},{"location":"howto/programmatic/api.html#you-can-use-your-browser","title":"You can use your browser","text":"<p>For example to see the metadata for all entries with elements Ti and O go here: http://localhost:8000/fairdi/nomad/latest/api/v1/entries?elements=Ti&amp;elements=O</p>"},{"location":"howto/programmatic/api.html#use-curl-or-wget","title":"Use <code>curl</code> or <code>wget</code>","text":"<p>REST API's use resources located via URLs. You access URLs with <code>curl</code> or <code>wget</code>. Same Ti, O example as before:</p> <pre><code>curl \"http://localhost:8000/fairdi/nomad/latest/api/v1/entries?results.material.elements=Ti&amp;results.material.elements=O\" | python -m json.tool\n</code></pre>"},{"location":"howto/programmatic/api.html#use-python-and-requests","title":"Use Python and <code>requests</code>","text":"<p><code>Requests</code> is a popular Python library to use the internet's HTTP protocol that is used to communicate with REST APIs. Install with <code>pip install requests</code>. See Using <code>requests</code>.</p>"},{"location":"howto/programmatic/api.html#use-our-dashboard","title":"Use our dashboard","text":"<p>The NOMAD API has an OpenAPI dashboard. This is an interactive documentation of all API functions that allows you to try these functions in the browser.</p>"},{"location":"howto/programmatic/api.html#use-nomads-python-package","title":"Use NOMAD's Python package","text":"<p>Install the NOMAD Python client library and use it's <code>ArchiveQuery</code> functionality for a more convenient query based access of archive data following the How-to access the processed data guide.</p>"},{"location":"howto/programmatic/api.html#using-requests","title":"Using <code>requests</code>","text":"<p>If you are comfortable with REST APIs and using Pythons <code>requests</code> library, this example demonstrates the basic concepts of NOMAD's main API. You can get more documentation and details on all functions from the API dashboard.</p> <p>The following issues a search query for all entries that have both Ti and O among the elements of their respective materials. It restricts the results to one entry and only returns the <code>entry_id</code>.</p> <pre><code>import requests\nimport json\n\nbase_url = 'http://nomad-lab.eu/prod/v1/api/v1'\n\nresponse = requests.post(\n    f'{base_url}/entries/query',\n    json={\n        'query': {\n            'results.material.elements': {\n                'all': ['Ti', 'O']\n            }\n        },\n        'pagination': {\n            'page_size': 1\n        },\n        'required': {\n            'include': ['entry_id']\n        }\n    })\nresponse_json = response.json()\nprint(json.dumps(response.json(), indent=2))\n</code></pre> <p>This will give you something like this: <pre><code>{\n  \"owner\": \"public\",\n  \"query\": {\n    \"name\": \"results.material.elements\",\n    \"value\": {\n      \"all\": [\n        \"Ti\",\n        \"O\"\n      ]\n    }\n  },\n  \"pagination\": {\n    \"page_size\": 1,\n    \"order_by\": \"entry_id\",\n    \"order\": \"asc\",\n    \"total\": 17957,\n    \"next_page_after_value\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\"\n  },\n  \"required\": {\n    \"include\": [\n      \"entry_id\"\n    ]\n  },\n  \"data\": [\n    {\n      \"entry_id\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\"\n    }\n  ]\n}\n</code></pre></p> <p>The <code>entry_id</code> is a unique identifier for, well, entries. You can use it to access other entry data. For example, you want to access the entry's archive. More precisely, you want to gather the formula and energies from the main workflow result. The following requests the archive based on the <code>entry_id</code> and only requires some archive sections.</p> <pre><code>first_entry_id = response_json['data'][0]['entry_id']\nresponse = requests.post(\n    f'{base_url}/entries/{first_entry_id}/archive/query',\n    json={\n        'required': {\n            'workflow': {\n                'calculation_result_ref': {\n                    'energy': '*',\n                    'system_ref': {\n                        'chemical_composition': '*'\n                    }\n                }\n            }\n        }\n    })\nresponse_json = response.json()\nprint(json.dumps(response_json, indent=2))\n</code></pre> <p>The result will look like this: <pre><code>{\n  \"required\": {\n    \"workflow\": {\n      \"calculation_result_ref\": {\n        \"energy\": \"*\",\n        \"system_ref\": {\n          \"chemical_composition\": \"*\"\n        }\n      }\n    }\n  },\n  \"entry_id\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\",\n  \"data\": {\n    \"entry_id\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\",\n    \"upload_id\": \"YXUIZpw5RJyV3LAsFI2MmQ\",\n    \"parser_name\": \"parsers/fhi-aims\",\n    \"archive\": {\n      \"run\": [\n        {\n          \"system\": [\n            {\n              \"chemical_composition\": \"OOSrTiOOOSrTiOOOSrTiOFF\"\n            }\n          ],\n          \"calculation\": [\n            {\n              \"energy\": {\n                \"fermi\": -1.1363378335891879e-18,\n                \"total\": {\n                  \"value\": -5.697771591896252e-14\n                },\n                \"correlation\": {\n                  \"value\": -5.070133798617076e-17\n                },\n                \"exchange\": {\n                  \"value\": -2.3099755059272454e-15\n                },\n                \"xc\": {\n                  \"value\": -2.360676843913416e-15\n                },\n                \"xc_potential\": {\n                  \"value\": 3.063766944960246e-15\n                },\n                \"free\": {\n                  \"value\": -5.697771595558439e-14\n                },\n                \"sum_eigenvalues\": {\n                  \"value\": -3.3841806795825544e-14\n                },\n                \"total_t0\": {\n                  \"value\": -5.697771593727346e-14\n                },\n                \"correction_entropy\": {\n                  \"value\": -1.8310927833270112e-23\n                },\n                \"correction_hartree\": {\n                  \"value\": -4.363790430157292e-17\n                },\n                \"correction_xc\": {\n                  \"value\": -2.3606768439090564e-15\n                }\n              },\n              \"system_ref\": \"/run/0/system/0\"\n            }\n          ]\n        }\n      ],\n      \"workflow\": [\n        {\n          \"calculation_result_ref\": \"/run/0/calculation/0\"\n        }\n      ]\n    }\n  }\n}\n</code></pre></p> <p>You can work with the results in the given JSON (or respective Python dict/list) data already. If you have NOMAD's Python library installed, you can take the archive data and use the Python interface. The Python interface will help with code-completion (e.g. in notebook environments), resolve archive references (e.g. from workflow to calculation to system), and allow unit conversion:</p> <pre><code>from nomad.datamodel import EntryArchive\nfrom nomad.metainfo import units\n\narchive = EntryArchive.m_from_dict(response_json['data']['archive'])\nresult = archive.workflow[0].calculation_result_ref\nprint(result.system_ref.chemical_composition)\nprint(result.energy.total.value.to(units('eV')))\n</code></pre> <p>This will give you an output like this:</p> <pre><code>OOSrTiOOOSrTiOOOSrTiOFF\n-355626.93095025205 electron_volt\n</code></pre>"},{"location":"howto/programmatic/api.html#different-kinds-of-data","title":"Different kinds of data","text":"<p>We distinguish between different kinds of NOMAD data and there are different functions in the API:</p> <ul> <li>Entry metadata, a summary of extracted data for an entry.</li> <li>Raw files, the files as they were uploaded to NOMAD.</li> <li>Archive data, all of the extracted data for an entry.</li> </ul> <p>There are also different entities (see also Datamodel) with different functions in the API:</p> <ul> <li>Entries</li> <li>Uploads</li> <li>Datasets</li> <li>Users</li> </ul> <p>The API URLs typically start with the entity, followed by the kind of data. Examples are:</p> <ul> <li><code>entries/query</code> - Query entries for metadata</li> <li><code>entries/archive/query</code> - Query entries for archive data</li> <li><code>entries/{entry-id}/raw</code> - Download raw data for a specific entry</li> <li><code>uploads/{upload-id}/raw/path/to/file</code> - Download a specific file of an upload</li> </ul>"},{"location":"howto/programmatic/api.html#common-concepts","title":"Common concepts","text":"<p>The Using <code>requests</code> above, showed how to execute a basic search. This includes some fundamental concepts that can be applied to many parts of the API. Let's discuss some of the common concepts.</p>"},{"location":"howto/programmatic/api.html#response-layout","title":"Response layout","text":"<p>Functions that have a JSON response, will have a common layout. First, the response will contain all keys and values of the request. The request is not repeated verbatim, but in a normalized form. Abbreviations in search queries might be expanded, default values for optional parameters are added, or additional response specific information is included. Second, the response will contain the results under the key <code>data</code>.</p>"},{"location":"howto/programmatic/api.html#owner","title":"Owner","text":"<p>All functions that allow a query will also allow to specify the <code>owner</code>. Depending on the API function, its default value will be mostly <code>visible</code>. Some values are only available if you are logged in.</p> <p>The <code>owner</code> allows to limit the scope of the search based on entry ownership. This is useful if you only want to search among all publicly downloadable entries or only among your own entries, etc.</p> <p>These are the possible owner values and their meaning:</p> <ul> <li><code>admin</code>: No restriction. Only usable by an admin user.</li> <li><code>all</code>: Published entries (with or without embargo), or entries that belong to you     or are shared with you.</li> <li><code>public</code>: Published entries without embargo.</li> <li><code>shared</code>: Entries that belong to you or are shared with you.</li> <li><code>staging</code>: Unpublished entries that belong to you or are shared with you.</li> <li><code>user</code>: Entries that belong to you.</li> <li><code>visible</code>: Published entries without embargo, or unpublished entries that belong to     you or are shared with you.</li> </ul>"},{"location":"howto/programmatic/api.html#queries","title":"Queries","text":"<p>A query can be a very simple list of parameters. Different parameters or values of the same parameter are combined with a logical and. The following query would search for all entries that are VASP calculations, contain Na and Cl, and are authored by Stefano Curtarolo and Chris Wolverton. <pre><code>{\n    \"results.material.elements\": [\"Na\", \"Cl\"],\n    \"results.method.simulation.program_name\": \"VASP\",\n    \"authors\": [\"Stefano Curtarolo\", \"Chris Wolverton\"]\n}\n</code></pre></p> <p>A short cut to change the logical combination of values in a list, is to add a suffix to the quantity <code>:any</code>: <pre><code>{\n    \"results.material.elements\": [\"Na\", \"Cl\"],\n    \"results.method.simulation.program_name\": \"VASP\",\n    \"authors:any\": [\"Stefano Curtarolo\", \"Chris Wolverton\"]\n}\n</code></pre></p> <p>Otherwise, you can also write complex logical combinations of parameters like this: <pre><code>{\n    \"and\": [\n        {\n            \"or\": [\n                {\n                    \"results.material.elements\": [\"Cl\", \"Na\"]\n                },\n                {\n                    \"results.material.elements\": [\"H\", \"O\"]\n                }\n            ]\n        },\n        {\n            \"not\": {\n                \"results.material.symmetry.crystal_system\": \"cubic\"\n            }\n        }\n    ]\n}\n</code></pre> Other short-cut prefixes are <code>none:</code> and <code>any:</code> (the default).</p> <p>By default all quantity values have to equal the given values to match. For some values you can also use comparison operators like this: <pre><code>{\n    \"upload_create_time\": {\n        \"gt\": \"2020-01-01\",\n        \"lt\": \"2020-08-01\"\n    },\n    \"results.properties.geometry_optimization.final_energy_difference\": {\n        \"lte\": 1.23e-18\n    }\n}\n</code></pre></p> <p>or shorter with suffixes: <pre><code>{\n    \"upload_create_time:gt\": \"2020-01-01\",\n    \"upload_create_time:lt\": \"2020-08-01\",\n    \"results.properties.geometry_optimization.final_energy_difference:lte\": 1.23e-18\n}\n</code></pre></p> <p>The searchable quantities are a subset of the NOMAD Archive quantities defined in the NOMAD Metainfo. The searchable quantities also depend on the API endpoint.</p> <p>There is also an additional query parameter that you can use to formulate queries based on the optimade filter language: <pre><code>{\n    \"optimade_filter\": \"nelements &gt;= 2 AND elements HAS ALL 'Ti', 'O'\"\n}\n</code></pre></p>"},{"location":"howto/programmatic/api.html#pagination","title":"Pagination","text":"<p>When you issue a query, usually not all results can be returned. Instead, an API returns only one page. This behavior is controlled through pagination parameters, like <code>page_site</code>, <code>page</code>, <code>page_offset</code>, or <code>page_after_value</code>.</p> <p>Let's consider a search for entries as an example.</p> <pre><code>response = requests.post(\n    f'{base_url}/entries/query',\n    json={\n        'query': {\n            'results.material.elements': {\n                'all': ['Ti', 'O']\n            }\n        },\n        'pagination': {\n            'page_size': 10\n        }\n    }\n)\n</code></pre> <p>This will only result in a response with a maximum of 10 entries. The response will contain a <code>pagination</code> object like this:</p> <pre><code>{\n    \"page_size\": 10,\n    \"order_by\": \"entry_id\",\n    \"order\": \"asc\",\n    \"total\": 17957,\n    \"next_page_after_value\": \"--SZVYOxA2jTu_L-mSxefSQFmeyF\"\n}\n</code></pre> <p>In this case, the pagination is based on after values. This means that the search can be continued with a follow up request at a certain point characterized by the <code>next_page_after_value</code>. If you follow up with:</p> <pre><code>response = requests.post(\n    f'{base_url}/entries/query',\n    json={\n        'query': {\n            'results.material.elements': {\n                'all': ['Ti', 'O']\n            }\n        },\n        'pagination': {\n            'page_size': 10,\n            'page_after_value': '--SZVYOxA2jTu_L-mSxefSQFmeyF'\n        }\n    }\n)\n</code></pre> <p>You will get the next 10 results.</p> <p>Here is a full example that collects the first 100 formulas from entries that match a certain query by paginating.</p> <pre><code>import requests\n\nbase_url = 'http://nomad-lab.eu/prod/v1/api/v1'\njson_body = {\n    'query': {\n        'results.material.elements': {\n            'all': ['Ti', 'O']\n        }\n    },\n    'pagination': {\n        'page_size': 10\n    },\n    'required': {\n        'include': ['results.material.chemical_formula_hill']\n    }\n}\n\nformulas = set()\n\nwhile len(formulas) &lt; 100:\n    response = requests.post(f'{base_url}/entries/query', json=json_body)\n    response_json = response.json()\n\n    for data in response_json['data']:\n        formulas.add(data['results']['material']['chemical_formula_hill'])\n\n    next_value = response_json['pagination'].get('next_page_after_value')\n    if not next_value:\n        break\n    json_body['pagination']['page_after_value'] = next_value\n\nprint(formulas)\n</code></pre>"},{"location":"howto/programmatic/api.html#authentication","title":"Authentication","text":"<p>Most of the API operations do not require any authorization and can be freely used without a user or credentials. However, to upload, edit, or view your own and potentially unpublished data, the API needs to authenticate you.</p> <p>The NOMAD API uses OAuth and tokens to authenticate users. We provide simple operations that allow you to acquire an access token via username and password:</p> <pre><code>import os\n\nimport requests\n\nresponse = requests.post(\n    'http://localhost:8000/fairdi/nomad/latest/api/v1/auth/token',\n    data={'username': os.getenv('NOMAD_USERNAME'), 'password': os.getenv('NOMAD_PASSWORD')},\n)\ntoken = response.json()['access_token']\n\nresponse = requests.get(\n    'http://localhost:8000/fairdi/nomad/latest/api/v1/uploads',\n    headers={'Authorization': f'Bearer {token}'})\nuploads = response.json()['data']\n</code></pre> <p>If you have the NOMAD Python package installed. You can use its <code>Auth</code> implementation:</p> <pre><code>import os\n\nimport requests\nfrom nomad.client import Auth\n\nresponse = requests.get(\n    'http://localhost:8000/fairdi/nomad/latest/api/v1/uploads',\n    auth=Auth(user=os.getenv('NOMAD_USERNAME'), password=os.getenv('NOMAD_PASSWORD')))\nuploads = response.json()['data']\n</code></pre> <p>To use authentication in the dashboard, simply use the Authorize button. The dashboard GUI will manage the access token and use it while you try out the various operations.</p>"},{"location":"howto/programmatic/api.html#app-token","title":"App token","text":"<p>If the short-term expiration of the default access token does not suit your needs, you can request an app token with a user-defined expiration. For example, you can send the GET request <code>/auth/app_token?expires_in=86400</code> together with some way of authentication, e.g. header <code>Authorization: Bearer &lt;access token&gt;</code>. The API will return an app token, which is valid for 24 hours in subsequent request headers with the format <code>Authorization: Bearer &lt;app token&gt;</code>. The request will be declined if the expiration is larger than the maximum expiration defined by the API config.</p> <p>Warning</p> <p>Despite the name, the app token is used to impersonate the user who requested it. It does not discern between different uses and will only become invalid once it expires (or when the API's secret is changed).</p>"},{"location":"howto/programmatic/api.html#search-for-entries","title":"Search for entries","text":"<p>See Using <code>requests</code> for a typical search example. Combine the different concepts above to create the queries that you need.</p> <p>Searching for entries is typically just an initial step. Once you know what entries exist you'll probably want to do one of the following things.</p>"},{"location":"howto/programmatic/api.html#download-raw-files","title":"Download raw files","text":"<p>You can use queries to download raw files, but typically you don't want to download file-by-file or entry-by-entry. Therefore, we allow to download a large set of files in one big zip-file. Here, you might want to use a program like curl to download directly from the shell:</p> <pre><code>curl \"http://localhost:8000/fairdi/nomad/latest/api/v1/entries/raw?results.material.elements=Ti&amp;results.material.elements=O\" -o download.zip\n</code></pre>"},{"location":"howto/programmatic/api.html#access-processed-data-archives","title":"Access processed data (archives)","text":"<p>Above under Using <code>requests</code>, you've already learned how to access archive data. A special feature of the archive API functions is that you can define what is <code>required</code> from the archives.</p> <pre><code>response = requests.post(\n    f'{base_url}/entries/archive/query',\n    json={\n        'query': ...,\n        'pagination': ...,\n        'required': {\n            'workflow': {\n                'calculation_result_ref': {\n                    'energy': '*',\n                    'system_ref': {\n                        'chemical_composition': '*'\n                    }\n                }\n            }\n        }\n    })\n</code></pre> <p>The <code>required</code> part allows you to specify what parts of the requested archives should be returned. The NOMAD Archive is a hierarchical data format and you can require certain branches (i.e. sections) in the hierarchy. By specifying certain sections with specific contents or all contents (via the directive <code>\"*\"</code>), you can determine what sections and what quantities should be returned. The default is the whole archive, i.e., <code>\"*\"</code>.</p> <p>For example to specify that you are only interested in the <code>metadata</code> use:</p> <pre><code>{\n    \"metadata\": \"*\"\n}\n</code></pre> <p>Or to only get the <code>energy_total</code> from each individual entry, use: <pre><code>{\n    \"run\": {\n        \"configuration\": {\n            \"energy\": \"*\"\n        }\n    }\n}\n</code></pre></p> <p>You can also request certain parts of a list, e.g. the last calculation: <pre><code>{\n    \"run\": {\n        \"calculation[-1]\": \"*\"\n    }\n}\n</code></pre></p> <p>These required specifications are also very useful to get workflow results. This works because we can use references (e.g. workflow to final result calculation) and the API will resolve these references and return the respective data. For example just the total energy value and reduced formula from the resulting calculation: <pre><code>{\n    \"workflow\": {\n        \"calculation_result_ref\": {\n            \"energy\": \"*\",\n            \"system_ref\": {\n                \"value\": {\n                    \"chemical_composition\": \"*\"\n                }\n            }\n        }\n    }\n}\n</code></pre></p> <p>You can also resolve all references in a branch with the <code>include-resolved</code> directive. This will resolve all references in the branch, and also all references in referenced sections: <pre><code>{\n    \"workflow\":\n        \"calculation_result_ref\": \"include-resolved\"\n    }\n}\n</code></pre></p> <p>By default, the targets of \"resolved\" references are added to the archive at their original hierarchy positions. This means, all references are still references, but they are resolvable within the returned data, since they targets are now part of the data. Another option is to add <code>\"resolve-inplace\": true</code> to the root of required. Here, the reference targets will replace the references: <pre><code>{\n    \"resolve-inplace\": true,\n    \"workflow\":\n        \"calculation_result_ref\": \"include-resolved\"\n    }\n}\n</code></pre></p> <p>You can browse the NOMAD metainfo schema or the archive of each entry (e.g. a VASP example) in the web-interface.</p>"},{"location":"howto/programmatic/api.html#limits","title":"Limits","text":"<p>The API allows you to send many requests in parallel and to put a lot of load on NOMAD servers. Since this can accidentally or deliberately reduce the service quality for other, we have to enforce a few limits.</p> <ul> <li>concurrency limit: you can only run a certain amount of requests at the same time</li> <li>rate limit: you can only run a certain amount of requests per second</li> <li>API limit: many API endpoints will enforce a maximum page size</li> </ul> <p>If you get responses with an HTTP code 503 Service Unavailable, you are hitting a rate limit and you cannot use the service until you fall back into our limits. Consider, to send fewer requests in a larger time frame.</p> <p>Rate limits are enforced based on your IP address. Please note that when you or your colleagues are sharing a single external IPs from within a local network, e.g. via NAT, you are also sharing the rate limits. Depending on the NOMAD installation, these limits can be as low as 30 requests per second or 10 concurrent requests.</p> <p>Consider to use endpoints that allow you to retrieve full pages of resources, instead of endpoints that force you to access resources one at a time. See also the sections on types of data and pagination.</p> <p>However, pagination also has its limits and you might ask for pages that are too large. If you get responses in the 400 range, e.g. 422 Unprocessable Content or 400 Bad request, you might hit an API limit. Those responses are typically accompanied by an error message in the response body that will inform you about the limit, e.g. the maximum allowed page size.</p>"},{"location":"howto/programmatic/api.html#user-groups","title":"User Groups","text":"<p>You can create a user group and add other users to that group via the API. These groups can be used as upload members to make it easier to give viewing or editing rights for an upload to multiple users at once, e.g. a working group.</p> <p>To create a group, send an authenticated POST request to <code>/groups</code> including the <code>group_name</code> and a list of user IDs in the <code>members</code> field:</p> <pre><code>// POST /groups\n{\n  \"group_name\": \"The Three Musketeers\",\n  \"members\": [\n    \"00000000-0000-0000-0000-004174686f73\",\n    \"00000000-0000-0000-0050-6f7274686f73\",\n    \"00000000-0000-0000-0000-4172616d6973\"\n  ]\n}\n</code></pre> <p>Editing is similar but at another endpoint. The <code>members</code> field accepts for convenience <code>add</code> and <code>remove</code> keys followed by a string or a list of strings, so you don't have to repeat all members (otherwise use <code>set</code> or just the full list):</p> <pre><code>// POST /groups/{group-id}/edit\n{\n  \"members\": {\n    \"add\": \"00000000-64e2-8099-4172-7461676e616e\"\n  }\n}\n</code></pre> <p>See the OpenAPI dashboard for a concise summary of the endpoints.</p>"},{"location":"howto/programmatic/archive_query.html","title":"How to access processed data","text":"<p>The <code>ArchiveQuery</code> allows you to search for entries and access their parsed and processed archive data at the same time. Furthermore, all data is accessible through a convenient Python interface based on the schema rather than plain JSON. See also this guide on using NOMAD's Python schemas to work with processed data.</p> <p>As a requirement, you have to install the <code>nomad-lab</code> Python package. Follow the How to install nomad-lab guide.</p>"},{"location":"howto/programmatic/archive_query.html#getting-started","title":"Getting started","text":"<p>To define a query, one can, for example, write</p> <pre><code>from nomad.client.archive import ArchiveQuery\n\nquery = ArchiveQuery(query={}, required='*', page_size=10, results_max=10000)\n</code></pre> <p>Although the above query object has an empty query.</p> <p>The query object is constructed only. To access the desired data, users need to perform two operations manually. Two interfaces that can be used in different environments are provided.</p>"},{"location":"howto/programmatic/archive_query.html#synchronous-interface","title":"Synchronous Interface","text":""},{"location":"howto/programmatic/archive_query.html#fetch","title":"Fetch","text":"<p>The fetch process is carried out synchronously. Users can call the following to fetch up to <code>results_max</code> entries.</p> <pre><code># number_of_entries = query.fetch(1000) # fetch 1000 entries\nnumber_of_entries = query.fetch()  # fetch at most results_max entries\n</code></pre> <p>An indicative number <code>n</code> can be provided <code>fetch(n)</code> to fetch <code>n</code> entries at once. The fetch process may submit multiple requests to the server, each request asks for <code>page_size</code> entries. The number of qualified entries will be returned. Meanwhile, the qualified entry list would be populated with their IDs. To check all qualified upload IDs, one can call <code>entry_list()</code> method to return the full list.</p> <pre><code>print(query.entry_list())\n</code></pre> <p>If applicable, it is possible to fetch a large number of entries first and then perform a second fetch by using some entry ID in the first fetch result as the <code>after</code> argument so that some middle segment can be downloaded.</p>"},{"location":"howto/programmatic/archive_query.html#download","title":"Download","text":"<p>After fetching the qualified entries, the desired data can be downloaded asynchronously. One can call</p> <pre><code># results = query.download(1000) # download 1000 entries\nresults = query.download()  # download all fetched entries if fetched otherwise fetch and download up to `results_max` entries\n</code></pre> <p>to download up to <code>results_max</code> entries. The downloaded results are returned as a list. Alternatively, it is possible to just download a portion of previously fetched entries at a single time. For example,</p> <pre><code># previously fetched for example 1000 entries\n# but only download the first 100 (approx.) entries\nresults = query.download(100)\n</code></pre> <p>The same <code>download(n)</code> method can be called repeatedly. If there are no sufficient entries, new entries will be automatically fetched. If there are no more entries, the returned result list is empty. For example,</p> <pre><code>total_results = []\nwhile True:\n    result = query.download(100)\n    if len(result) == 0:\n        break\n    total_results.extend(result)\n</code></pre> <p>There is no retry mechanism in the download process. If any entries fail to be downloaded due to server error, it is kept in the list otherwise removed.</p>"},{"location":"howto/programmatic/archive_query.html#pandas-dataframe","title":"Pandas Dataframe","text":"<p>You can also convert the downloaded results to pandas dataframe directly by calling <code>entries_to_dataframe</code> method on the <code>query</code> object. In order to filter the final dataframe to contain only specific keys/column_names, you can use the option <code>keys_to_filter</code> with a list of relevant keys. For example:</p> <pre><code>results = query.entries_to_dataframe(keys_to_filter=[])\n</code></pre> <p>The option <code>from_query</code> can be used to control the formatting of the dataframe(s). By setting this option to <code>False</code>, all entries with their entire contents are flattened and returned in one single (and potentially huge) dataframe, and by setting it to <code>True</code>, it returns a python dictionary with each key denoting a separate distinct nested path in the <code>required</code> and each value specifying the corresponding dataframe.</p>"},{"location":"howto/programmatic/archive_query.html#asynchronous-interface","title":"Asynchronous Interface","text":"<p>Some applications, such as Jupyter Notebook, may run a global/top level event loop. To query data in those environments, one can use the asynchronous interface.</p> <pre><code>number_of_entries = await query.async_fetch()  # indicative number n applies: async_fetch(n)\nresults = await query.async_download()  # indicative number n applies: async_download(n)\n</code></pre> <p>Alternatively, if one wants to use the asynchronous interface, it is necessary to patch the global event loop to allow nested loops.</p> <p>To do so, one can add the following at the beginning of the notebook.</p> <pre><code>import nest_asyncio\n\nnest_asyncio.apply()\n</code></pre>"},{"location":"howto/programmatic/archive_query.html#a-complete-rundown","title":"A Complete Rundown","text":"<p>Here we show a valid query and acquire data from server.</p> <p>We first define the desired query and construct the object. We limit the maximum number of entries to be 10000 and 10 entries per page.</p> <pre><code>from nomad.client.archive import ArchiveQuery\n\nrequired = {\n    'workflow': {\n        'calculation_result_ref': {\n            'energy': '*',\n            'system_ref': {\n                'chemical_composition_reduced': '*'\n            }\n        }\n    }\n}\n\nquery = {\n    'results.method.simulation.program_name': 'VASP',\n    'results.material.elements': ['Ti']\n}\n\nquery = ArchiveQuery(query=query, required=required, page_size=10, results_max=10000)\n</code></pre> <p>Let's fetch some entries.</p> <pre><code>query.fetch(10)\nprint(query.entry_list())\n</code></pre> <p>If we print the entry list, it would be</p> <pre><code>[('---CU_ejqV7yjEFteUAH0rG0SKIS', 'aimE2ajMQnOKruRbQjzpCA'), ('---Dz9vL-eyErWEk7-1tX4zLVmwo', 'Vyb6k1OTRSuyXfvsRk4CPQ'), ('---pRcX7NG_XDx_4ufUaeEnZnmrO', 'IL_YBCD8TSyLlsqzIcBgYw'), ('--0RSTtl4mvX3Nd0JjhL_V1YV1ip', 'tF8R9nmZTyyfnv2zWADI0A'), ('--0SDuSOM_gpweM3PDb0WOFgYDyv', 'mLO6o1GBShWrtXfoSJHgfw'), ('--0jLz1eNRwtR_oRxPpgVC9U437y', 'HVheHWfxTpe28HhbHpcO1A'), ('--1cY4hzXaXdThxsw7saN3nd3xyt', 'h79v1yw_Qf-kOVTa0WYfdg'), ('--2nakQLLxyI_vsEOIbwzHMgWWPQ', 'iIGUoyaiT5i4b4UynPlnSQ'), ('--3-SQGOswGzwaEo5QiQNCcZQhi8', '1lQ90kNaSWyJXBQ_kK91Tg'), ('--3Km0GSVTHRkNCJHHjQdHqxwVfR', 'dHAJP-NvQw22FoBeDXiAIg')]\n</code></pre> <p>Each is a tuple of two strings. The first is the entry ID while the second is the upload ID.</p> <p>Now data can be downloaded.</p> <pre><code>result = query.download(8)\nprint(f'Downloaded {len(result)} entries.')  # Downloaded 100 entries.\n</code></pre> <p>The first eight entries will be downloaded. If one prints the list again, only the last two are present.</p> <pre><code>print(query.entry_list())\n</code></pre> <pre><code>[('--3-SQGOswGzwaEo5QiQNCcZQhi8', '1lQ90kNaSWyJXBQ_kK91Tg'), ('--3Km0GSVTHRkNCJHHjQdHqxwVfR', 'dHAJP-NvQw22FoBeDXiAIg')]\n</code></pre> <p>It is possible to download more data. We perform one more download call to illustrate that the fetch process will be automatically triggered.</p> <pre><code>result = query.download(5)\nprint(f'Downloaded {len(result)} entries.')  # Downloaded 200 entries.\n</code></pre> <p>In the above, we request five entries. However, the list contains only two entries, the fetch process will be called to fetch extra three entries from server. But since the page size is 10, the server will return 10 entries. You will see the following message in the terminal.</p> <pre><code>Fetching remote uploads...\n10 entries are qualified and added to the download list.\nDownloading 5 entries...  [####################################]  100%\n</code></pre> <p>Now that we have downloaded a few entries, we can convert them into pandas dataframe.</p> <pre><code>dataframes = query.entries_to_dataframe(keys_to_filter=['workflow2.results.calculation_result_ref'])\nprint(dataframes)\n</code></pre> <p>By setting <code>keys_to_filter</code> to <code>['workflow2.results.calculation_result_ref']</code>, we create a dataframe representing the content that exists in the response tree under the section <code>calculation_result_ref</code>. Below, you can see the final dataframe printed in the Python console. You can also try setting this option to an empty list (or simply removing the option) to see the results containing the entire response tree in dataframe format. Furthermore, since we have converted our data into a pandas dataframe, we can proceed to export CSV files, obtain statistics, create plots, and more.</p> <pre><code>    energy.fermi  ...  system_ref.chemical_composition_reduced\n0            NaN  ...                                     O3Ti\n1            NaN  ...                                   LiO3Ti\n2            NaN  ...                                     O3Ti\n3            NaN  ...                                     O3Ti\n4            NaN  ...                                   LiO3Ti\n5            NaN  ...                                     O3Ti\n6            NaN  ...                                  Cu44OTi\n7  -1.602177e-18  ...                                  O51Ti26\n8  -1.602177e-18  ...                                  O51Ti26\n9            NaN  ...                                     O3Ti\n10 -1.602177e-18  ...                                  O51Ti26\n11 -1.602177e-18  ...                                  O51Ti26\n12 -1.602177e-18  ...                                  O51Ti26\n13           NaN  ...                                     O3Ti\n14           NaN  ...                                    KO3Ti\n15           NaN  ...                                    KO3Ti\n16           NaN  ...                                     O3Ti\n17           NaN  ...                                   AlO3Ti\n18           NaN  ...                                   O3TiZn\n\n[19 rows x 7 columns]\n</code></pre>"},{"location":"howto/programmatic/archive_query.html#argument-list","title":"Argument List","text":"<p>The following arguments are acceptable for <code>ArchiveQuery</code>.</p> <ul> <li><code>owner</code> : <code>str</code> The scope of data to access. Default: <code>'visible'</code></li> <li><code>query</code> : <code>dict</code> The API query. There are no validations of any means carried out by the class, users shall make sure   the provided query is valid. Otherwise, server would return error messages.</li> <li><code>required</code> : <code>dict</code> The required quantities.</li> <li><code>url</code> : <code>str</code> The database url. It can be the one of your local database. The official NOMAD database is used be   default if no valid one defined. Default: <code>http://nomad-lab.eu/prod/v1/api</code></li> <li><code>after</code> : <code>str</code> It can be understood that the data is stored in a sequential list. Each upload has a unique ID,   if <code>after</code> is not provided, the query always starts from the first upload. One can choose to query the uploads in the   middle of storage by assigning a proper value of <code>after</code>.</li> <li><code>results_max</code> : <code>int</code> Determine how many entries to download. Note each upload may have multiple entries.</li> <li><code>page_size</code> : <code>int</code> Page size.</li> <li><code>username</code> : <code>str</code> Username for authentication.</li> <li><code>password</code> : <code>str</code> Password for authentication.</li> <li><code>retry</code> : <code>int</code> In the case of server errors, the fetch process is automatically retried every <code>sleep_time</code> seconds.   This argument limits the maximum times of retry.</li> <li><code>sleep_time</code> : <code>float</code> The interval of fetch retry.</li> </ul>"},{"location":"howto/programmatic/archive_query.html#the-complete-example","title":"The complete example","text":"<p>Attention</p> <p>This examples uses the new <code>workflow2</code> workflow system. This is still under development and this example might not yet produce results on the public nomad data.</p> <pre><code>--8 &lt; -- \"examples/archive/archive_query.py\"\n</code></pre>"},{"location":"howto/programmatic/download.html","title":"Download data","text":"<p>A common use-case for the NOMAD API is to download large amounts of NOMAD data. In this how-to guide, we use curl and API endpoints that stream .zip files to download many resources with a single request directly from the command line.</p>"},{"location":"howto/programmatic/download.html#prerequisites","title":"Prerequisites","text":"<p>Here is some background information to understand the examples better.</p>"},{"location":"howto/programmatic/download.html#curl","title":"curl","text":"<p>To download resources from a REST API using curl, you can utilize the powerful command-line tool to send HTTP requests and retrieve the desired data. Curl provides a simple and efficient way to interact with RESTful APIs, allowing you to specify the necessary headers, parameters, and authentication details. Whether you need to download files, retrieve JSON data, or access other resources, curl offers a flexible and widely supported solution for programmatically fetching data from REST APIs.</p>"},{"location":"howto/programmatic/download.html#raw-files-vs-processed-data","title":"Raw files vs processed data","text":"<p>We are covering two types of resources: raw files and processed data. The former is organized into uploads and sub directory. The organization depends on how the author was providing the files. The later is organized by entries. Each NOMAD entry has corresponding structured data.</p> <p>Endpoints that target raw files typically contain <code>raw</code>, e.g. <code>uploads/&lt;id&gt;/raw</code> or <code>entries/raw/query</code>. Endpoints that target processed data contain <code>archive</code> (because we call the entirety of all processed data the NOMAD Archive), e.g. <code>entries/&lt;id&gt;/archive</code> or <code>entries/archive/query</code>.</p>"},{"location":"howto/programmatic/download.html#entry-vs-upload","title":"Entry vs upload","text":"<p>API endpoints for data download either target entries or uploads. For both types of entities, endpoints for raw files and processed data (as well as searchable metadata) exist. API endpoint paths start with the entity, e.g. <code>uploads/&lt;id&gt;/raw</code> or <code>entries/&lt;id&gt;/raw</code>.</p>"},{"location":"howto/programmatic/download.html#download-a-whole-upload","title":"Download a whole upload","text":"<p>Let's assume you want to download an entire upload. In this example the upload id is <code>wW45wJKiREOYTY0ARuknkA</code>.</p> <pre><code>curl -X GET \"http://localhost:8000/fairdi/nomad/latest/api/v1/uploads/wW45wJKiREOYTY0ARuknkA/raw\" -o download.zip\n</code></pre> <p>This will create a <code>download.zip</code> file in the current folder. The zip file will contain the raw file directory of the upload.</p> <p>The used <code>uploads/&lt;id&gt;/raw</code> endpoint is only available for published uploads. For those, all raw files have already been packed into a zip file and this endpoint simply lets you download it. This is the simplest and most reliable download implementation.</p> <p>Alternatively, you can download specific files or sub-directories. This method is available for all uploads. Including un-published uploads.</p> <pre><code>curl -X GET \"http://localhost:8000/fairdi/nomad/latest/api/v1/uploads/wW45wJKiREOYTY0ARuknkA/raw/?compress=true\" -o download.zip\n</code></pre> <p>This endpoint looks very similar, but is implemented very differently. Note that we put an empty path <code>/</code> to the end of the URL, plus a query parameter <code>compress=true</code>. The path can be replaced with any directory or file path in the upload; <code>/</code> would denote the whole upload. The query parameter says that we want to download the whole directory as a zip file, instead of an individual file. This traverses through all files and creates a zip file on the fly.</p>"},{"location":"howto/programmatic/download.html#download-a-whole-dataset","title":"Download a whole dataset","text":"<p>Now let's assume that you want to download all raw files that are associated with all the entries of an entire dataset. In this example the dataset DOI is <code>10.17172/NOMAD/2023.11.17-2</code>.</p> <pre><code>curl -X POST \"http://localhost:8000/fairdi/nomad/latest/api/v1/entries/raw/query\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"query\": {\n        \"datasets.doi\": \"10.17172/NOMAD/2023.11.17-2\"\n    }\n}' \\\n-o download.zip\n</code></pre> <p>This time, we use the <code>entries/raw/query</code> endpoint that is based on entries and not on uploads. Here, we select entries with a query. In the example, we query for the dataset DOI, but you can replace this with any NOMAD search query (look out for the <code>&lt;&gt;</code> symbol on the search interface). The zip file will contain all raw files from all the directories that have the mainfile of one of the entries that match the queries.</p> <p>This might not necessarily download all uploaded files. Alternatively, you can use a query to get all upload ids and then use the method from the previous section:</p> <pre><code>curl -X POST \"http://localhost:8000/fairdi/nomad/latest/api/v1/entries/query\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"query\": {\n        \"datasets.doi\": \"10.17172/NOMAD/2023.11.17-2\"\n    },\n    \"pagination\": {\n        \"page_size\": 0\n    },\n    \"aggregations\": {\n        \"upload_ids\": {\n            \"terms\": {\n                \"quantity\": \"upload_id\"\n            }\n        }\n    }\n}'\n</code></pre> <p>The last command will print JSON data that contains all the upload ids. It uses the <code>entries/query</code> endpoint that allows you to query NOMAD's search. It does not return any results (<code>page_size: 0</code>), but performs an aggregation over all search results and collects the upload ids from all entries.</p>"},{"location":"howto/programmatic/download.html#download-some-processed-data-for-a-whole-dataset","title":"Download some processed data for a whole dataset","text":"<p>Similar to raw files, you can also download processed data. This is also an entry based operation based on a query. This time we also specify a <code>required</code> to explain which parts of the processed data, we are interested in:</p> <pre><code>curl -X POST \"http://localhost:8000/fairdi/nomad/latest/api/v1/entries/archive/download/query\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"query\": {\n        \"datasets.doi\": \"10.17172/NOMAD/2023.11.17-2\"\n    },\n    \"required\": {\n        \"metadata\": {\n            \"entry_id\": \"*\",\n            \"mainfile\": \"*\",\n            \"upload_id\": \"*\"\n        },\n        \"results\": {\n            \"material\": \"*\"\n        },\n        \"run\": {\n            \"system[-1]\": {\n                \"atoms\": \"*\"\n            }\n        }\n    }\n}' \\\n-o download.zip\n</code></pre> <p>Here we use the <code>entries/archive/download/query</code> endpoint. The result is a zip file with one json file per entry. There are no directories and the files are named <code>&lt;entry-id&gt;.json</code>. To associate the json files with entries, you should require information that tells you more about the entries, e.g. <code>required.metadata.mainfile</code>.</p> <p>See also the How to access processed data how-to guide.</p>"},{"location":"howto/programmatic/json_transformer.html","title":"Transform JSON Data Structures with Transformer","text":""},{"location":"howto/programmatic/json_transformer.html#who-is-this-how-to-guide-for","title":"Who is this how-to guide for?","text":"<p>This guide is designed for mid-level NOMAD users who need to parse JSON-formatted data structures into another JSON format. An example use case is transforming an external API response, partially or wholly, onto a NOMAD archive in a standardized format. This document will cover the basic principles and common applications of the JsonToJson Transformer from the <code>nomad-lab</code> package.</p>"},{"location":"howto/programmatic/json_transformer.html#what-should-you-know-before-this-how-to-guide","title":"What should you know before this how-to guide?","text":"<p>Before diving into this guide, you should be familiar with the following:</p> <ul> <li>A basic understanding of the <code>nomad-lab</code> package. Follow the How to install nomad-lab guide</li> </ul>"},{"location":"howto/programmatic/json_transformer.html#what-you-will-know-at-the-end-of-this-how-to-guide","title":"What you will know at the end of this how-to guide?","text":"<p>By the end of this how-to guide, you will:</p> <ul> <li>Understand how to use the <code>Transformer</code> class to transform JSON data structures.</li> <li>Be able to apply transformation rules to transform data from one format to another.</li> <li>Learn how to customize and extend data conversion rules for specific needs.</li> </ul>"},{"location":"howto/programmatic/json_transformer.html#steps","title":"Steps","text":""},{"location":"howto/programmatic/json_transformer.html#1-define-your-transformation-rules","title":"1. Define Your Transformation Rules","text":"<p>Create a Python file and define the rules that specify how to transform your JSON data. These rules dictate where to source data in the input JSON and where and how to place it in the output JSON. Set the following JSON data to a variable <code>json_example</code> and create rules as:</p> <pre><code>{\n  \"data\": {\n    \"a\": 1,\n    \"b\": 2\n  },\n  \"schema\": {\n    \"rules\": {\n      \"rule_a\": {\n        \"source\": \"a\",\n        \"target\": \"a\"\n      },\n      \"rule_b\": {\n        \"source\": \"b\",\n        \"target\": \"b\"\n      }\n    }\n  }\n}\n</code></pre> <p>Use this script to load the rules:</p> <pre><code>from nomad.datamodel.metainfo.annotations import Rules\n\nrules = {\n    \"example_transformation\": Rules(\n        json_example['schema']\n    )\n}\n</code></pre>"},{"location":"howto/programmatic/json_transformer.html#2-initialize-the-transformer","title":"2. Initialize the Transformer","text":"<p>In your Python script, initialize the <code>Transformer</code> with the rules you defined.</p> <pre><code>from nomad.utils.json_transformer import Transformer\n\ntransformer = Transformer(rules)\n</code></pre>"},{"location":"howto/programmatic/json_transformer.html#3-prepare-your-source-json","title":"3. Prepare Your Source JSON","text":"<p>Prepare the JSON data that needs to be transformed. This data can come from files, API responses, or other sources. Here we are reading from the <code>json_example</code> from above:</p> <pre><code>source_json = json_example['data']\n</code></pre>"},{"location":"howto/programmatic/json_transformer.html#4-transform-the-data","title":"4. Transform the Data","text":"<p>Use the <code>transform</code> method of your transformer instance to apply the transformation rules to your source JSON.</p> <pre><code>transformed_json = transformer.transform(source_json, \"example_transformation\")\nprint(transformed_json)\n</code></pre> <p>this should produce:</p> <pre><code>{\n  \"a\": 1,\n  \"b\": 2\n}\n</code></pre>"},{"location":"howto/programmatic/json_transformer.html#advanced-usage-of-jsontojson-transformer","title":"Advanced Usage of JsonToJson Transformer","text":""},{"location":"howto/programmatic/json_transformer.html#handling-complex-json-transformations","title":"Handling Complex JSON Transformations","text":"<p>In more complex scenarios, you may need to handle nested JSON structures, apply conditional logic, or resolve references. This section will guide you through using advanced features of the <code>Transformer</code> class to address these challenges.</p>"},{"location":"howto/programmatic/json_transformer.html#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, it is better to have read the basic instructions explained above as it contains information on how to load the transformer.</p>"},{"location":"howto/programmatic/json_transformer.html#complex-rules-definition","title":"Complex Rules Definition","text":"<p>In more sophisticated environments, transformation rules may require the evaluation of conditions, the manipulation of lists, or the resolution of nested structures. Below are examples of such advanced usage:</p>"},{"location":"howto/programmatic/json_transformer.html#conditional-copy-based-on-regex","title":"Conditional Copy Based on Regex","text":"<p>You can use regular expressions to conditionally copy data based on pattern matching. This is useful when you need to filter or format data before placing it in the target JSON.</p> <pre><code>{\n  \"data\": {\n    \"a\": 30\n  },\n  \"schema\": {\n    \"rules\": {\n      \"rule_age\": {\n        \"source\": \"a\",\n        \"target\": \"age\",\n        \"conditions\": [\n          {\n            \"regex_condition\": {\n              \"regex_path\": \"a\",\n              \"regex_pattern\": \"^3\\\\d$\"\n            }\n          }\n        ],\n        \"default_value\": \"default_age\"\n      }\n    }\n  }\n}\n</code></pre> <pre><code>transformer = Transformer(mapping_dict=rules)\n\ntransformed_json = transformer.transform(source_json, \"conditional_transformation_met\")\nprint(transformed_json)\n</code></pre> <p>The rule checks if the value at path \"a\" matches the regex pattern (i.e., starts with '3' followed by any digit). If the condition is not met, the target \"age\" is set to \"default_age\".</p> <p>this should produce:</p> <pre><code>{\n  \"a\": 30\n}\n</code></pre>"},{"location":"howto/programmatic/json_transformer.html#resolving-references","title":"Resolving References","text":"<p>When dealing with complex data structures, it might be necessary to use the structure of another <code>rule</code> defined in a different part of the transformation. For this you can set the <code>reference</code> value to the path of your interest (keep in mind that this path should be started with <code>#</code> sign).</p> <p>Important</p> <p>The referenced rule's values will be overwritten by the local rule; meaning you can partly import the referenced rule and overwrite other fields onto your desired values.</p> <pre><code>source_json = {\n    \"users\": [\n        {\"name\": \"user_1\", \"role\": \"manager\", \"manager_id\": \"101\"},\n        {\"name\": \"user_2\", \"role\": \"employee\", \"manager_id\": \"102\"},\n        {\"name\": \"user_3\", \"role\": \"manager\", \"manager_id\": \"103\"}\n    ],\n    \"details\": [\n        {\"id\": \"101\", \"name\": \"user_4\", \"department\": \"A\"},\n        {\"id\": \"102\", \"name\": \"user_5\", \"department\": \"B\"},\n        {\"id\": \"103\", \"name\": \"user_6\", \"department\": \"C\"}\n    ]\n}\n\nrules = {\n    \"employee_info\": Rules(\n        name=\"Employee Info Mapping\",\n        rules={\n            \"rule_manager\": Rule(\n                source=\"users[?role=='manager'].manager_id | [0]\",\n                target=\"manager_details\",\n                use_rule=\"#employee_info.details\"\n            ),\n            \"details\": Rule(\n                source=\"details[?id=='101'] | [0]\",\n                target=\"specific_manager\"\n            )\n        }\n    )\n}\n</code></pre> <p>In this example, the first item in the <code>rule</code> list, is a reference to the second item in the list. This setup extracts manager details a specific manager from the list.</p>"},{"location":"howto/programmatic/json_transformer.html#implementing-advanced-transformations","title":"Implementing Advanced Transformations","text":""},{"location":"howto/programmatic/json_transformer.html#nested-structure-manipulation","title":"Nested Structure Manipulation","text":"<p>You can manipulate nested structures by specifying deeper paths and using lists or dictionaries as intermediary storage.</p> <pre><code>{\n  \"data\": {\n    \"f\": {\n      \"nested\": {\n        \"key\": \"value\"\n      }\n    }\n  },\n  \"schema\": {\n    \"rules\": {\n      \"rule_f_nested_key\": {\n        \"source\": \"f.nested.key\",\n        \"target\": \"nested_value\"\n      }\n    }\n  }\n}\n</code></pre> <p>The Transformer can handle deeply nested JSON structures. Define rules that navigate through nested paths to extract or set data.</p>"},{"location":"howto/programmatic/publish_python.html","title":"How to publish data using python","text":""},{"location":"howto/programmatic/publish_python.html#uploading-changing-metadata-and-publishing-via-python-api","title":"Uploading, changing metadata, and publishing via python API","text":"<p>The NOMAD API allows uploading, publishing, etc. using a local python environment, as an alternative to the NOMAD GUI. An overview of all API functionalities is provided in How to use the API</p> <p>We have prepare some simple python functions to facilitate use of this API. For use as demonstrated below, copy the following code into a file called NOMAD_API.py:</p> <pre><code>import os\nimport requests\n\ndef get_authentication_token(nomad_url, username, password):\n    '''Get the token for accessing your NOMAD unpublished uploads remotely'''\n    try:\n        response = requests.get(\n            nomad_url + 'auth/token', params=dict(username=username, password=password), timeout=10)\n        token = response.json().get('access_token')\n        if token:\n            return token\n\n        print('response is missing token: ')\n        print(response.json())\n        return\n    except Exception:\n        print('something went wrong trying to get authentication token')\n        return\n\n\ndef create_dataset(nomad_url, token, dataset_name):\n    '''Create a dataset to group a series of NOMAD entries'''\n    try:\n        response = requests.post(\n            nomad_url + 'datasets/',\n            headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n            json={\"dataset_name\": dataset_name},\n            timeout=10\n            )\n        dataset_id = response.json().get('dataset_id')\n        if dataset_id:\n            return dataset_id\n\n        print('response is missing dataset_id: ')\n        print(response.json())\n        return\n    except Exception:\n        print('something went wrong trying to create a dataset')\n        return\n\ndef upload_to_NOMAD(nomad_url, token, upload_file):\n    '''Upload a single file as a new NOMAD upload. Compressed zip/tar files are\n    automatically decompressed.\n    '''\n    with open(upload_file, 'rb') as f:\n        try:\n            response = requests.post(\n                f'{nomad_url}uploads?file_name={os.path.basename(upload_file)}',\n                headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n                data=f, timeout=30)\n            upload_id = response.json().get('upload_id')\n            if upload_id:\n                return upload_id\n\n            print('response is missing upload_id: ')\n            print(response.json())\n            return\n        except Exception:\n            print('something went wrong uploading to NOMAD')\n            return\n\ndef check_upload_status(nomad_url, token, upload_id):\n    '''\n    # upload success =&gt; returns 'Process publish_upload completed successfully'\n    # publish success =&gt; 'Process publish_upload completed successfully'\n    '''\n    try:\n        response = requests.get(\n            nomad_url + 'uploads/' + upload_id,\n            headers={'Authorization': f'Bearer {token}'}, timeout=30)\n        status_message = response.json().get('data').get('last_status_message')\n        if status_message:\n            return status_message\n\n        print('response is missing status_message: ')\n        print(response.json())\n        return\n    except Exception:\n        print('something went wrong trying to check the status of upload' + upload_id)\n        # upload gets deleted from the upload staging area once published...or in this case something went wrong\n        return\n\ndef edit_upload_metadata(nomad_url, token, upload_id, metadata):\n    '''\n    Example of new metadata:\n    upload_name = 'Test_Upload_Name'\n    metadata = {\n        \"metadata\": {\n        \"upload_name\": upload_name,\n        \"references\": [\"https://doi.org/xx.xxxx/xxxxxx\"],\n        \"datasets\": dataset_id,\n        \"embargo_length\": 0,\n        \"coauthors\": [\"coauthor@affiliation.de\"],\n        \"comment\": 'This is a test upload...'\n        },\n    }\n    '''\n\n    try:\n        response = requests.post(\n            nomad_url+'uploads/' + upload_id + '/edit',\n            headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n            json=metadata, timeout=30)\n        return response\n    except Exception:\n        print('something went wrong trying to add metadata to upload' + upload_id)\n        return\n\ndef publish_upload(nomad_url, token, upload_id):\n    '''Publish an upload'''\n    try:\n        response = requests.post(\n            nomad_url+'uploads/' + upload_id + '/action/publish',\n            headers={'Authorization': f'Bearer {token}', 'Accept': 'application/json'},\n            timeout=30)\n        return response\n    except Exception:\n        print('something went wrong trying to publish upload: ' + upload_id)\n        return\n</code></pre> <p>Now, we will demonstrate how to use these functions. Within a notebook or python script, import the above functions:</p> <pre><code>from Nomad_API import *\n</code></pre> <p>Define the following user information: <pre><code>username = 'nomad_email@affiliation.edu'\npassword = 'password'\n</code></pre></p> <p>Define the NOMAD API endpoint: <pre><code># nomad_url = 'https://nomad-lab.eu/prod/v1/api/v1/'  # production nomad\nnomad_url = 'https://nomad-lab.eu/prod/v1/test/api/v1/'  # test nomad (deleted occassionally)\n</code></pre></p> <p>Get a token for accessing your unpublished uploads:</p> <pre><code>token = get_authentication_token(nomad_url, username, password)\n</code></pre> <p>Create a dataset for grouping uploads that belong to, e.g., a publication:</p> <pre><code>dataset_id = create_dataset(nomad_url, token, 'Test_Dataset')\n</code></pre> <p>Upload some test data to NOMAD:</p> <pre><code>upload_id = upload_to_NOMAD(nomad_url, token, 'test_data.zip')\n</code></pre> <p>Check the status to make sure the upload was processed correctly:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <p>The immediate result may be:</p> <pre><code>'Waiting for results (level 0)'\n</code></pre> <p>After some time you will get:</p> <pre><code>'Process process_upload completed successfully'\n</code></pre> Tip <p>Some data, e.g., large systems or molecular dynamics trajectories, take some time to process. In this case, you can call the above function intermittantly, e.g., in a while loop with a sleep call in between, waiting for <code>last_status_message</code> to be \"Process process_upload completed successfully\"</p> <p>Now that the upload processing is complete, we can add coauthors, references, and other comments, as well as link to a dataset and provide a proper name for the upload:</p> <pre><code>metadata = {\n    \"metadata\": {\n    \"upload_name\": 'Test_Upload',\n    \"references\": [\"https://doi.org/xx.xxxx/x.xxxx\"],\n    \"datasets\": dataset_id,\n    \"embargo_length\": 0,\n    \"coauthors\": [\"coauthor@affiliation.de\"],\n    \"comment\": 'This is a test upload...',\n},\n}\nresponse = edit_upload_metadata(nomad_url, token, upload_id, metadata)\n</code></pre> <p>Check the upload again to make sure that the metadata was changed:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <pre><code>'Process edit_upload_metadata completed successfully'\n</code></pre> <p>Now, we are ready to publish:</p> <pre><code>response = publish_upload(nomad_url, token, upload_id)\n</code></pre> <p>Once again check the status:</p> <pre><code>last_status_message = check_upload_status(nomad_url, token, upload_id)\nprint(last_status_message)\n</code></pre> <pre><code>'Process publish_upload completed successfully'\n</code></pre>"},{"location":"howto/programmatic/pythonlib.html","title":"How to install nomad-lab","text":"<p>We provide a Python package called <code>nomad-lab</code>. The package can be used to run certain NOMAD features within local Python programming environments. It includes the NOMAD parsers and normalizers, or convenience functions to query the processed data on NOMAD.</p> <p>Released version of the package are hosted on pypi and you can install it with pip (or conda).</p> <p>To install the newest pypi release, simply use pip: <pre><code>pip install nomad-lab\n</code></pre></p> <p>Attention</p> <p>The latest develop versions might still be considered beta and might not be published to pypi. If you require specific new features you might need to install <code>nomad-lab</code> from our GitLab package registry. To use features of a specific commit or branch, consider to clone and build the project yourself.</p> <p>To install the latest release developer releases from our GitLab use: <pre><code>pip install nomad-lab --extra-index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre></p> <p>To install an older version of NOMAD (e.g. v0.10.x), you can of use reference the respective version on pypy: <pre><code>pip install nomad-lab==1.0.10\n</code></pre></p> <p>Certain functionality might require more dependencies. The basic install above, installs the dependencies for accessing the NOMAD Archive or running most of the NOMAD parsers.</p> <p>Other functions, e.g. running the NOMAD infrastructure, require additional dependencies. You can use the <code>[extra]</code> notation to install these extra requirements:</p> <p><pre><code>pip install nomad-lab[parsing]\npip install nomad-lab[infrastructure]\npip install nomad-lab[dev]\n</code></pre> The various extras have the following meaning:</p> <ul> <li>parsing, run all parsers, incl. parsers based on HDF5, netCDF, or asr</li> <li>infrastructure, everything to run NOMAD services</li> <li>dev, necessary to run development and build tools, e.g. pytest, pylint, mypy</li> </ul>"},{"location":"reference/annotations.html","title":"Annotations","text":"<p>Definitions in a schema can have annotations. These annotations provide additional information that NOMAD can use to alter its behavior around these definitions. Annotations are named blocks of key-value pairs:</p> <pre><code>definitions:\n  sections:\n    MyAnnotatedSection:\n      m_annotations:\n        annotation_name:\n          key1: value\n          key2: value\n</code></pre> <p>Many annotations control the representation of data in the GUI. This can be for plots or data entry/editing capabilities.</p>"},{"location":"reference/annotations.html#eln-annotations","title":"ELN annotations","text":"<p>These annotations control how data can be entered and edited. Use the key <code>eln</code> to add this annotations. For example:</p> <pre><code>class Sample(EntryData):\n    sample_id = Quantity(type=str, a_eln=dict(component='StringEditQuantity'))`)\n</code></pre> <p>or in YAML schemas: <pre><code>Sample:\n  quantities:\n    sample_id:\n      type: str\n      m_annotations:\n        eln:\n          component: StringEditQuantity\n</code></pre></p> <p>An <code>eln</code> annotation can be added to section and quantity definitions to different effects. In both cases, it controls how sections and quantities are represented in the GUI with different parameters; see below.</p> <p>The UI gives an overview about all ELN edit annotations and components here.</p> name type component <code>str</code> The form field component that is used to make the annotated quantity editable. If no component is given, the quantity won't be editable. This can be used on quantities only.The supported values are:<code>StringEditQuantity</code>: For editing simple short string values. <code>URLEditQuantity</code>: For editing strings that are validated to be URLs. <code>EnumEditQuantity</code>: For Editing enum values. Uses a dropdown list with enum values. This component may be used for short enumerates. <code>RadioEnumEditQuantity</code>: For Editing enum values. Uses radio buttons. <code>AutocompleteEditQuantity</code>: For editing enum values. Uses an autocomplete form with dropdown list. This component may be used for longer enumerates. <code>FileEditQuantity</code>: For editing a reference to a file. Will allow to choose a file or upload a file. <code>BoolEditQuantity</code>: For editing boolean choices. <code>NumberEditQuantity</code>: For editing numbers with our without unit. <code>SliderEditQuantity</code>: For editing numbers with a horizontal slider widget. <code>DateTimeEditQuantity</code>: For editing datetimes. <code>RichTextEditQuantity</code>: For editing long styled text with a rich text editor. <code>ReferenceEditQuantity</code>: For editing references to other sections. <code>UserEditQuantity</code>: For entering user information. Lets you choose a nomad user or enter information manually. <code>AuthorEditQuantity</code>: For entering author information manually.options: - <code>StringEditQuantity</code> - <code>URLEditQuantity</code> - <code>EnumEditQuantity</code> - <code>RadioEnumEditQuantity</code> - <code>AutocompleteEditQuantity</code> - <code>FileEditQuantity</code> - <code>BoolEditQuantity</code> - <code>NumberEditQuantity</code> - <code>SliderEditQuantity</code> - <code>DateTimeEditQuantity</code> - <code>DateEditQuantity</code> - <code>TimeEditQuantity</code> - <code>RichTextEditQuantity</code> - <code>ReferenceEditQuantity</code> - <code>UserEditQuantity</code> - <code>AuthorEditQuantity</code> - <code>QueryEditQuantity</code> - <code>ActionEditQuantity</code> label <code>str</code> [Deprecated] ELN label annotation has been deprecated and it is advised to utilize display annotation instead. Custom label for the quantity shown on the form field. It is recommended to adhere to the convention of using lowercase letters for the label, except for abbreviations which could be capitalized. props <code>dict[str, Any]</code> A dictionary with additional props that are passed to the edit component. default <code>Any</code> Prefills any set form field component with the given value. This is different from the quantities <code>default</code> property. The quantities default is not stored in the data; the default value is assumed if no other value is given. The ELN form field default value will be stored, even if not changed. defaultDisplayUnit <code>str</code> This attribute is deprecated, use the <code>unit</code> attribute of <code>display</code> annotation instead. Allows to define a default unit to initialize a <code>NumberEditQuantity</code> with. The unit has to be compatible with the unit of the annotation quantity and the annotated quantity must have a unit. Only applies to quantities and with <code>component=NumberEditQuantity</code>.deprecated minValue <code>int | float</code> Allows to specify a minimum value for quantity annotations with number type. Will show an error, if outside numbers are entered. Only works on quantities and in conjunction with <code>component=NumberEditQuantity</code>. maxValue <code>int | float</code> Allows to specify a maximum value for quantity annotations with number type. Will show an error, if outside numbers are entered. Only works on quantities and in conjunction with <code>component=NumberEditQuantity</code>. showSectionLabel <code>bool</code> To customize the ReferenceEditQuantity behaviour. If true the section label will be shown instead of referenced file name and the path to the section. hide <code>list[str]</code> This attribute is deprecated. Use <code>visible</code> attribute of <code>display</code> annotation instead. Allows you to hide certain quantities from a section editor. Give a list of quantity names. Quantities must exist in the section that this annotation is added to. Can only be used in section annotations.deprecated overview <code>bool</code> Shows the annotation section on the entry's overview page. Can only be used on section annotations. lane_width <code>str | int</code> Value to overwrite the css width of the lane used to render the annotation section and its editor. properties <code>SectionProperties</code> The value to customize the quantities and sub sections of the annotation section. The supported keys: <code>visible</code>: To determine the visible quantities and sub sections by their names <code>editable</code>: To render things visible but not editable, e.g. in inheritance situations <code>order</code>: # To order things, properties listed in that order first, then the rest"},{"location":"reference/annotations.html#sectionproperties","title":"SectionProperties","text":"<p>The display settings for quantities and subsections. (Deprecated)</p> name type visible <code>Filter | None</code> Defines the visible quantities and subsections. (Deprecated)default: <code>1</code> editable <code>Filter | None</code> Defines the editable quantities and subsections. (Deprecated) order <code>list[str] | None</code> To customize the order of the quantities and subsections. (Deprecated)"},{"location":"reference/annotations.html#filter","title":"Filter","text":"<p>A filter defined by an include list or and exclude list of the quantities or subsections.</p> name type include <code>list[str] | None</code> The list of quantity or subsection names to be included. exclude <code>list[str] | None</code> The list of quantity or subsection names to be excluded."},{"location":"reference/annotations.html#browser","title":"Browser","text":"<p>The <code>browser</code> annotation allows to specify if the processed data browser needs to display a quantity differently. It can be applied to quantities. For example</p> <pre><code>    class Experiment(EntryData):\n        description = Quantity(type=str, a_browser=dict(render_value='HtmlValue'))\n</code></pre> <p>or in yaml</p> <pre><code>Experiment:\n  quantities:\n    description:\n      type: str\n      m_annotations:\n        browser:\n          render_value: HtmlValue\n</code></pre> name type adaptor <code>str</code> Allows to change the Adaptor implementation that is used to render the lane for this quantity. Possible values are:<code>RawFileAdaptor</code>: An adopter that is used to show files, including all file actions, like file preview.options: - <code>RawFileAdaptor</code> render_value <code>str</code> Allows to change the Component used to render the value of the quantity. Possible values are:<code>HtmlValue</code>: Renders a string as HTML. <code>JsonValue</code>: Renders a dict or list in a collapsable tree.options: - <code>JsonValue</code> - <code>HtmlValue</code>"},{"location":"reference/annotations.html#display-annotations","title":"Display annotations","text":""},{"location":"reference/annotations.html#display-annotation-for-quantities","title":"Display annotation for quantities","text":"<p>This annotations control how quantities are displayed in the GUI.  Use the key <code>display</code> to add this annotation. For example in Python:</p> <pre><code>class Example(EntryData):\n    sample_id = Quantity(type=str, a_display={'visible': False})\n</code></pre> <p>or in YAML: <pre><code>definitions:\n  Example:\n    quantities:\n      sample_id:\n        type: str\n        m_annotations:\n          display:\n            visible: false\n</code></pre></p> name type visible <code>Filter | None</code> Defines the visible quantities and subsections.default: <code>1</code> editable <code>Filter | None</code> Defines the editable quantities and subsections. unit <code>str | None</code> To determine the default display unit for quantity."},{"location":"reference/annotations.html#display-annotation-for-sections","title":"Display annotation for sections","text":"<p>This annotations control how sections are displayed in the GUI. Use the key <code>display</code> to add this annotation. For example in Python:</p> <pre><code>class Example(MSection):\n    m_def = Section(a_display={\n        'visible': False\n    })\n</code></pre> <p>or in YAML: <pre><code>definitions:\n  sections:\n    Example:\n      m_annotations:\n        display:\n          visible: false\n</code></pre></p> name type visible <code>Filter | None</code> Defines the visible quantities and subsections.default: <code>1</code> editable <code>Filter | None</code> Defines the editable quantities and subsections. order <code>list[str] | None</code> To customize the order of the quantities and subsections."},{"location":"reference/annotations.html#label_quantity","title":"<code>label_quantity</code>","text":"<p>This annotation goes in the section that we want to be filled with tabular data, not in the single quantities. It is used to give a name to the instances that might be created by the parser. If it is not provided, the name of the section itself will be used as name. Many times it is useful because, i. e., one might want to create a bundle of instances of, say, a \"Substrate\" class, each instance filename not being \"Substrate_1\", \"Substrate_2\", etc., but being named after a quantity contained in the class that is, for example, the specific ID of that sample.</p> <pre><code>MySection:\n  more:\n    label_quantity: my_quantity\n  quantities:\n    my_quantity:\n      type: np.float64\n      shape: [\"*\"]\n      description: \"my quantity to be filled from the tabular data file\"\n      unit: K\n      m_annotations:\n        tabular:\n          name: \"Sheet1/my header\"\n        plot:\n          x: timestamp\n          y: ./my_quantity\n</code></pre> <p>Important</p> <p>The quantity designated as <code>label_quantity</code> should not be an array but a integer, float or string, to be set as the name of a file. If an array quantity is chosen, the parser would fall back to the use of the section as name.</p>"},{"location":"reference/annotations.html#schema-annotation","title":"Schema annotation","text":"<p>Used to annotate Schemas to control how and if they are displayed in the GUI.</p> <p>You can attach this annotation to a schema class definition using the <code>m_def</code> attribute, for example:</p> <pre><code>from nomad.datamodel.data import Schema\nfrom nomad.datamodel.metainfo.annotations import SchemaAnnotation\n\nclass MySchema(Schema):\n    m_def = Section(a_schema=SchemaAnnotation(\n        label='My Schema',\n        enabled=False\n    ))\n</code></pre> name type label <code>str</code> Custom label to show for this schema in the GUI when instantiating new entries from it. If not set, the label set for the section will be used.default: <code>PydanticUndefined</code> enabled <code>bool</code> If set to False, the schema will not be available for instantiating entries in the GUI. Note that the schema can still be instantiated regularly with Python even it if is disabled in the GUI.default: <code>True</code>"},{"location":"reference/annotations.html#tabular-data","title":"Tabular data","text":""},{"location":"reference/annotations.html#tabular","title":"<code>tabular</code>","text":"<p>Allows to map a quantity to a row or a column of a spreadsheet data-file. Should only be used in conjunction with <code>tabular_parser</code>.</p> name type name <code>str</code> The column name that should be mapped to the annotation quantity. Has to be the same string that is used in the header, i.e. first <code>.csv</code> line or first excel file <code>row</code>. For excel files with multiple sheets, the name can have the form <code>&lt;sheet name&gt;/&lt;column name&gt;</code>. Otherwise, only the first sheets is used. Has to be applied to the quantity that a column should be mapped to. unit <code>str</code> The unit of the value in the file. Has to be compatible with the annotated quantity's unit. Will be used to automatically convert the value. If this is not defined, the values will not be converted. Has to be applied to the quantity that a column should be mapped to. <p>Each and every quantity to be filled with data from tabular data files should be annotated as the following example. A practical example is provided in How To section.</p> <pre><code>my_quantity:\n  type: np.float64\n  shape: [\"*\"]\n  description: \"my quantity to be filled from the tabular data file\"\n  unit: K\n  m_annotations:\n    tabular:\n      name: \"Sheet1/my header\"\n    plot:\n      x: timestamp\n      y: ./my_quantity\n</code></pre>"},{"location":"reference/annotations.html#tabular_parser","title":"<code>tabular_parser</code>","text":"<p>One special quantity will be dedicated to host the tabular data file. In the following examples it is called <code>data_file</code>, it contains the <code>tabular_parser</code> annotation, as shown below.</p> <p>Instructs NOMAD to treat a string valued scalar quantity as a file path and interprets the contents of this file as tabular data. Supports both <code>.csv</code> and Excel files.</p> name type parsing_options <code>TabularParsingOptions</code> Options on how to extract the data from csv/xlsx file. Under the hood, NOMAD uses pandas <code>Dataframe</code> to parse the data from tabular files. These are the available options that can be passed down to the parser.The supported values are:<code>skiprows</code>: Number of rows to be skipped while reading the file. <code>sep</code>: The character used to separate cells (specific to csv files). <code>comment</code>: The character denoting the commented lines. <code>separator</code>: An alias for <code>sep</code>.default: Complex object, default value not displayed. mapping_options <code>list[TabularMappingOptions]</code> A list of directives on how to map the extracted data from the csv/xlsx file to NOMAD. Each directive is a distinct directive, which allows for more modular definition of your tabular parser schema. If no item is provided, the entire schema is treated to be parsed under column mode.The supported values in each item of this list are:<code>mapping_mode</code>: A <code>list</code> of paths to the repeating sub-sections where the tabular quantities are to be filled from     individual rows of the excel/csv file (i.e. in the row mode). Each path is a <code>/</code> separated list of     nested sub-sections. The targeted sub-sections, will be considered when mapping table rows to quantities.     Has to be used to annotate the quantity that holds the path to the <code>.csv</code> or excel file. <code>file_mode</code>: The character used to separate cells (specific to csv files). <code>sections</code>: The character denoting the commented lines.default: <code>[]</code>"},{"location":"reference/annotations.html#tabularmappingoptions","title":"TabularMappingOptions","text":"name type mapping_mode <code>str</code> This controls the behaviour of mapping of the extracted data onto NOMAD schema.The supported values are:<code>row</code>: A <code>list</code> of paths to the repeating sub-sections where the tabular quantities are to be filled from     individual rows of the excel/csv file (i.e. in the row mode). Each path is a <code>/</code> separated list of     nested sub-sections. The targeted sub-sections, will be considered when mapping table rows to quantities.     Has to be used to annotate the quantity that holds the path to the <code>.csv</code> or excel file. <code>column</code>: A <code>list</code> of paths to the sub-sections where the tabular quantities are to be filled from the     entire column of the excel/csv file (i.e. in the column mode). Each path is a <code>/</code>     separated list of nested sub-sections. The targeted sub-sections, will be     considered when mapping table columns to quantities. Has to be used to annotate the quantity that     holds the path to the <code>.csv</code> or excel file. <code>enrty</code>: A <code>list</code> of paths to the (sub)sections where the tabular quantities are to be filled from individual rows     of the excel/csv file, to create distinct entries. Each path is a     <code>/</code> separated list of nested sub-sections. The targeted (sub)sections, will be     considered when mapping table rows to quantities. The schema of the resultant entry follows the     (sub)section's schema. In order to parse the entire schema using entry mode, then set the     first item in this list to <code>root</code>.     Has to be used to annotate the quantity that     holds the path to the <code>.csv</code> or excel file.default: <code>TabularMode.column</code>options: - <code>row</code> - <code>column</code> file_mode <code>str</code> This controls the behaviour of the parser towards working physical files in file system.The supported values are:<code>current_entry</code>: Processing the data into the same NOMAD entry. <code>single_new_entry</code>: Creating a new entry and processing the data into this new NOMAD entry. <code>multiple_new_entries</code>: Creating many new entries and processing the data into these new NOMAD entries.options: - <code>current_entry</code> - <code>single_new_entry</code> - <code>multiple_new_entries</code> sections <code>list[str]</code> A <code>list</code> of paths to the (sub)sections where the tabular quantities are to be filled from the data extracted from the tabular file."},{"location":"reference/annotations.html#tabularparsingoptions","title":"TabularParsingOptions","text":"name type skiprows <code>list[int] | int</code> Number of rows to skip sep <code>str</code> Character identifier of a separator comment <code>str</code> Character identifier of a commented line separator <code>str</code> Alias for <code>sep</code>"},{"location":"reference/annotations.html#available-combinations","title":"Available Combinations","text":"Tutorial ref. <code>file_mode</code> <code>mapping_mode</code> <code>sections</code> How to ref. 1 <code>current_entry</code> <code>column</code> <code>root</code> HowTo 2 <code>current_entry</code> <code>column</code> my path HowTo np1 <code>current_entry</code> <code>row</code> <code>root</code> Not possible 3 <code>current_entry</code> <code>row</code> my path HowTo np2 <code>single_new_entry</code> <code>column</code> <code>root</code> Not possible 4 <code>single_new_entry</code> <code>column</code> my path HowTo np3 <code>single_new_entry</code> <code>row</code> <code>root</code> Not possible 5 <code>single_new_entry</code> <code>row</code> my path HowTo np4 <code>multiple_new_entries</code> <code>column</code> <code>root</code> Not possible np5 <code>multiple_new_entries</code> <code>column</code> my path Not possible 6 <code>multiple_new_entries</code> <code>row</code> <code>root</code> HowTo 7 <code>multiple_new_entries</code> <code>row</code> my path HowTo <pre><code>data_file:\n  type: str\n  description: \"the tabular data file containing data\"\n  m_annotations:\n    tabular_parser:\n      parsing_options:\n        comment: \"#\"\n      mapping_options:\n        - mapping_mode: column\n          file_mode: single_new_entry\n          sections:\n            - my_section/my_quantity\n</code></pre>"},{"location":"reference/annotations.html#plot","title":"Plot","text":"<p>The PlotSection base section serves as an additional functionality to your sections. This base section is designed to simplify the process of creating various types of plots, making it easy to use Plotly Express, Plotly Subplot, and the general Plotly graph objects.</p> <p>Features:</p> <ul> <li>Plotly Express: Create simple and quick plots with a high-level, expressive API.</li> <li>Plotly Subplot: Organize multiple plots into subplots for more complex visualizations.</li> <li>General Plotly Graph Objects: Fine-tune your plots by working directly with Plotly's graph objects.</li> </ul> <p>Usage:</p> <ul> <li>Inherit from this base section to leverage its plot functionality.</li> <li>Customize your plots using the annotations plotly-express, plotly-subplots, or/and plotly-graph-object.</li> </ul> <p>The PlotSection class makes it possible to define plots that are shown alongside your data. Underneath, we use the Plotly Open Source Graphing Libraries to control the creation of the plots, and you can find many useful examples in their documentation.</p> <p>In Python schemas, the PlotSection class gives you full freedom to define plots programmatically. For example, you could use plotly.express and plotly.graph_objs to define plots like this:</p> <pre><code>from nomad.datamodel.metainfo.plot import PlotSection, PlotlyFigure\nfrom nomad.datamodel.data import EntryData\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\nclass CustomSection(PlotSection, EntryData):\n    m_def = Section()\n    time = Quantity(type=float, shape=['*'], unit='s', a_eln=dict(component='NumberEditQuantity'))\n    substrate_temperature = Quantity(type=float, shape=['*'], unit='K', a_eln=dict(component='NumberEditQuantity'))\n    chamber_pressure = Quantity(type=float, shape=['*'], unit='Pa', a_eln=dict(component='NumberEditQuantity'))\n\n    def normalize(self, archive, logger):\n        super(CustomSection, self).normalize(archive, logger)\n\n        first_line = px.scatter(x=self.time, y=self.substrate_temperature)\n        second_line = px.scatter(x=self.time, y=self.chamber_pressure)\n        figure1 = make_subplots(rows=1, cols=2, shared_yaxes=True)\n        figure1.add_trace(first_line.data[0], row=1, col=1)\n        figure1.add_trace(second_line.data[0], row=1, col=2)\n        figure1.update_layout(height=400, width=716, title_text=\"Creating Subplots in Plotly\")\n        self.figures.append(PlotlyFigure(label='figure 1', figure=figure1.to_plotly_json()))\n\n        figure2 = px.scatter(x=self.substrate_temperature, y=self.chamber_pressure, color=self.chamber_pressure, title=\"Chamber as a function of Temperature\")\n        self.figures.append(PlotlyFigure(label='figure 2', index=1, figure=figure2.to_plotly_json()))\n\n        heatmap_data = [[None, None, None, 12, 13, 14, 15, 16],\n             [None, 1, None, 11, None, None, None, 17],\n             [None, 2, 6, 7, None, None, None, 18],\n             [None, 3, None, 8, None, None, None, 19],\n             [5, 4, 10, 9, None, None, None, 20],\n             [None, None, None, 27, None, None, None, 21],\n             [None, None, None, 26, 25, 24, 23, 22]]\n\n        heatmap = go.Heatmap(z=heatmap_data, showscale=False, connectgaps=True, zsmooth='best')\n        figure3 = go.Figure(data=heatmap)\n        figure_json = figure3.to_plotly_json()\n        figure_json['config'] = {'staticPlot': True}\n        self.figures.append(PlotlyFigure(label='figure 3', index=0, figure=figure_json)\n</code></pre> <p>To customize the plot configuration in python one can add the config to the generated json by to_plotly_json().</p> <pre><code>figure_json['config'] = {'staticPlot': True}\n</code></pre> <p>In YAML schemas, plots can be defined by using the PlotSection as a base class, and additionally utilizing different flavours of plot annotations. The different annotation options are described below.</p>"},{"location":"reference/annotations.html#plotlygraphobjectannotation","title":"PlotlyGraphObjectAnnotation","text":"<p>Allows to plot figures using plotly graph object.</p> <pre><code>    Example:\n      base_sections:\n        - 'nomad.datamodel.metainfo.plot.PlotSection'\n      m_annotations:\n        plotly_graph_object:\n        - data:\n            x: '#xArr'\n            y: '#xArr'\n          layout:\n            title:\n              text: 'Plotly Graph Object'\n          label: 'Plotly Graph Object'\n          index: 1\n          open: true\n</code></pre> name type label <code>str</code> Figure label data <code>dict</code> Plotly data layout <code>dict</code> Plotly layout config <code>dict</code> Plotly config"},{"location":"reference/annotations.html#plotlyexpressannotation","title":"PlotlyExpressAnnotation","text":"<p>Allows to plot multi trace figures using plotly Express.</p> <pre><code>  sections:\n    Example:\n      base_sections:\n        - 'nomad.datamodel.metainfo.plot.PlotSection'\n      m_annotations:\n        plotly_express:\n          method: scatter\n          x: '#xArr'\n          y: '#yArr'\n          label: 'Example Express Plot'\n          index: 0\n          open: true\n          layout:\n            title:\n              text: 'Example Express Plot'\n            xaxis:\n              title:\n                text: 'x axis'\n            yaxis:\n              title:\n                text: 'y axis'\n          traces:\n            - method: scatter\n              x: '#xArr'\n              y: '#zArr'\n</code></pre> name type method <code>str</code> Plotly express plot method layout <code>dict</code> Plotly layout x <code>list[float] | list[str] | str</code> Plotly express x y <code>list[float] | list[str] | str</code> Plotly express y z <code>list[float] | list[str] | str</code> Plotly express z color <code>list[float] | list[str] | str</code> Plotly express color symbol <code>str</code> Plotly express symbol title <code>str</code> Plotly express title label <code>str</code> Figure label traces <code>list[PlotlyExpressTraceAnnotation]</code> List of traces added to the main trace defined by plotly_express methoddefault: <code>[]</code>"},{"location":"reference/annotations.html#plotlyexpresstraceannotation","title":"PlotlyExpressTraceAnnotation","text":"<p>Allows to plot figures using plotly Express.</p> name type method <code>str</code> Plotly express plot method layout <code>dict</code> Plotly layout x <code>list[float] | list[str] | str</code> Plotly express x y <code>list[float] | list[str] | str</code> Plotly express y z <code>list[float] | list[str] | str</code> Plotly express z color <code>list[float] | list[str] | str</code> Plotly express color symbol <code>str</code> Plotly express symbol title <code>str</code> Plotly express title"},{"location":"reference/annotations.html#plotlysubplotsannotation","title":"PlotlySubplotsAnnotation","text":"<p>Allows to plot figures in subplots.</p> <pre><code>    Example:\n      base_sections:\n        - 'nomad.datamodel.metainfo.plot.PlotSection'\n      m_annotations:\n        plotly_subplots:\n          parameters:\n            rows: 2\n            cols: 2\n          layout:\n            title:\n              text: 'All plots'\n          plotly_express:\n            - method: scatter\n              x: '#xArr'\n              y: '#yArr'\n              title: 'subplot 1'\n            - method: scatter\n              x: '#xArr'\n              y: '#zArr'\n              title: 'subplot 2'\n            - method: scatter\n              x: '#zArr'\n              y: '#xArr'\n              title: 'subplot 3'\n            - method: scatter\n              x: '#zArr'\n              y: '#yArr'\n              title: 'subplot 4'\n</code></pre> name type label <code>str</code> Figure label layout <code>dict</code> Plotly layout parameters <code>dict</code> plotly.subplots.make_subplots parameters i.e. rows, cols, shared_xaxes, shared_xaxes, horizontal_spacing , ... See plotly make_subplots documentation for more information. plotly_express <code>list[PlotlyExpressAnnotation]</code> List of subplots defined by plotly_express methoddefault: <code>[]</code>"},{"location":"reference/annotations.html#plot-annotations-in-python","title":"plot annotations in python","text":"<p>For simple plots in Python schema one could use the annotations without normalizer:</p> <pre><code>from nomad.datamodel.metainfo.plot import PlotSection\nfrom nomad.metainfo import Quantity, Section\nfrom nomad.datamodel.data import EntryData\n\nclass CustomSection(PlotSection, EntryData):\n    m_def = Section(\n        a_plotly_graph_object=[\n            {\n                'label': 'graph object 1',\n                'data': {'x': '#time', 'y': '#chamber_pressure'},\n                'layout': {\n                    'title': {\n                        'text': 'Plot in section level'\n                    },\n                    'xaxis': {\n                        'title': {\n                            'text': 'x data'\n                        }\n                    },\n                    'yaxis': {\n                        'title': {\n                            'text': 'y data'\n                        }\n                    }\n                }\n            }, {\n                'label': 'graph object 2',\n                'data': {'x': '#time', 'y': '#substrate_temperature'}\n            }\n        ],\n        a_plotly_express={\n            'label': 'fig 2',\n            'index': 2,\n            'method': 'scatter',\n            'x': '#substrate_temperature',\n            'y': '#chamber_pressure',\n            'color': '#chamber_pressure'\n        },\n        a_plotly_subplots={\n            'label': 'fig 1',\n            'index': 1,\n            'parameters': {'rows': 2, 'cols': 2},\n            'layout': {\n                'title': {\n                    'text': 'All plots'\n                }\n            },\n            'plotly_express': [\n                {\n                    'method': 'scatter',\n                    'x': '#time',\n                    'y': '#chamber_pressure',\n                    'color': '#chamber_pressure'\n                },\n                {\n                    'method': 'scatter',\n                    'x': '#time',\n                    'y': '#substrate_temperature',\n                    'color': '#substrate_temperature'\n                },\n                {\n                    'method': 'scatter',\n                    'x': '#substrate_temperature',\n                    'y': '#chamber_pressure',\n                    'color': '#chamber_pressure'\n                },\n                {\n                    'method': 'scatter',\n                    'x': '#substrate_temperature',\n                    'y': '#chamber_pressure',\n                    'color': '#substrate_temperature'\n                }\n            ]\n        }\n    )\n    time = Quantity(type=float, shape=['*'], unit='s', a_eln=dict(component='NumberEditQuantity'))\n    substrate_temperature = Quantity(type=float, shape=['*'], unit='K', a_eln=dict(component='NumberEditQuantity'))\n    chamber_pressure = Quantity(type=float, shape=['*'], unit='Pa', a_eln=dict(component='NumberEditQuantity'))\n</code></pre>"},{"location":"reference/annotations.html#plotannotation-deprecated","title":"PlotAnnotation (Deprecated)","text":"<p>The <code>PlotAnnotation</code> is now deprecated and will be removed in future releases. We recommend transitioning to the use of <code>PlotSection</code> and <code>PlotlyGraphObjectAnnotation</code> for your plotting needs.</p> <p>This annotation can be used to add a plot to a section or quantity. Example:</p> <pre><code>class Evaporation(MSection):\n    m_def = Section(a_plot={\n        'label': 'Temperature and Pressure',\n        'x': 'process_time',\n        'y': ['./substrate_temperature', './chamber_pressure'],\n        'config': {\n            'editable': True,\n            'scrollZoom': False\n        }\n    })\n    time = Quantity(type=float, shape=['*'], unit='s')\n    substrate_temperature = Quantity(type=float, shape=['*'], unit='K')\n    chamber_pressure = Quantity(type=float, shape=['*'], unit='Pa')\n</code></pre> <p>You can create multi-line plots by using lists of the properties <code>y</code> (and <code>x</code>). You either have multiple sets of <code>y</code>-values over a single set of <code>x</code>-values. Or you have pairs of <code>x</code> and <code>y</code> values. For this purpose the annotation properties <code>x</code> and <code>y</code> can reference a single quantity or a list of quantities. For repeating sub sections, the section instance can be selected with an index, e.g. \"sub_section_name/2/parameter_name\" or with a slice notation <code>start:stop</code> where negative values index from the end of the array, e.g. \"sub_section_name/1:-5/parameter_name\".</p> <p>The interactive examples of the plot annotations can be found here.</p> name type label <code>str</code> Is passed to plotly to define the label of the plot. x <code>list[str] | str</code> A path or list of paths to the x-axes values. Each path is a <code>/</code> separated list of sub-section and quantity names that leads from the annotation section to the quantity. Repeating sub sections are indexed between two <code>/</code>s with an integer or a slice <code>start:stop</code>.default: <code>PydanticUndefined</code> y <code>list[str] | str</code> A path or list of paths to the y-axes values. list of sub-section and quantity names that leads from the annotation section to the quantity. Repeating sub sections are indexed between two <code>/</code>s with an integer or a slice <code>start:stop</code>.default: <code>PydanticUndefined</code> lines <code>list[dict]</code> A list of dicts passed as <code>traces</code> to plotly to configure the lines of the plot. See https://plotly.com/javascript/reference/scatter/ for details. layout <code>dict</code> A dict passed as <code>layout</code> to plotly to configure the plot layout. See https://plotly.com/javascript/reference/layout/ for details. config <code>dict</code> A dict passed as <code>config</code> to plotly to configure the plot functionality. See https://plotly.com/javascript/configuration-options/ for details."},{"location":"reference/annotations.html#h5web","title":"H5Web","text":"<p>The H5WebAnnotation provides a way to control how H5Web renders visualization for HDF5Dataset quantities. The annotation values are written to the corresponding HDF5 object attributes and are subsequently read by H5Web.</p> <p>Usage:</p> <ul> <li>Use this base section as a type in your Quantity.</li> <li>Add addictional annotations to trigger the H5Web visualizer to the section containing the Quantity, and, optionally, to its parent sections.</li> </ul> <p>Note</p> <p>If an EntryData class contain the <code>a_h5web</code> annotation, the H5Web plot is shown in the entry overview page.</p> <p>An example of use of H5WebAnnotation in Python schemas:</p> <pre><code>from nomad.datamodel.data import ArchiveSection, EntryData\nfrom nomad.metainfo import Section\nfrom nomad.datamodel.hdf5 import HDF5Dataset\nfrom nomad.datamodel.metainfo.annotations import H5WebAnnotation\n\nclass A(ArchiveSection):\n\n    m_def = Section(a_h5web=H5WebAnnotation(\n        axes='x',\n        signal='y'\n        auxiliary_signals=['y_err']))\n\n    x = Quantity(\n        type=HDF5Dataset,\n        unit='s',\n        a_h5web = H5WebAnnotation(\n            long_name='my_x_label (s)'\n        ))\n\n    y = Quantity(\n        type=HDF5Dataset,\n        unit='m',\n        a_h5web = H5WebAnnotation(\n            long_name='my_y_label (m)'\n        ))\n\n    y_err = Quantity(\n        type=HDF5Dataset,\n        unit='m',\n        a_h5web = H5WebAnnotation(\n            long_name='my_y_err_label'\n        ))\n\nclass B(EntryData):\n\n    m_def = Section(a_h5web=H5WebAnnotation(\n      axes='value_x', signal='value_y', paths=['a/0']))\n\n    a = SubSection(sub_section=A, repeats=True)\n\n    value_x = Quantity(type=HDF5Dataset)\n\n    value_y = Quantity(type=HDF5Dataset, unit='m')\n</code></pre> <p>In this example, an H5Web view of the variables <code>x</code>, <code>y</code>, and <code>y_err</code> is displayed in the page of the subsection <code>A</code>. The plot of variables <code>x_value</code> and <code>y_value</code> is also displayed; as the <code>B</code> class is an <code>EntryData</code>, the plot is shown in the entry overview page. Additionally, alongside with the plot of the class <code>B</code>, the overview page presents also the plot contained in the subsection <code>A</code>, due to the <code>paths</code> attribute. The <code>paths</code> attribute allows to show in a parent section or subsection the plots originally contained in children subsections. Parents and children refer here to composed object.</p> <p>Note</p> <p>The <code>paths</code> variable points in the example above to a repeated subsection, hence the path provided includes a serial number pointing to the subsection object to be displayed in the entry overview page. To show in the overview page a non-repeatable subsection, no serial number is required in the path.</p> <p>H5Web implements visualization features through the attributes shown above, they can be attached to datasets and groups of an HDF5 file. The conventions for the attributes are rooted in the NeXus language and more explanations can be found in the NXData documentation page and in the Associating plottable data documentation page.</p>"},{"location":"reference/annotations.html#h5webannotation","title":"H5WebAnnotation","text":"<p>Provides interface to the H5Web visualizer for HDF5 files. The annotation can be used for section and quantity definitions in order to include group and dataset attributes to the archive HDF5 file.</p> <p>Refer to https://h5web.panosc.eu/ for a more detailed description of the annotation fields.</p> name type axes <code>str | list[str]</code> Names of the HDF5Dataset quantities to plot on the independent axes. signal <code>str</code> Name of the HDF5Dataset quantity to plot on the dependent axis. long_name <code>str</code> Label for the hdf5 dataset. Note: this attribute will overwrite also the unit. auxiliary_signals <code>list[str]</code> Additional datasets to include in plot as signal. title <code>str</code> Title of the plot errors <code>str</code> Name of the HDF5Dataset quantity to plot as error bars. indices <code>int | list[int]</code> Indices of the HDF5Dataset to include in plot. paths <code>list[str]</code> List of section paths to visualize.default: <code>[]</code>"},{"location":"reference/basesections.html","title":"Base Sections","text":"<p>The <code>nomad.datamodel.metainfo.basesections</code> Metainfo package contains a set of base sections. They provides shared definitions across materials science domains and schemas. Some functionality, e.g. the workflow visualisation, depend on these definitions. Inherit from these base sections when you create your own schemas to align your definitions with those of other schemas and to make use of respective functionality.</p>"},{"location":"reference/basesections.html#basesection","title":"BaseSection","text":"<p>description: A generic abstract base section that provides a few commonly used properties. If you inherit from this section, but do not need some quantities, list those quantities in the <code>eln.hide</code> annotation of your inheriting section definition.</p> <p>Besides predefining some quantities, these base sections will add some metadata to NOMAD's search. A particular example are <code>tags</code>, if you define a string or enum quantity in your sections named <code>tags</code>, its values will be searchable.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>links: http://purl.obolibrary.org/obo/BFO_0000001</p> <p>properties:</p> name type name <code>str</code> A short human readable and descriptive name. datetime <code>nomad.metainfo.data_type.Datetime</code> The date and time associated with this section. lab_id <code>str</code> An ID string that is unique at least for the lab that produced this data. description <code>str</code> Any information that cannot be captured in the other fields. <p>normalization: </p> <ul> <li>If the instance is of type <code>EntryData</code>, it sets the archive's entry name based on the instance's name.</li> <li>Sets the <code>datetime</code> field to the current time if it is not already set.</li> <li>Manages the <code>lab_id</code> field and updates the archive's <code>results.eln.lab_ids</code> list.</li> <li>Adds the instance's <code>name</code> and <code>description</code> to the archive's <code>results.eln.names</code> and <code>results.eln.descriptions</code> lists, respectively.</li> <li>Handles the <code>tags</code> attribute, if present, and updates the archive's <code>results.eln.tags</code> list.</li> <li>Appends the section's name to the archive's <code>results.eln.sections</code> list.</li> </ul>"},{"location":"reference/basesections.html#entity","title":"Entity","text":"<p>description: An object that persists, endures, or continues to exist through time while maintaining its identity.</p> <p>inherits from: <code>BaseSection</code></p> <p>links: http://purl.obolibrary.org/obo/BFO_0000002</p> <p>normalization: </p> <ul> <li>If the instance is of type <code>EntryData</code>, it sets the archive's entry name based on the instance's name.</li> <li>Sets the <code>datetime</code> field to the current time if it is not already set.</li> <li>Manages the <code>lab_id</code> field and updates the archive's <code>results.eln.lab_ids</code> list.</li> <li>Adds the instance's <code>name</code> and <code>description</code> to the archive's <code>results.eln.names</code> and <code>results.eln.descriptions</code> lists, respectively.</li> <li>Handles the <code>tags</code> attribute, if present, and updates the archive's <code>results.eln.tags</code> list.</li> <li>Appends the section's name to the archive's <code>results.eln.sections</code> list.</li> </ul>"},{"location":"reference/basesections.html#activitystep","title":"ActivityStep","text":"<p>description: Any dependant step of an <code>Activity</code>.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type name <code>str</code> A short and descriptive name for this step. start_time <code>nomad.metainfo.data_type.Datetime</code> Optionally, the starting time of the activity step. If omitted, it is assumed to follow directly after the previous step. comment <code>str</code> Any additional information about the step not captured by the other fields."},{"location":"reference/basesections.html#activity","title":"Activity","text":"<p>description: An action that has a temporal extension and for some time depends on some entity.</p> <p>inherits from: <code>BaseSection</code></p> <p>links: http://purl.obolibrary.org/obo/BFO_0000015</p> <p>properties:</p> name type datetime <code>nomad.metainfo.data_type.Datetime</code> The date and time when this activity was started. method <code>str</code> A short consistent handle for the applied method. location <code>str</code> location of the experiment. steps <code>ActivityStep</code> An ordered list of all the dependant steps that make up this activity.sub-section, repeats <p>normalization: </p> <ul> <li>Ensures the <code>results.eln.methods</code> list is initialized and appends the method or section name.</li> <li>Converts each step in <code>self.steps</code> to a task, using the steps <code>to_task()</code> method, and assigns it to <code>archive.workflow2.tasks</code>.</li> </ul>"},{"location":"reference/basesections.html#sectionreference","title":"SectionReference","text":"<p>description: A section used for referencing another section.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type name <code>str</code> A short descriptive name for the role of this reference. reference <code>nomad.datamodel.data.ArchiveSection</code> A reference to a NOMAD archive section."},{"location":"reference/basesections.html#entityreference","title":"EntityReference","text":"<p>description: A section used for referencing an Entity.</p> <p>inherits from: <code>SectionReference</code></p> <p>properties:</p> name type reference <code>Entity</code> A reference to a NOMAD <code>Entity</code> entry. lab_id <code>str</code> The readable identifier for the entity. <p>normalization: </p> <p>Will attempt to fill the <code>reference</code> from the <code>lab_id</code> or vice versa.</p>"},{"location":"reference/basesections.html#experimentstep","title":"ExperimentStep","text":"<p>description: Any dependant step of an <code>Experiment</code>.</p> <p>inherits from: <code>ActivityStep</code></p> <p>properties:</p> name type activity <code>Activity</code> The activity that makes up this step of the experiment. lab_id <code>str</code> The readable identifier for the activity. <p>normalization: </p> <p>Will attempt to fill the <code>activity</code> from the <code>lab_id</code> or vice versa. If the activity reference is filled but the start time is not the time will be taken from the <code>datetime</code> property of the referenced activity.</p>"},{"location":"reference/basesections.html#experiment","title":"Experiment","text":"<p>description: A section for grouping activities together into an experiment.</p> <p>inherits from: <code>Activity</code></p> <p>properties:</p> name type steps <code>ExperimentStep</code> An ordered list of all the dependant steps that make up this activity.sub-section, repeats <p>normalization: </p> <ul> <li>Ensures the <code>results.eln.methods</code> list is initialized and appends the method or section name.</li> <li>Converts each step in <code>self.steps</code> to a task, using the steps <code>to_task()</code> method, and assigns it to <code>archive.workflow2.tasks</code>.</li> </ul>"},{"location":"reference/basesections.html#collection","title":"Collection","text":"<p>description: A section for grouping entities together into a collection.</p> <p>inherits from: <code>Entity</code></p> <p>properties:</p> name type entities <code>EntityReference</code> References to the entities that make up the collection.sub-section, repeats <p>normalization: </p> <ul> <li>If the instance is of type <code>EntryData</code>, it sets the archive's entry name based on the instance's name.</li> <li>Sets the <code>datetime</code> field to the current time if it is not already set.</li> <li>Manages the <code>lab_id</code> field and updates the archive's <code>results.eln.lab_ids</code> list.</li> <li>Adds the instance's <code>name</code> and <code>description</code> to the archive's <code>results.eln.names</code> and <code>results.eln.descriptions</code> lists, respectively.</li> <li>Handles the <code>tags</code> attribute, if present, and updates the archive's <code>results.eln.tags</code> list.</li> <li>Appends the section's name to the archive's <code>results.eln.sections</code> list.</li> </ul>"},{"location":"reference/basesections.html#elementalcomposition","title":"ElementalComposition","text":"<p>description: A section for describing the elemental composition of a system, i.e. the element and its atomic fraction.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type element <code>['Ac', 'Ag', 'Al', 'Am', 'Ar', 'As', 'At', 'Au', 'B', 'Ba', 'Be', 'Bh', 'Bi', 'Bk', 'Br', 'C', 'Ca', 'Cd', 'Ce', 'Cf', 'Cl', 'Cm', 'Cn', 'Co', 'Cr', 'Cs', 'Cu', 'Db', 'Ds', 'Dy', 'Er', 'Es', 'Eu', 'F', 'Fe', 'Fl', 'Fm', 'Fr', 'Ga', 'Gd', 'Ge', 'H', 'He', 'Hf', 'Hg', 'Ho', 'Hs', 'I', 'In', 'Ir', 'K', 'Kr', 'La', 'Li', 'Lr', 'Lu', 'Lv', 'Mc', 'Md', 'Mg', 'Mn', 'Mo', 'Mt', 'N', 'Na', 'Nb', 'Nd', 'Ne', 'Nh', 'Ni', 'No', 'Np', 'O', 'Og', 'Os', 'P', 'Pa', 'Pb', 'Pd', 'Pm', 'Po', 'Pr', 'Pt', 'Pu', 'Ra', 'Rb', 'Re', 'Rf', 'Rg', 'Rh', 'Rn', 'Ru', 'S', 'Sb', 'Sc', 'Se', 'Sg', 'Si', 'Sm', 'Sn', 'Sr', 'Ta', 'Tb', 'Tc', 'Te', 'Th', 'Ti', 'Tl', 'Tm', 'Ts', 'U', 'V', 'W', 'Xe', 'Y', 'Yb', 'Zn', 'Zr']</code> The symbol of the element, e.g. 'Pb'. atomic_fraction <code>float64</code> The atomic fraction of the element in the system it is contained within. Per definition a positive value less than or equal to 1. mass_fraction <code>float64</code> The mass fraction of the element in the system it is contained within. Per definition a positive value less than or equal to 1. <p>normalization: </p> <p>Will add a results.material subsection if none exists. Will append the element to the elements property of that subsection and a nomad.datamodel.results.ElementalComposition instances to the elemental_composition property  using the element and atomic fraction from this section.</p>"},{"location":"reference/basesections.html#system","title":"System","text":"<p>description: A base section for any system of materials which is investigated or used to construct other systems.</p> <p>inherits from: <code>Entity</code></p> <p>properties:</p> name type elemental_composition <code>ElementalComposition</code> A list of all the elements found in the system together and their respective atomic fraction within the system.sub-section, repeats <p>normalization: </p> <p>Will attempt to fill mass fractions or atomic fractions if left blank.</p>"},{"location":"reference/basesections.html#instrument","title":"Instrument","text":"<p>description: A base section that can be used for instruments.</p> <p>inherits from: <code>Entity</code></p> <p>normalization: </p> <p>Adds the name of the instrument to the <code>results.eln.instruments</code> list.</p>"},{"location":"reference/basesections.html#instrumentreference","title":"InstrumentReference","text":"<p>description: A section used for referencing an Instrument.</p> <p>inherits from: <code>EntityReference</code></p> <p>properties:</p> name type reference <code>Instrument</code> A reference to a NOMAD <code>Instrument</code> entry. <p>normalization: </p> <p>Will attempt to fill the <code>reference</code> from the <code>lab_id</code> or vice versa.</p>"},{"location":"reference/basesections.html#component","title":"Component","text":"<p>description: A section for describing a component and its role in a composite system.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type name <code>str</code> A short name for the component. mass <code>float64</code> The mass of the component.unit=<code>kilogram</code> mass_fraction <code>float64</code> The mass fraction of the component in the composite system."},{"location":"reference/basesections.html#systemcomponent","title":"SystemComponent","text":"<p>description: A section for describing a system component and its role in a composite system.</p> <p>inherits from: <code>Component</code></p> <p>properties:</p> name type system <code>System</code> A reference to the component system. <p>normalization: </p> <p>If none is set, the normalizer will set the name of the component to be that of the referenced system if it has one.</p>"},{"location":"reference/basesections.html#puresubstancesection","title":"PureSubstanceSection","text":"<p>description: A sub section for describing any elemental, molecular or single phase pure substance.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type name <code>str</code> A short name for the substance. iupac_name <code>str</code> IUPAC name. molecular_formula <code>str</code> Molecular formula. molecular_mass <code>float64</code> The mass of the most likely isotopic composition for a single molecule, corresponding to the most intense ion/molecule peak in a mass spectrum.unit=<code>dalton</code> molar_mass <code>float64</code> The molar mass is the sum of all atomic masses of the constituent atoms in a compound, measured in g/mol. In the absence of explicit isotope labelling, averaged natural abundance is assumed. If an atom bears an explicit isotope label, 100%% isotopic purity is assumed at this location.unit=<code>gram / mole</code> monoisotopic_mass <code>float64</code> The mass of a molecule, calculated using the mass of the most abundant isotope of each element.unit=<code>dalton</code> inchi <code>str</code> Inchi. inchi_key <code>str</code> Inchi key. smile <code>str</code> Smile. canonical_smile <code>str</code> Canonical smile. cas_number <code>str</code> CAS number."},{"location":"reference/basesections.html#puresubstancecomponent","title":"PureSubstanceComponent","text":"<p>description: A section for describing a substance component and its role in a composite system.</p> <p>inherits from: <code>Component</code></p> <p>properties:</p> name type substance_name <code>str</code> The name of the substance within the section where this component is contained. pure_substance <code>PureSubstanceSection</code> Section describing the pure substance that is the component.sub-section <p>normalization: </p> <p>If none is set, the normalizer will set the name of the component to be the molecular formula of the substance.</p>"},{"location":"reference/basesections.html#compositesystem","title":"CompositeSystem","text":"<p>description: A base section for a material systems composed of components. Each component of the composite system is of a (sub)type of <code>System</code>.</p> <p>inherits from: <code>System</code></p> <p>properties:</p> name type components <code>Component</code> A list of all the components of the composite system containing a name, reference to the system section and mass of that component.sub-section, repeats <p>normalization: </p> <p>If the elemental composition list is empty, the normalizer will iterate over the components and extract all the elements for populating the elemental composition list. If masses are provided for all components and the elemental composition of all components contain atomic fractions the normalizer will also calculate the atomic fractions for the composite system. The populated elemental composition list is added to the results by the normalizer in the <code>System</code> super class.</p>"},{"location":"reference/basesections.html#compositesystemreference","title":"CompositeSystemReference","text":"<p>description: A section used for referencing a CompositeSystem.</p> <p>inherits from: <code>EntityReference</code></p> <p>properties:</p> name type reference <code>CompositeSystem</code> A reference to a NOMAD <code>CompositeSystem</code> entry. <p>normalization: </p> <p>Will attempt to fill the <code>reference</code> from the <code>lab_id</code> or vice versa.</p>"},{"location":"reference/basesections.html#processstep","title":"ProcessStep","text":"<p>description: Any dependant step of a <code>Process</code>.</p> <p>inherits from: <code>ActivityStep</code></p> <p>properties:</p> name type duration <code>float</code> The duration time of the process step.unit=<code>second</code>"},{"location":"reference/basesections.html#process","title":"Process","text":"<p>description: A planned process which results in physical changes in a specified input material. [ obi : prs obi : mc obi : fg obi : jf obi : bp ]</p> <p>Synonyms:  - preparative method  - sample preparation  - sample preparative method  - material transformations</p> <p>inherits from: <code>Activity</code></p> <p>links: http://purl.obolibrary.org/obo/OBI_0000094</p> <p>properties:</p> name type end_time <code>nomad.metainfo.data_type.Datetime</code> The date and time when this process was finished. steps <code>ProcessStep</code> An ordered list of all the dependant steps that make up this activity.sub-section, repeats instruments <code>InstrumentReference</code> A list of all the instruments and their role in this process.sub-section, repeats samples <code>CompositeSystemReference</code> The samples as that have undergone the process.sub-section, repeats <p>normalization: </p> <ul> <li>Sets the start time for each step in <code>self.steps</code> if not already set, based on the <code>datetime</code> and <code>duration</code> fields.</li> <li>Sets the <code>end_time</code> field to the calculated end time if it is not already set.</li> <li>Updates the <code>archive.workflow2.outputs</code> list with links to the samples processed.</li> </ul>"},{"location":"reference/basesections.html#activityresult","title":"ActivityResult","text":"<p>description: A section for the results of an <code>Activity</code>.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type name <code>str</code> A short and descriptive name for the result."},{"location":"reference/basesections.html#analysisresult","title":"AnalysisResult","text":"<p>description: A section for the results of an <code>Analysis</code> process.</p> <p>inherits from: <code>ActivityResult</code></p>"},{"location":"reference/basesections.html#analysis","title":"Analysis","text":"<p>description: A planned process that produces output data from input data. Synonyms: - data processing - data analysis</p> <p>inherits from: <code>Activity</code></p> <p>links: http://purl.obolibrary.org/obo/OBI_0200000</p> <p>properties:</p> name type inputs <code>SectionReference</code> The input data of the analysis.sub-section, repeats outputs <code>AnalysisResult</code> The output data of the analysis.sub-section, repeats <p>normalization: </p> <ul> <li>Updates the <code>archive.workflow2.inputs</code> list with links to the input data.</li> <li>Updates the <code>archive.workflow2.outputs</code> list with links to the output data.</li> </ul>"},{"location":"reference/basesections.html#synthesismethod","title":"SynthesisMethod","text":"<p>description: A method used to synthesise a sample.</p> <p>inherits from: <code>Process</code></p> <p>links: http://purl.obolibrary.org/obo/CHMO_0001301</p> <p>normalization: </p> <ul> <li>Sets the start time for each step in <code>self.steps</code> if not already set, based on the <code>datetime</code> and <code>duration</code> fields.</li> <li>Sets the <code>end_time</code> field to the calculated end time if it is not already set.</li> <li>Updates the <code>archive.workflow2.outputs</code> list with links to the samples processed.</li> </ul>"},{"location":"reference/basesections.html#measurementresult","title":"MeasurementResult","text":"<p>description: A section for the results of an <code>Measurement</code> process.</p> <p>inherits from: <code>ActivityResult</code></p>"},{"location":"reference/basesections.html#measurement","title":"Measurement","text":"<p>description: A planned process with the objective to produce information about the material entity that is the evaluant, by physically examining it or its proxies. [ obi : pppb ]</p> <p>inherits from: <code>Activity</code></p> <p>links: http://purl.obolibrary.org/obo/OBI_0000070</p> <p>properties:</p> name type samples <code>CompositeSystemReference</code> A list of all the samples measured during the measurement.sub-section, repeats instruments <code>InstrumentReference</code> A list of all the instruments and their role in this process.sub-section, repeats results <code>MeasurementResult</code> The result of the measurement.sub-section, repeats <p>normalization: </p> <ul> <li>Updates the <code>archive.workflow2.inputs</code> list with links to the input samples.</li> <li>Updates the <code>archive.workflow2.outputs</code> list with links to the measurement results.</li> </ul>"},{"location":"reference/basesections.html#puresubstance","title":"PureSubstance","text":"<p>description: A base section for any elemental, molecular, or single phase pure substance.</p> <p>inherits from: <code>System</code></p> <p>links: http://purl.obolibrary.org/obo/CHEBI_23367</p> <p>properties:</p> name type name <code>str</code> The name of the substance entry. lab_id <code>str</code> A human human readable substance ID that is at least unique for the lab. description <code>str</code> A field for adding additional information about the substance that is not captured by the other quantities and subsections. pure_substance <code>PureSubstanceSection</code> Section with properties describing the substance.sub-section <p>normalization: </p> <p>This method will populate the results.material section and the elemental composition sub section using the molecular formula.</p>"},{"location":"reference/basesections.html#pubchempuresubstancesection","title":"PubChemPureSubstanceSection","text":"<p>description: A section for pure substances existing as \"compounds\" in the PubChem database.</p> <p>inherits from: <code>PureSubstanceSection</code></p> <p>properties:</p> name type pub_chem_cid <code>int</code> pub_chem_link <code>str</code> <p>normalization: </p> <p>This method will attempt to get data on the substance instance from the PubChem PUG REST API: https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest If a PubChem CID is specified the details are retrieved directly. Otherwise a search query is made for the filled attributes in the following order:</p> <ol> <li><code>smile</code></li> <li><code>canonical_smile</code></li> <li><code>inchi_key</code></li> <li><code>iupac_name</code></li> <li><code>name</code></li> <li><code>molecular_formula</code></li> <li><code>cas_number</code></li> </ol>"},{"location":"reference/basesections.html#casexperimentalproperty","title":"CASExperimentalProperty","text":"<p>description: A section for experimental properties retrieved from the CAS API.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type name <code>str</code> CAS experimental property name. property <code>str</code> CAS experimental property. sourceNumber <code>str</code> CAS experimental property source."},{"location":"reference/basesections.html#caspropertycitation","title":"CASPropertyCitation","text":"<p>description: A section for citations of the experimental properties retrieved from the CAS API.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type docUri <code>str</code> CAS property citation document uri. sourceNumber <code>int</code> source <code>str</code> CAS property citation source."},{"location":"reference/basesections.html#caspuresubstancesection","title":"CASPureSubstanceSection","text":"<p>description: A base section for any <code>PureSubstance</code> with a CAS number.</p> <p>inherits from: <code>PureSubstanceSection</code></p> <p>properties:</p> name type cas_uri <code>str</code> CAS uri cas_number <code>str</code> CAS number. cas_name <code>str</code> CAS name. image <code>str</code> CAS image. cas_synonyms <code>str</code> CAS synonyms.shape=<code>['*']</code> cas_experimental_properties <code>CASExperimentalProperty</code> sub-section, repeats cas_property_citations <code>CASPropertyCitation</code> sub-section, repeats <p>normalization: </p> <p>This method will attempt to get data on the pure substance instance from the CAS API: https://commonchemistry.cas.org/api-overview If a CAS number is specified the details are retrieved directly. Otherwise a search query is made for the filled attributes in the following order:</p> <ol> <li><code>cas_name</code></li> <li><code>inchi</code></li> <li><code>inchi_key</code></li> <li><code>smile</code></li> <li><code>canonical_smile</code></li> <li><code>name</code></li> </ol>"},{"location":"reference/basesections.html#readableidentifiers","title":"ReadableIdentifiers","text":"<p>description: A base section that can be used to generate readable IDs. If the <code>owner</code>, <code>short_name</code>, <code>institute</code>, and <code>datetime</code> quantities are provided, the lab_id will be automatically created as a combination of these four quantities.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type institute <code>str</code> Alias/short name of the home institute of the owner, i.e. HZB. owner <code>str</code> Alias for the owner of the identified thing. This should be unique within the institute. datetime <code>nomad.metainfo.data_type.Datetime</code> A datetime associated with the identified thing. In case of an <code>Activity</code>, this should be the starting time and, in case of an <code>Entity</code>, the creation time. short_name <code>str</code> A short name of the the identified thing (e.g. the identifier scribed on the sample, the process number, or machine name), e.g. 4001-8, YAG-2-34. This is to be managed and decided internally by the labs, although we recommend to avoid the following characters in it: \"_\", \"/\", \"\\\" and \".\". lab_id <code>str</code> Full readable id. Ideally a human readable id convention, which is simple, understandable and still have chances of becoming unique. If the <code>owner</code>, <code>short_name</code>, <code>\u00ecnstitute</code>, and <code>datetime</code> are provided, this will be formed automatically by joining these components by an underscore (_). Spaces in any of the individual components will be replaced with hyphens (-). An example would be hzb_oah_20200602_4001-08. <p>normalization: </p> <p>If owner is not filled the field will be filled by the first two letters of the first name joined with the first two letters of the last name of the author. If the institute is not filled a institute abreviations will be constructed from the author's affiliation.</p> <p>If no datetime is filled, the datetime will be taken from the <code>datetime</code> property of the parent, if it exists, otherwise the current date and time will be used.</p> <p>If no short name is filled, the name will be taken from the parent name, if it exists, otherwise it will be taken from the archive metadata entry name, if it exists, and finally if no other options are available it will use the name of the mainfile.</p>"},{"location":"reference/basesections.html#publicationreference","title":"PublicationReference","text":"<p>description: A base section that can be used for references.</p> <p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type DOI_number <code>str</code> The DOI number referring to the published paper or dataset where the data can be found. Examples: 10.1021/jp5126624 10.1016/j.electacta.2017.06.032 publication_authors <code>str</code> The authors of the publication. If several authors, end with et al. If the DOI number is given correctly, this will be extracted automatically from www.crossref.orgshape=<code>['*']</code> publication_date <code>nomad.metainfo.data_type.Datetime</code> Publication date. If the DOI number is given correctly, this will be extracted automatically from www.crossref.org journal <code>str</code> Name of the journal where the data is published. If the DOI number is given correctly, this will be extracted automatically from www.crossref.org publication_title <code>str</code> Title of the publication. If the DOI number is given correctly, this will be extracted automatically from www.crossref.org <p>normalization: </p> <ul> <li>If a DOI number is provided, retrieves publication details from the CrossRef API.</li> <li>Populates the <code>publication_authors</code>, <code>journal</code>, <code>publication_title</code>, and <code>publication_date</code> fields based on the CrossRef response.</li> <li>Ensures the DOI number has the prefix <code>https://doi.org/</code>.</li> <li>Updates the archive's metadata references with the DOI number if it is not already present.</li> </ul>"},{"location":"reference/basesections.html#hdf5normalizer","title":"HDF5Normalizer","text":"<p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>normalization without further documentation</p>"},{"location":"reference/cli.html","title":"Command Line Interface (CLI)","text":""},{"location":"reference/cli.html#nomad","title":"nomad","text":"<p>This is the entry point to nomad's command line interface CLI. It uses a sub-command structure similar to the git command.</p> <p>Usage:</p> <pre><code>nomad [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -v, --verbose     sets log level to info\n  --debug           sets log level to debug\n  --log-label TEXT  Label applied to logg entries.\n  --help            Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>admin: The nomad admin commands to do nasty stuff directly on the databases.</li> <li>clean: Cleanse the given path by removing empty folders.</li> <li>client: Commands that use the nomad API to do useful things</li> <li>dev: Commands related to the nomad source code.</li> <li>parse: Run parsing and normalizing locally.</li> </ul>"},{"location":"reference/cli.html#nomad-admin","title":"nomad admin","text":"<p>The nomad admin commands to do nasty stuff directly on the databases. Remember: With great power comes great responsibility!</p> <p>Usage:</p> <pre><code>nomad admin [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>clean: Checks consistency of files and es vs mongo and deletes orphan entries.</li> <li>entries: Entry related commands</li> <li>lift-embargo: Check and lift embargo of data with expired embargo period.</li> <li>ops: Generate scripts and commands for nomad operation.</li> <li>reset: Reset/remove all databases.</li> <li>reset-processing: Reset all uploads and entries \"stuck\" in processing using level mongodb operations.</li> <li>rewrite-doi-urls: </li> <li>run: Run a nomad service locally (outside docker).</li> <li>upgrade: Commands for upgrading to a newer NOMAD version</li> <li>uploads: Upload related commands</li> <li>users: Add, import, export users.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-clean","title":"nomad admin clean","text":"<p>Checks consistency of files and es vs mongo and deletes orphan entries.</p> <p>Usage:</p> <pre><code>nomad admin clean [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --dry           Do not delete anything, just check.\n  --skip-entries  Skip cleaning entries with missing uploads.\n  --skip-fs       Skip cleaning the filesystem.\n  --skip-es       Skip cleaning the es index.\n  --staging-too   Also clean published entries in staging, make sure these\n                  files are not due to reprocessing\n  --force         Do not ask for confirmation.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-entries","title":"nomad admin entries","text":"<p>Entry related commands</p> <p>Usage:</p> <pre><code>nomad admin entries [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>rm: Delete selected entries from mongo and elastic</li> </ul>"},{"location":"reference/cli.html#nomad-admin-entries-rm","title":"nomad admin entries rm","text":"<p>Delete selected entries from mongo and elastic</p> <p>Usage:</p> <pre><code>nomad admin entries rm [OPTIONS] [ENTRIES]...\n</code></pre> <p>Options:</p> <pre><code>  --skip-es     Keep the elastic index version of the data.\n  --skip-mongo  Keep uploads and entries in mongo.\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-lift-embargo","title":"nomad admin lift-embargo","text":"<p>Check and lift embargo of data with expired embargo period.</p> <p>Usage:</p> <pre><code>nomad admin lift-embargo [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --dry               Do not lift the embargo, just show what needs to be\n                      done.\n  --parallel INTEGER  Use the given amount of parallel processes. Default is\n                      1.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops","title":"nomad admin ops","text":"<p>Generate scripts and commands for nomad operation.</p> <p>Usage:</p> <pre><code>nomad admin ops [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>dump: Dump the mongo db.</li> <li>nginx-conf: Generate an nginx.conf to serve the GUI and proxy pass to API container.</li> <li>prototypes-update: Updates the AFLOW prototype information using the latest online version and writes the results to a python module in the given FILEPATH.</li> <li>restore: Restore the mongo db.</li> <li>springer-update: Updates the springer database in nomad.config.normalize.springer_db_path.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-ops-dump","title":"nomad admin ops dump","text":"<p>Dump the mongo db.</p> <p>Usage:</p> <pre><code>nomad admin ops dump [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --restore  Do not dump, but restore.\n  --help     Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-nginx-conf","title":"nomad admin ops nginx-conf","text":"<p>Generate an nginx.conf to serve the GUI and proxy pass to API container.</p> <p>Usage:</p> <pre><code>nomad admin ops nginx-conf [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --prefix TEXT           Alter the url path prefix.\n  --host TEXT             Alter the NOMAD app host.\n  --port TEXT             Alter the NOMAD port host.\n  --server / --no-server  Control writing of the outer server {} block. Useful\n                          when conf file is included within another\n                          nginx.conf.\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-prototypes-update","title":"nomad admin ops prototypes-update","text":"<p>Updates the AFLOW prototype information using the latest online version and writes the results to a python module in the given FILEPATH.</p> <p>Usage:</p> <pre><code>nomad admin ops prototypes-update [OPTIONS] FILEPATH\n</code></pre> <p>Options:</p> <pre><code>  --matches-only  Only update the match information that depends on the\n                  symmetry analysis settings. Will not perform an online\n                  update.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-restore","title":"nomad admin ops restore","text":"<p>Restore the mongo db.</p> <p>Usage:</p> <pre><code>nomad admin ops restore [OPTIONS] PATH_TO_DUMP\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-ops-springer-update","title":"nomad admin ops springer-update","text":"<p>Updates the springer database in nomad.config.normalize.springer_db_path.</p> <p>Usage:</p> <pre><code>nomad admin ops springer-update [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --max-n-query INTEGER  Number of unsuccessful springer request before\n                         returning an error. Default is 10.\n  --retry-time INTEGER   Time in seconds to retry after unsuccessful request.\n                         Default is 120.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-reset","title":"nomad admin reset","text":"<p>Reset/remove all databases.</p> <p>Usage:</p> <pre><code>nomad admin reset [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --remove            Do not just reset all dbs, but also remove them.\n  --i-am-really-sure  Must be set for the command to to anything.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-reset-processing","title":"nomad admin reset-processing","text":"<p>Reset all uploads and entries \"stuck\" in processing using level mongodb operations.</p> <p>Usage:</p> <pre><code>nomad admin reset-processing [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --zero-complete-time  Sets the complete time to epoch zero.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-rewrite-doi-urls","title":"nomad admin rewrite-doi-urls","text":"<p>Rewrites the existing dataset URLs in existing DOI records with freshly generated dataset URLs. This is useful, if the URL layout has changed.</p> <p>Usage:</p> <pre><code>nomad admin rewrite-doi-urls [OPTIONS] [DOIS]...\n</code></pre> <p>Options:</p> <pre><code>  --dry                         Just test if DOI exists and print is current\n                                URL.\n  --save-existing-records TEXT  A filename to store the existing DOI records\n                                in.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run","title":"nomad admin run","text":"<p>Run a nomad service locally (outside docker).</p> <p>Usage:</p> <pre><code>nomad admin run [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>app: Run the nomad development app with all apis.</li> <li>appworker: Run both app and worker.</li> <li>hub: Run the jupyter hub.</li> <li>worker: Run the nomad development worker.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-run-app","title":"nomad admin run app","text":"<p>Run the nomad development app with all apis.</p> <p>Usage:</p> <pre><code>nomad admin run app [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --with-gui         The app will configure the gui for production and service\n                     it.\n  --host TEXT        Passed as host parameter.\n  --port INTEGER     Passed as port parameter.\n  --log-config TEXT  Passed as log-config parameter.\n  --gunicorn         Run app with gunicorn instead of uvicorn.\n  --workers INTEGER  Passed to uvicorn workers parameter.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run-appworker","title":"nomad admin run appworker","text":"<p>Run both app and worker.</p> <p>Usage:</p> <pre><code>nomad admin run appworker [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --app-host TEXT            Passed as app host parameter.\n  --app-port INTEGER         Passed as app port parameter.\n  --fastapi-workers INTEGER  Number of FastAPI workers.\n  --celery-workers INTEGER   Number of Celery workers.\n  --dev                      Use one worker (for dev. env.).\n  --help                     Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run-hub","title":"nomad admin run hub","text":"<p>Run the jupyter hub.</p> <p>Usage:</p> <pre><code>nomad admin run hub [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-run-worker","title":"nomad admin run worker","text":"<p>Run the nomad development worker.</p> <p>Usage:</p> <pre><code>nomad admin run worker [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --workers INTEGER  Number of celery workers.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-upgrade","title":"nomad admin upgrade","text":"<p>Commands for upgrading to a newer NOMAD version</p> <p>Usage:</p> <pre><code>nomad admin upgrade [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>migrate-mongo: Converts (upgrades) records from one mongodb and migrates to another.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-upgrade-migrate-mongo","title":"nomad admin upgrade migrate-mongo","text":"<p>Converts (upgrades) records from one mongodb and migrates to another. Note, it is strongly recommended to run this command with loglevel verbose, i.e.</p> <pre><code>nomad -v upgrade migrate-mongo ...\n</code></pre> <p>Usage:</p> <pre><code>nomad admin upgrade migrate-mongo [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --host TEXT                     The mongodb host. By default same as the\n                                  configured db.\n  --port INTEGER                  The mongodb port. By default same as the\n                                  configured db.\n  --src-db-name TEXT              The name of the source database.  [required]\n  --dst-db-name TEXT              The name of the destination database. By\n                                  default same as the configured db.\n  --upload-query TEXT             An mongo upload query. All uploads matching\n                                  the query will be included in the migration.\n  --entry-query TEXT              An mongo entry query. All uploads with an\n                                  entry matching the query will be included in\n                                  the migration.\n  --ids-from-file TEXT            Reads upload IDs from the specified file.\n                                  Cannot be used together with the --upload-\n                                  query or --entry-query options. This can for\n                                  example be used to retry just the uploads\n                                  that has previously failed (as these ids can\n                                  be exported to file using --failed-ids-to-\n                                  file). You can specify both --ids-from-file\n                                  and --failed-ids-to-file at the same time\n                                  with the same file name.\n  --failed-ids-to-file TEXT       Write the IDs of failed and skipped uploads\n                                  to the specified file. This can for example\n                                  be used to subsequently retry just the\n                                  uploads that failed (as these ids can be\n                                  loaded from file using --ids-from-file). You\n                                  can specify both --ids-from-file and\n                                  --failed-ids-to-file at the same time with\n                                  the same file name.\n  --upload-update TEXT            json with updates to apply to all converted\n                                  uploads\n  --entry-update TEXT             json with updates to apply to all converted\n                                  entries\n  --overwrite [always|if-newer|never]\n                                  If an upload already exists in the\n                                  destination db, this option determines\n                                  whether it and its child records should be\n                                  overwritten with the data from the source\n                                  db. Possible values are \"always\", \"if-\n                                  newer\", \"never\". Selecting \"always\" always\n                                  overwrites, \"never\" never overwrites, and\n                                  \"if-newer\" overwrites if the upload either\n                                  doesn't exist in the destination, or it\n                                  exists but its complete_time (i.e. last time\n                                  it was processed) is older than in the\n                                  source db.\n  --fix-problems                  If a minor, fixable problem is encountered,\n                                  fixes it automaticall; otherwise fail.\n  --dry                           Dry run (not writing anything to the\n                                  destination database).\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads","title":"nomad admin uploads","text":"<p>Upload related commands</p> <p>Usage:</p> <pre><code>nomad admin uploads [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --uploads-mongo-query TEXT      A query\n  --entries-mongo-query TEXT      A query\n  --entries-es-query TEXT         A query\n  --unpublished                   Select only uploads in staging\n  --published                     Select only uploads that are publised\n  --outdated                      Select published uploads with older nomad\n                                  version\n  --processing                    Select only processing uploads\n  --processing-failure-uploads    Select uploads with failed processing\n  --processing-failure-entries    Select uploads with entries with failed\n                                  processing\n  --processing-failure            Select uploads where the upload or any entry\n                                  has failed processing\n  --processing-incomplete-uploads\n                                  Select uploads that have not yet been\n                                  processed\n  --processing-incomplete-entries\n                                  Select uploads where any entry has net yot\n                                  been processed\n  --processing-incomplete         Select uploads where the upload or any entry\n                                  has not yet been processed\n  --processing-necessary          Select uploads where the upload or any entry\n                                  has either not been processed or processing\n                                  has failed in the past\n  --unindexed                     Select uploads that have no entries in the\n                                  elastic search index.\n  --help                          Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>chown: Change the owner of the upload and all its entries.</li> <li>convert-archive: Convert selected uploads to the new format.</li> <li>export: List selected uploads</li> <li>export-bundle: Export one or more uploads as bundles.</li> <li>import-bundle: Import one or more uploads from bundles. Unless specified by the user,</li> <li>index: (Re-)index all entries of the given uploads.</li> <li>integrity: Check certain integrity criteria and return a list of upload IDs.</li> <li>ls: List selected uploads</li> <li>process: Reprocess selected uploads.</li> <li>publish: Publish selected uploads.</li> <li>re-pack: Repack selected uploads.</li> <li>reset: Reset the processing state.</li> <li>rm: Delete selected upload</li> <li>stop: Attempt to abort the processing of uploads.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-uploads-chown","title":"nomad admin uploads chown","text":"<p>Change the owner of the upload and all its entries.</p> <p>Usage:</p> <pre><code>nomad admin uploads chown [OPTIONS] USERNAME [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-convert-archive","title":"nomad admin uploads convert-archive","text":"<p>Convert selected uploads to the new format.</p> <p>Usage:</p> <pre><code>nomad admin uploads convert-archive [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  -o, --overwrite           Overwrite existing target files.\n  -d, --delete-old          Delete old archives once successfully converted.\n  -m, --migrate             Only convert v1 archive files to v1.2 archive\n                            files.\n  -f, --force-repack        Force repacking existing archives that are already\n                            in the new format\n  -p, --parallel INTEGER    Number of processes to use for conversion. Default\n                            is os.cpu_count().\n  -s, --size-limit INTEGER  Only handle archives under limited size in GB.\n                            Default is -1 (no limit).\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-export","title":"nomad admin uploads export","text":"<p>List selected uploads</p> <p>Usage:</p> <pre><code>nomad admin uploads export [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --required TEXT    The required in JSON format\n  -o, --output TEXT  The file to write data to\n  --help             Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-export-bundle","title":"nomad admin uploads export-bundle","text":"<p>Export one or more uploads as bundles.</p> <p>Usage:</p> <pre><code>nomad admin uploads export-bundle [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --out-dir TEXT       Output folder. Default value is \"./bundles\" (defined in\n                       config)\n  --uncompressed       Specify to export each bundle as an uncompressed\n                       folder, instead of a zip-file.\n  --overwrite          Specify to, for each bundle, overwrite the destination\n                       file/folder if it already exists.\n  -s, --settings TEXT  The export settings, specified as json. Settings not\n                       specified in the dictionary will be set to the default\n                       values.\n  -i, --ignore-errors  Specify to ignore errors on individual uploads, and\n                       continue exporting (the default behaviour is to abort\n                       on first failing upload).\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-import-bundle","title":"nomad admin uploads import-bundle","text":"<p>Import one or more uploads from bundles. Unless specified by the user, the configured default import settings are used.</p> <p>Usage:</p> <pre><code>nomad admin uploads import-bundle [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --in TEXT                     The input path, specifying a bundle or a\n                                folder containing multiple bundles.\n  -m, --multi                   Specify this flag if the input_path is a\n                                folder containing multiple bundles, and all\n                                these should be imported. If this option is\n                                specified without specifying --in, we will\n                                default the input path to ./bundles\n  -s, --settings TEXT           The import settings, specified as json.\n                                Settings not specified in the dictionary will\n                                be set to the default values.\n  -e, --embargo_length INTEGER  The embargo length (0-36 months). 0 means no\n                                embargo. If unspecified, the embargo period\n                                defined in the bundle will be used.\n  -c, --use-celery              If specified, uses celery and the worker pool\n                                to do the main part of the import. NOTE: this\n                                requires that the workers can access the\n                                bundle via the exact same path.\n  -i, --ignore-errors           Specify this flag to ignore errors on\n                                individual bundles, and continue importing\n                                (the default behaviour is to abort on first\n                                failing bundle).\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-index","title":"nomad admin uploads index","text":"<p>(Re-)index all entries of the given uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads index [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --parallel INTEGER        Use the given amount of parallel processes.\n                            Default is 1.\n  --transformer TEXT        Qualified name to a Python function that should be\n                            applied to each EntryMetadata.\n  --skip-materials          Only update the entries index.\n  --print-progress INTEGER  Prints a dot every given seconds. Can be used to\n                            keep terminal open that have an i/o-based timeout.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-integrity","title":"nomad admin uploads integrity","text":"<p>Check certain integrity criteria and return a list of upload IDs.</p> <p>Usage:</p> <pre><code>nomad admin uploads integrity [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --both-storages           Select uploads that have both staging and public\n                            versions.\n  --missing-storage         Select uploads of which the corresponding raw\n                            folder (for staging) or the raw zip archive (for\n                            public) is missing in the file system. This only\n                            checks the existence of folder/archive uploaded by\n                            user. To check the contents, use the --missing-\n                            raw-files flag.\n  --missing-raw-files       Select uploads that any of the files listed in\n                            metadata/files is missing. Use --check-all-entries\n                            to check files for all entries in the upload. It\n                            uses the indexed ES data and does not open the\n                            msgpack archive files.\n  --missing-archive-files   Select uploads that miss archive (msgpack) files.\n  --missing-index           Select uploads of which the ES index information\n                            is missing.\n  --entry-mismatch          Select uploads that have different numbers of\n                            entries in mongo and ES.\n  --nomad-version-mismatch  Select uploads that have different nomad versions\n                            in archive and ES.\n  --old-archive-format      Select uploads that are using the old archive\n                            format (v1).\n  --not-preferred-suffix    Select uploads that are using the preferred\n                            (first) suffix in the configuration.\n  --check-all-entries       Check all entries in the upload, otherwise only\n                            check one entry per upload.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-ls","title":"nomad admin uploads ls","text":"<p>List selected uploads</p> <p>Usage:</p> <pre><code>nomad admin uploads ls [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  -e, --entries   Include details about upload entries in the output.\n  --ids           Only include the upload ids in the output.\n  --json          Output a JSON array instead of a tabulated list.\n  --size INTEGER  Controls the maximum size of returned uploads, use -1 to\n                  return all.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-process","title":"nomad admin uploads process","text":"<p>Reprocess selected uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads process [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --parallel INTEGER        Use the given amount of parallel processes.\n                            Default is 1.\n  --process-running         Also reprocess already running processes.\n  --setting TEXT            key=value to overwrite a default reprocess config\n                            setting.\n  --print-progress INTEGER  Prints a dot every given seconds. Can be used to\n                            keep terminal open that have an i/o-based timeout.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-publish","title":"nomad admin uploads publish","text":"<p>Publish selected uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads publish [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --parallel INTEGER        Use the given amount of parallel processes.\n                            Default is 1.\n  --embargo-length INTEGER  Use an embargo length (months) for the\n                            publication.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-re-pack","title":"nomad admin uploads re-pack","text":"<p>Repack selected uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads re-pack [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-reset","title":"nomad admin uploads reset","text":"<p>Reset the processing state.</p> <p>Usage:</p> <pre><code>nomad admin uploads reset [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --with-entries  Also reset all entries.\n  --success       Set the process status to success instead of pending\n  --failure       Set the process status to failure instead of pending.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-rm","title":"nomad admin uploads rm","text":"<p>Delete selected upload</p> <p>Usage:</p> <pre><code>nomad admin uploads rm [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --skip-es     Keep the elastic index version of the data.\n  --skip-mongo  Keep uploads and entries in mongo.\n  --skip-files  Keep all related files.\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-uploads-stop","title":"nomad admin uploads stop","text":"<p>Attempt to abort the processing of uploads.</p> <p>Usage:</p> <pre><code>nomad admin uploads stop [OPTIONS] [UPLOADS]...\n</code></pre> <p>Options:</p> <pre><code>  --entries    Only stop entries processing.\n  --kill       Use the kill signal and force task failure.\n  --no-celery  Do not attempt to stop the actual celery tasks\n  --help       Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-admin-users","title":"nomad admin users","text":"<p>Add, import, export users.</p> <p>Usage:</p> <pre><code>nomad admin users [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>import: Import users to keycloak from a JSON file.</li> </ul>"},{"location":"reference/cli.html#nomad-admin-users-import","title":"nomad admin users import","text":"<p>Import users to keycloak from a JSON file.</p> <p>Usage:</p> <pre><code>nomad admin users import [OPTIONS] PATH_TO_USERS_FILE\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-clean","title":"nomad clean","text":"<p>Cleanse the given path by removing empty folders.</p> <p>Usage:</p> <pre><code>nomad clean [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --path TEXT  Cleanse the given path by removing empty folders.\n  --help       Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client","title":"nomad client","text":"<p>Commands that use the nomad API to do useful things</p> <p>Usage:</p> <pre><code>nomad client [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -n, --url TEXT       The URL where nomad is running, default is\n                       \"http://nomad-lab.eu/prod/v1/api\".\n  -u, --user TEXT      the user name to login, default is no login.\n  -w, --password TEXT  the password used to login.\n  --token-via-api      retrieve the access token from the api not keycloak.\n  --no-ssl-verify      disables SSL verificaton when talking to nomad.\n  --help               Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>datatests: Metainfo compatibility tests against data in a NOMAD installation.</li> <li>integrationtests: Runs a few example operations as a test.</li> <li>local: Run processing locally.</li> <li>synchdb: Synchronizes the NOMAD database with the given external database.</li> <li>upload: Upload files to nomad. The given path can be a single file or a directory. For file(s) that are not .zip or .tar files, a temporary .zip file will be created and uploaded.</li> </ul>"},{"location":"reference/cli.html#nomad-client-datatests","title":"nomad client datatests","text":"<p>Metainfo compatibility tests against data in a NOMAD installation.</p> <p>Usage:</p> <pre><code>nomad client datatests [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-integrationtests","title":"nomad client integrationtests","text":"<p>Runs a few example operations as a test.</p> <p>Usage:</p> <pre><code>nomad client integrationtests [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --skip-parsers  Skip extensive upload and parser tests.\n  --skip-publish  Skip publish the upload. Should not be done on an production\n                  environment.\n  --skip-doi      Skip assigning a doi to a dataset.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-local","title":"nomad client local","text":"<p>Run processing locally.</p> <p>Usage:</p> <pre><code>nomad client local [OPTIONS] ENTRY_ID\n</code></pre> <p>Options:</p> <pre><code>  --override                     Override existing local entry data.\n  --show-archive / --no-archive  Whether to print out the archive data into\n                                 stdout.\n  --show-metadata                Print the extracted repo metadata.\n  --skip-normalizers             Do not normalize.\n  --not-strict                   Also match artificial parsers.\n  --help                         Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-synchdb","title":"nomad client synchdb","text":"<p>Synchronizes the NOMAD database with the given external database.</p> <p>Usage:</p> <pre><code>nomad client synchdb [OPTIONS] DB_NAME ROOT_URL\n</code></pre> <p>Options:</p> <pre><code>  --outfile TEXT      File to read/write files missing in NOMAD database\n  --nomadfile TEXT    File to read/write files in NOMAD database\n  --dbfile TEXT       File to read/write files in given database\n  --local_path TEXT   Directory to which the files will be downloaded\n  --parallel INTEGER  Number of processes to spawn to download/upload files\n  --do-download       Flag to automatically download downloaded files\n  --do-upload         Flag to automatically upload downloaded files\n  --do-publish        Flag to automatically publish upload\n  --cleanup           Flag to clean up downloaded files\n  --help              Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-client-upload","title":"nomad client upload","text":"<p>Upload files to nomad. The given path can be a single file or a directory. For file(s) that are not .zip or .tar files, a temporary .zip file will be created and uploaded.</p> <p>Usage:</p> <pre><code>nomad client upload [OPTIONS] PATH...\n</code></pre> <p>Options:</p> <pre><code>  --upload-name TEXT    Optional name for the upload of a single file. Will be\n                        ignored on directories.\n  --local-path          Upload files \"offline\": files will not be uploaded,\n                        but processed were they are. Only works when run on\n                        the nomad host.\n  --publish             Automatically move upload out of the staging area\n                        after successful processing\n  --ignore-path-prefix  Ignores common path prefixes when creating an upload.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev","title":"nomad dev","text":"<p>Commands related to the nomad source code.</p> <p>Usage:</p> <pre><code>nomad dev [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>api-model: Export an API model in JSON schema.</li> <li>example-data: Adds a few pieces of data to NOMAD.</li> <li>gui-artifacts: Generates all python-based GUI artifacts into javascript code.</li> <li>gui-config: Generates the GUI development config JS file based on NOMAD config.</li> <li>gui-env: Generates the GUI development .env file based on NOMAD config.</li> <li>gui-qa: Runs tests and linting of the nomad gui source code. Useful before committing code.</li> <li>metainfo: Generates a JSON with all metainfo.</li> <li>parser-metadata: Generates a JSON file that compiles all the parser metadata from each parser project.</li> <li>qa: Runs tests and linting of the nomad python source code. Useful before committing code.</li> <li>search-quantities: Generates a JSON with all search quantities.</li> <li>update-parser-readmes: Updates parser<code>s README files by combining a general template with  a parser</code>s metadata YAML file.</li> </ul>"},{"location":"reference/cli.html#nomad-dev-api-model","title":"nomad dev api-model","text":"<p>Export an API model in JSON schema.</p> <p>Usage:</p> <pre><code>nomad dev api-model [OPTIONS] MODEL\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-example-data","title":"nomad dev example-data","text":"<p>Adds a few pieces of data to NOMAD.</p> <p>Usage:</p> <pre><code>nomad dev example-data [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -u, --username TEXT  The main author username.\n  --help               Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-artifacts","title":"nomad dev gui-artifacts","text":"<p>Generates all python-based GUI artifacts into javascript code.</p> <p>Usage:</p> <pre><code>nomad dev gui-artifacts [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-config","title":"nomad dev gui-config","text":"<p>Generates the GUI development config JS file based on NOMAD config.</p> <p>Usage:</p> <pre><code>nomad dev gui-config [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-env","title":"nomad dev gui-env","text":"<p>Generates the GUI development .env file based on NOMAD config.</p> <p>Usage:</p> <pre><code>nomad dev gui-env [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-gui-qa","title":"nomad dev gui-qa","text":"<p>Runs tests and linting of the nomad gui source code. Useful before committing code.</p> <p>Usage:</p> <pre><code>nomad dev gui-qa [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --skip-tests  Do no tests, just do code checks.\n  --help        Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-metainfo","title":"nomad dev metainfo","text":"<p>Generates a JSON with all metainfo.</p> <p>Usage:</p> <pre><code>nomad dev metainfo [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-parser-metadata","title":"nomad dev parser-metadata","text":"<p>Generates a JSON file that compiles all the parser metadata from each parser project.</p> <p>Usage:</p> <pre><code>nomad dev parser-metadata [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-qa","title":"nomad dev qa","text":"<p>Runs tests and linting of the nomad python source code. Useful before committing code.</p> <p>Usage:</p> <pre><code>nomad dev qa [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --skip-tests     Do no tests, just do code checks.\n  -x, --exitfirst  Stop testing after first failed test case.\n  --help           Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-search-quantities","title":"nomad dev search-quantities","text":"<p>Generates a JSON with all search quantities.</p> <p>Usage:</p> <pre><code>nomad dev search-quantities [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-dev-update-parser-readmes","title":"nomad dev update-parser-readmes","text":"<p>Updates parser<code>s README files by combining a general template with  a parser</code>s metadata YAML file.</p> <p>Usage:</p> <pre><code>nomad dev update-parser-readmes [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --parser TEXT  Only updated the README of the given parsers subdirctory.\n  --help         Show this message and exit.\n</code></pre>"},{"location":"reference/cli.html#nomad-parse","title":"nomad parse","text":"<p>Run parsing and normalizing locally.</p> <p>Usage:</p> <pre><code>nomad parse [OPTIONS] MAINFILE\n</code></pre> <p>Options:</p> <pre><code>  --show-archive / --no-archive  Whether to print out the archive data into\n                                 stdout.\n  --archive-with-meta            If the archvie is printed, it would include\n                                 metadata like m_def and m_annotations.\n  --show-metadata                Print the extracted repo metadata.\n  --preview-plots                Preview generated plots\n  --skip-normalizers             Do not run the normalizer.\n  --not-strict                   Do also match artificial parsers.\n  --parser TEXT                  Skip matching and use the provided parser\n                                 (format: `parsers/&lt;name&gt;`). Additional\n                                 selection rules still apply for parsers with\n                                 multiple main files.\n  --server-context               Whether to use server context.\n  --username TEXT                Username for authentication.\n  --password TEXT                Password for authentication.\n  --save-plot-dir DIRECTORY      Directory to save the plot\n  --help                         Show this message and exit.\n</code></pre>"},{"location":"reference/code_guidelines.html","title":"Code guidelines","text":"<p>NOMAD has a long history and many people are involved in its development. These guidelines are set out to keep the code quality high and consistent. Please read them carefully.</p>"},{"location":"reference/code_guidelines.html#principles-and-rules","title":"Principles and rules","text":"<ul> <li> <p>simple first, complicated only when necessary</p> </li> <li> <p>search and adopt generic established 3rd-party solutions before implementing specific   solutions</p> </li> <li> <p>only unidirectional dependencies between components/modules, no circles</p> </li> <li> <p>only one language: Python (except GUI of course)</p> </li> </ul> <p>The are some rules or better strong guidelines for writing code. The following applies to all Python code (and where applicable, also to Javascript and other code):</p> <ul> <li> <p>Use an IDE (e.g. VS Code) or otherwise automatically   enforce   code formatting and linting.</p> </li> <li> <p>Use <code>nomad qa</code> before committing. This will run all tests, static type checks, linting,   etc.</p> </li> <li> <p>Test the public interface of each submodule (i.e. Python file).</p> </li> <li> <p>There is a style guide to Python. Write   PEP 8-compliant Python code. An exception   is the line cap at 79, which can be broken but keep it 90-ish.</p> </li> <li> <p>Be Pythonic and watch   this talk about best practices.</p> </li> <li> <p>Add docstrings to the public interface of each submodule (i.e. Python file). This   includes APIs that are exposed to other submodules (i.e. other Python files).</p> </li> <li> <p>The project structure follows   this guide. Keep it!</p> </li> <li> <p>Write tests for all contributions.</p> </li> <li> <p>Adopt Clean Code practices. Here is a good   introductory talk to Clean Code.</p> </li> </ul>"},{"location":"reference/code_guidelines.html#enforcing-rules-with-cicd","title":"Enforcing rules with CI/CD","text":"<p>These guidelines are partially enforced by CI/CD. As part of CI all tests are run on all branches; further we run a linter, PEP 8 checker, and mypy (static type checker). You can run <code>nomad qa</code> to run all these tests and checks before committing.</p> <p>See the contributing guide for more details on how to work with issues, branches, merge requests, and CI/CD.</p>"},{"location":"reference/code_guidelines.html#documenting-code","title":"Documenting code","text":"<p>Write Clean Code that is easy to comprehend.</p> <p>However, you should document the whole publicly exposed interface of a module. For Python this includes most classes and functions that you will write, for React its exported components and their props.</p> <p>For all functionality that is exposed to clients (APIs, CLI, schema base classes and annotations, UI functionality), you must consider to add explanations, tutorials, and examples to the documentation system. This is built with mkdocs and published as part of each NOMAD installation. Additionally, you can refer to the <code>src/nomad_docs</code> folder to see how the documentation is generated, and to the <code>mkdocs.yaml</code> file to understand its structure. </p> <p>To document Python functions and classes, use Google docstrings. Use Markdown if you need to add markup but try to reduce this to a minimum. You can use VS Code plugins like autoDocstring to help. Always use single quotes, pad single-line docstrings with spaces and start multi-line ones on a new line. Here are a few examples:</p> <pre><code>def generate_uuid() -&gt; str:\n    '''Generates a base64 encoded Version 4 unique identifier. '''\n\n    return base64.encode(uuid4())\n\ndef add(a: float, b: float) -&gt; float:\n    '''\n    Adds two numbers.\n\n    Args:\n      a (float): One number.\n      b (float): The other number.\n\n    Returns:\n      float: The sum of a and b.\n    '''\n\n    return a + b\n</code></pre> <p>The only reason to comment individual lines is because there is absolutely no way to write it simple enough. The typical scenarios are:</p> <ul> <li> <p>workarounds to known issues with used dependencies</p> </li> <li> <p>complex interactions between seemingly unrelated pieces of code that cannot be resolved   otherwise</p> </li> <li> <p>code that has to be cumbersome due to performance optimizations</p> </li> </ul> <p>Do not comment out code. We have Git for that.</p>"},{"location":"reference/code_guidelines.html#names-and-identifiers","title":"Names and identifiers","text":"<p>There is a certain terminology consistently used in this documentation and the source code. Use this terminology for identifiers.</p> <p>Do not use abbreviations. There are (few) exceptions: <code>proc</code> (processing), <code>exc</code> or <code>e</code> (exception), <code>calc</code> (calculation), <code>repo</code> (repository), <code>utils</code> (utilities), and <code>aux</code> (auxiliary). Other exceptions are <code>f</code> for file-like streams and <code>i</code> for index running variables, although the latter is almost never necessary in Python.</p> <p>Terms:</p> <ul> <li> <p>upload: A logical unit that comprises a collection of files uploaded by a user,   organized in a directory structure.</p> </li> <li> <p>entry: An archive item, created by parsing a mainfile. Each entry belongs to an   upload and is associated with various metadata (an upload may have many entries).</p> </li> <li> <p>child entry: Some parsers generate multiple entries -- a main entry plus some number   of child entries. Child entries are identified by the mainfile plus a mainfile_key   (string value).</p> </li> <li> <p>calculation: denotes the results of either a theoretical computation created by CMS   code, or an experiment.</p> </li> <li> <p>raw file: A user uploaded file, located somewhere in the upload's directory structure.</p> </li> <li> <p>mainfile: A raw file identified as parsable, defining an entry of the upload in   question.</p> </li> <li> <p>aux file: Additional files within an upload.</p> </li> <li> <p>entry metadata: Some quantities of an entry that are searchable in NOMAD.</p> </li> <li> <p>archive data: The normalized data of an entry in NOMAD's Metainfo-based format.</p> </li> </ul> <p>Throughout NOMAD, we use different ids. If something is called id, it is usually a random uuid and has no semantic connection to the entity it identifies. If something is called a hash then it is a hash generated based on the entity it identifies. This means either the whole thing or just some properties of this entities.</p> <ul> <li> <p>The most common hash is the <code>entry_hash</code> based on <code>mainfile</code> and aux file contents.</p> </li> <li> <p>The <code>upload_id</code> is a UUID assigned to the upload on creation. It never changes.</p> </li> <li> <p>The <code>mainfile</code> is a path within an upload that points to a file identified as parsable.   This also uniquely identifies an entry within the upload.</p> </li> <li> <p>The <code>entry_id</code> (previously called <code>calc_id</code>) uniquely identifies an entry. It is a hash   over the <code>mainfile</code> and respective <code>upload_id</code>. NOTE: For backward compatibility,   <code>calc_id</code> is also still supported in the API, but using it is strongly discouraged.</p> </li> <li> <p>We often use pairs of <code>upload_id/entry_id</code>, which in many contexts allow to resolve an   entry-related file on the filesystem without having to ask a database about it.</p> </li> <li> <p>The <code>pid</code> or (<code>coe_calc_id</code>) is a legacy sequential integer id, previously used to   identify entries. We still store the <code>pid</code> on these older entries for historical   purposes.</p> </li> <li> <p>Calculation <code>handle</code> or <code>handle_id</code> are created based on those <code>pid</code>.   To create hashes we use <code>nomad.utils.hash</code>.</p> </li> </ul>"},{"location":"reference/code_guidelines.html#logging","title":"Logging","text":"<p>There are three important prerequisites to understand about nomad-FAIRDI's logging:</p> <ul> <li> <p>All log entries are recorded in a central Elasticsearch database. To make this database   useful, log entries must be sensible in size, frequency, meaning, level, and logger   name. Therefore, we need to follow some rules when it comes to logging.</p> </li> <li> <p>We use a structured logging approach. Instead of encoding all kinds of information   in log messages, we use key-value pairs that provide context to a log event. In the   end, all entries are stored as JSON dictionaries with <code>@timestamp</code>, <code>level</code>,   <code>logger_name</code>, <code>event</code> plus custom context data. Keep events very short, most   information goes into the context.</p> </li> <li> <p>We use logging to inform about the state of nomad-FAIRDI, not about user behavior,   input, or data. Do not confuse this when determining the log level for an event.   For example, a user providing an invalid upload file should never be an error.</p> </li> </ul> <p>Please follow the following rules when logging:</p> <ul> <li> <p>If a logger is not already provided, only use <code>nomad.utils.get_logger</code> to   acquire a new logger. Never use the built-in logging directly. These loggers work like   the system loggers, but allow you to pass keyword arguments with additional context   data. See also the structlog docs.</p> </li> <li> <p>In many context, a logger is already provided (e.g. API, processing, parser,   normalizer). This provided logger has already context information bounded. So it is   important to use those instead of acquiring your own loggers. Have a look for methods   called <code>get_logger</code> or attributes called <code>logger</code>.</p> </li> <li> <p>Keep events (what usually is called message) very short. Examples are:   file uploaded, extraction failed, etc.</p> </li> <li> <p>Structure the keys for context information. When you analyze logs in ELK, you will   see that the set of all keys over all log entries can be quite large. Structure your   keys to make navigation easier. Use keys like <code>nomad.proc.parser_version</code> instead of   <code>parser_version</code>. Use module names as prefixes.</p> </li> <li> <p>Don't log everything. Try to anticipate how you would use the logs in case of bugs,   error scenarios, etc.</p> </li> <li> <p>Don't log sensitive data.</p> </li> <li> <p>Think before logging data (especially dicts, list, NumPy arrays, etc.).</p> </li> <li> <p>Logs should not be abused as a printf-style debugging tool.</p> </li> </ul> <p>The following keys are used in the final logs that are piped to Logstash. Notice that the key name is automatically formed by a separate formatter and may differ from the one used in the actual log call.</p> <p>Keys that are autogenerated for all logs:</p> <ul> <li><code>@timestamp</code>: Timestamp for the log</li> <li><code>@version</code>: Version of the logger</li> <li><code>host</code>: Host name from which the log originated</li> <li><code>path</code>: Path of the module from which the log was created</li> <li><code>tags</code>: Tags for this log</li> <li><code>type</code>: message_type as set in the LogstashFormatter</li> <li><code>level</code>: Log level: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code></li> <li><code>logger_name</code>: Name of the logger</li> <li><code>nomad.service</code>: Service name as configured in <code>config.py</code></li> <li><code>nomad.release</code>: Release name as configured in <code>config.py</code></li> </ul> <p>Keys that are present for events related to processing an entry:</p> <ul> <li><code>nomad.upload_id</code>: id of the currently processed upload</li> <li><code>nomad.entry_id</code>: id of the currently processed entry</li> <li><code>nomad.mainfile</code>: mainfile of the currently processed entry</li> </ul> <p>Keys that are present for events related to exceptions:</p> <ul> <li> <p><code>exc_info</code>: Stores the full Python exception that was encountered. All uncaught   exceptions will be stored automatically here.</p> </li> <li> <p><code>digest</code>: If an exception was raised, the last 256 characters of the message are stored   automatically into this key. If you wish to search for exceptions in   Kibana, you will want to use this value as it will   be indexed unlike the full exception object.</p> </li> </ul>"},{"location":"reference/code_guidelines.html#copyright-notices","title":"Copyright notices","text":"<p>We follow this recommendation of the Linux Foundation for the copyright notice that is placed on top of each source code file.</p> <p>It is intended to provide a broad generic statement that allows all authors/contributors of the NOMAD project to claim their copyright, independent of their organization or individual ownership.</p> <p>You can simply copy the notice from another file. From time to time we can use a tool like licenseheaders to ensure correct notices. In addition we keep a purely informative AUTHORS file.</p>"},{"location":"reference/code_guidelines.html#git-submodules-and-other-in-house-dependencies","title":"Git submodules and other \"in-house\" dependencies","text":"<p>As the NOMAD ecosystem grows, you might develop libraries that are used by NOMAD instead of being part of its main codebase. The same guidelines should apply. You can use GitHub Actions if your library is hosted on Github to ensure automated linting and tests.</p>"},{"location":"reference/config.html","title":"Configuration","text":""},{"location":"reference/config.html#introduction","title":"Introduction","text":"<p>Many aspects of NOMAD and its operation can be modified through configuration. Most configuration items have reasonable defaults and typically only a small subset has to be overwritten.</p> <p>Configuration items get their value in the following order:</p> <ol> <li>The item is read from the environment. This has the highest priority and will overwrite values in a <code>nomad.yaml</code> file or the NOMAD source-code.</li> <li>The value is given in a <code>nomad.yaml</code> configuration file.</li> <li>There is no custom value, and the value hard-coded in the NOMAD sources will be used.</li> </ol> <p>Configuration items are structured. The configuration is hierarchical and items are aggregated in potentially nested section. For example the configuration item <code>services.api_host</code> denotes the attribute <code>api_host</code> in the configuration section <code>services</code>.</p>"},{"location":"reference/config.html#setting-values-from-the-environment","title":"Setting values from the environment","text":"<p>NOMAD services will look at the environment. All environment variables starting with <code>NOMAD_</code> are considered. The rest of the name is interpreted as a configuration item. Sections and attributes are concatenated with a <code>_</code>. For example, the environment variable <code>NOMAD_SERVICES_API_HOST</code> will set the value for the <code>api_host</code> attribute in the <code>services</code> section.</p>"},{"location":"reference/config.html#setting-values-from-a-nomadyaml","title":"Setting values from a <code>nomad.yaml</code>","text":"<p>NOMAD services will look for a <code>nomad.yaml</code> file. By default, they will look in the current working directory. This location can be overwritten with the <code>NOMAD_CONFIG</code> environment variable.</p> <p>The configuration sections and attributes can be denoted with YAML objects and attributes. Here is an example <code>nomad.yaml</code> file: <pre><code>services:\n  api_host: 'localhost'\n  api_base_path: '/nomad-oasis'\n\noasis:\n  is_oasis: true\n  uses_central_user_management: true\n\nnorth:\n  jupyterhub_crypt_key: '978bfb2e13a8448a253c629d8dd84ff89587f30e635b753153960930cad9d36d'\n\nmeta:\n  deployment: 'oasis'\n  deployment_url: 'https://my-oasis.org/api'\n  maintainer_email: 'me@my-oasis.org'\n\nlogtransfer:\n  enabled: false\n\nmongo:\n    db_name: nomad_oasis_v1\n\nelastic:\n    entries_index: nomad_oasis_entries_v1\n    materials_index: nomad_oasis_materials_v1\n</code></pre></p> <p>When overwriting an object in the configuration, the new value will be merged with the default value. The new merged object will have all of the attributes of the new object in addition to any old attributes that were not overwritten. This allows you to simply change an individual setting without having to provide the entire structure again, which simplifies customization that happens deep in the configuration hierarchy. When overwriting anything else (numbers, strings, lists etc.) the new value completely replaces the old one.</p>"},{"location":"reference/config.html#user-interface-customization","title":"User interface customization","text":"<p>Many of the UI options use a data model that contains the following three fields: <code>include</code>, <code>exclude</code> and <code>options</code>. This structure allows you to easily disable, enable, reorder and modify the UI layout with minimal config rewrite. Here are examples of common customization tasks using the search columns as an example:</p> <p>Disable item: <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          exclude: ['upload_create_time']\n</code></pre></p> <p>Explicitly select the shown items and their order <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          include: ['entry_id', 'upload_create_time']\n</code></pre></p> <p>Modify existing option <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          options:\n            upload_create_time:\n              label: \"Uploaded\"\n</code></pre></p> <p>Add a new item that does not yet exist in options. Note that by default all options are shown in the order they have been declared unless the order is explicitly given in <code>include</code>. <pre><code>ui:\n  apps:\n    options:\n      entries:\n        columns:\n          options:\n            upload_id:\n              label: \"Upload ID\"\n</code></pre></p> <p>The following is a reference of all configuration sections and attributes.</p>"},{"location":"reference/config.html#services","title":"Services","text":""},{"location":"reference/config.html#services_1","title":"services","text":"<p>Contains basic configuration of the NOMAD services (app, worker, north).</p> name type api_host <code>str</code> The external hostname that clients can use to reach this NOMAD installation.default: <code>localhost</code> api_port <code>str | int</code> The port used to expose the NOMAD app and api to clients.default: <code>8000</code> api_base_path <code>str</code> The base path prefix for the NOMAD app and api.default: <code>/fairdi/nomad/latest</code> api_secret <code>str</code> A secret that is used to issue download and other tokens.default: <code>defaultApiSecret</code> api_timeout <code>int</code> If the NOMAD app is run with gunicorn as process manager, this timeout (in s) is passed and worker processes will be restarted, if they do not respond in time.default: <code>600</code> https <code>bool</code> Set to <code>True</code>, if external clients are using SSL to connect to this installation. Requires to setup a reverse-proxy (e.g. the one used in the docker-compose based installation) that handles the SSL encryption.default: <code>False</code> https_upload <code>bool</code> Set to <code>True</code>, if upload curl commands should suggest the use of SSL for file uploads. This can be configured independently of <code>https</code> to suggest large file via regular HTTP.default: <code>False</code> admin_user_id <code>str</code> The admin user <code>user_id</code>. All users are treated the same; there are no particular authorization information attached to user accounts. However, the API will grant the user with the given <code>user_id</code> more rights, e.g. using the <code>admin</code> owner setting in accessing data.default: <code>00000000-0000-0000-0000-000000000000</code> encyclopedia_base <code>str</code> This enables links to the given encyclopedia installation in the UI.default: <code>https://nomad-lab.eu/prod/rae/encyclopedia/#</code> optimade_enabled <code>bool</code> If true, the app will serve the optimade API.default: <code>True</code> dcat_enabled <code>bool</code> If true the app will serve the DCAT API.default: <code>True</code> h5grove_enabled <code>bool</code> If true the app will serve the h5grove API.default: <code>True</code> console_log_level <code>int | str</code> The log level that controls console logging for all NOMAD services (app, worker, north). The level is given in Python <code>logging</code> log level numbers.default: <code>30</code> upload_limit <code>int</code> The maximum allowed unpublished uploads per user. If a user exceeds this amount, the user cannot add more uploads.default: <code>10</code> force_raw_file_decoding <code>bool</code> By default, text raw-files are interpreted with utf-8 encoding. If this fails, the actual encoding is guessed. With this setting, we force to assume iso-8859-1 encoding, if a file is not decodable with utf-8.default: <code>False</code> max_entry_download <code>int</code> There is an inherent limit in page-based pagination with Elasticsearch. If you increased this limit with your Elasticsearch, you can also adopt this setting accordingly, changing the maximum amount of entries that can be paginated with page-base pagination.Page-after-value-based pagination is independent and can be used without limitations.default: <code>50000</code> max_entry_metadata_download <code>int</code> The maximum amount of entries metadata that can be downloaded.default: <code>100000</code> unavailable_value <code>str</code> Value that is used in <code>results</code> section Enum fields (e.g. system type, spacegroup, etc.) to indicate that the value could not be determined.default: <code>unavailable</code> app_token_max_expires_in <code>int</code> Maximum expiration time for an app token in seconds. Requests with a higher value will be declined.default: <code>2592000</code> html_resource_http_max_age <code>int</code> Used for the max_age cache-control directive on statically served html, js, css resources.default: <code>60</code> image_resource_http_max_age <code>int</code> Used for the max_age cache-control directive on statically served image resources.default: <code>2592000</code> upload_members_group_search_enabled <code>bool</code> If true, the GUI will show a search for groups as upload members.default: <code>True</code> log_api_queries <code>bool</code> If true, all queries to the /entries/query API endpoint will be logged.default: <code>True</code>"},{"location":"reference/config.html#meta","title":"meta","text":"<p>Metadata about the deployment and how it is presented to clients.</p> name type version <code>str</code> The NOMAD version string.default: <code>1.3.16rc2</code> commit <code>str</code> The source-code commit that this installation's NOMAD version is build from.default: \"\" deployment <code>str</code> Human-friendly name of this nomad deployment.default: <code>devel</code> deployment_url <code>str</code> The NOMAD deployment's url. If not explicitly set, will default to the (api url) read from the configuration. label <code>str</code> An additional log-stash data key-value pair added to all logs. Can be used to differentiate deployments when analyzing logs. service <code>str</code> Name for the service that is added to all logs. Depending on how NOMAD is installed, services get a name (app, worker, north) automatically.default: <code>unknown nomad service</code> name <code>str</code> Web-site title for the NOMAD UI.default: <code>NOMAD</code>deprecated homepage <code>str</code> Provider homepage.default: <code>https://nomad-lab.eu</code>deprecated source_url <code>str</code> URL of the NOMAD source-code repository.default: <code>https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR</code>deprecated maintainer_email <code>str</code> Email of the NOMAD deployment maintainer.default: <code>markus.scheidgen@physik.hu-berlin.de</code> beta <code>dict</code> Additional data that describes how the deployment is labeled as a beta-version in the UI.default: Complex object, default value not displayed."},{"location":"reference/config.html#oasis","title":"oasis","text":"<p>Settings related to the configuration of a NOMAD Oasis deployment.</p> name type is_oasis <code>bool</code> Set to <code>True</code> to indicate that this deployment is a NOMAD Oasis.default: <code>False</code> allowed_users <code>list[str]</code> A list of usernames or user account emails. These represent a white-list of allowed users. With this, users will need to login right-away and only the listed users might use this deployment. All API requests must have authentication information as well. uses_central_user_management <code>bool</code> Set to True to use the central user-management. Typically the NOMAD backend is using the configured <code>keycloak</code> to access user data. With this, the backend will use the API of the central NOMAD (<code>central_nomad_deployment_url</code>) instead.default: <code>False</code> central_nomad_deployment_url <code>str</code> The URL of the API of the NOMAD deployment that is considered the central NOMAD.default: <code>https://nomad-lab.eu/prod/v1/api</code>"},{"location":"reference/config.html#north","title":"north","text":"<p>Settings related to the operation of the NOMAD remote tools hub service north.</p> name type hub_ip <code>str</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation.default: <code>0.0.0.0</code> hub_port <code>int | str</code> The internal port that NOMAD services use to connect to the jupyterhub API.default: <code>9000</code> windows <code>bool</code> Enable windows OS hacks.default: <code>True</code> nomad_access_token_expiry_time <code>int</code> All tools are run with an access token for the NOMAD api in the NOMAD_CLIENT_ACCESS_TOKEN environment variable. This token will be automatically used by the nomad-lab Python package, e.g. if you use the ArchiveQuery to access data. This option sets the amount of seconds that this token is valid for.default: <code>86400</code> tools <code>NORTHTools</code> The available north tools. Either the tools definitions as dict or a path to a .json file.default: Complex object, default value not displayed. hub_service_api_token <code>str</code> A secret token shared between NOMAD and the NORTH jupyterhub. This needs to be the token of an admin service.default: <code>secret-token</code> enabled <code>bool | None</code> Enables or disables the NORTH API and UI views. This is independent of whether you run a jupyter hub or not.default: <code>True</code> hub_connect_ip <code>str | None</code> Overwrites the default hostname that can be used from within a north container to reach the host system.Typically has to be set for non Linux hosts. Set this to <code>host.docker.internal</code> on windows/macos. hub_connect_url <code>str | None</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation. docker_network <code>str | None</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation. hub_host <code>str | None</code> The internal host name that NOMAD services use to connect to the jupyterhub API.default: <code>localhost</code> jupyterhub_crypt_key <code>str | None</code> This setting is forwarded to jupyterhub; refer to the jupyterhub documentation. nomad_host <code>str | None</code> The NOMAD app host name that spawned containers use."},{"location":"reference/config.html#northtools","title":"NORTHTools","text":"name type options <code>dict[str, NORTHTool]</code> The available plugin.default: Complex object, default value not displayed. include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include."},{"location":"reference/config.html#northtool","title":"NORTHTool","text":"name type image_pull_policy <code>str</code> The image pull policy used in k8s deployments.default: <code>Always</code> privileged <code>bool</code> Whether the tool needs to run in privileged mode.default: <code>False</code> with_path <code>bool</code> Whether the tool supports a path to a file or directory. This also enables tools to be launched from files in the NOMAD UI.default: <code>False</code> file_extensions <code>list[str]</code> The file extensions of files that this tool should be launchable for.default: <code>[]</code> maintainer <code>list[NORTHToolMaintainer]</code> The maintainers of the tool.default: <code>[]</code> external_mounts <code>list[NORTHExternalMount]</code> Additional mounts to be added to tool containers.default: <code>[]</code> short_description <code>str | None</code> A short description of the tool, e.g. shown in the NOMAD GUI. description <code>str | None</code> A description of the tool, e.g. shown in the NOMAD GUI. image <code>str | None</code> The docker image (incl. tags) to use for the tool. cmd <code>str | None</code> The container cmd that is passed to the spawner. default_url <code>str | None</code> An optional path prefix that is added to the container URL to reach the tool, e.g. \"/lab\" for jupyterlab. path_prefix <code>str | None</code> An optional path prefix that is added to the container URL to reach the files, e.g. \"lab/tree\" for jupyterlab. mount_path <code>str | None</code> The path in the container where uploads and work directories will be mounted, e.g. /home/jovyan for Jupyter containers. icon <code>str | None</code> A URL to an icon that is used to represent the tool in the NOMAD UI."},{"location":"reference/config.html#northtoolmaintainer","title":"NORTHToolMaintainer","text":"name type name <code>str</code> default: <code>PydanticUndefined</code> email <code>str</code> default: <code>PydanticUndefined</code>"},{"location":"reference/config.html#northexternalmount","title":"NORTHExternalMount","text":"name type host_path <code>str</code> default: <code>PydanticUndefined</code> bind <code>str</code> default: <code>PydanticUndefined</code> mode <code>str</code> default: <code>ReadMode.ro</code>options: - <code>ro</code> - <code>rw</code>"},{"location":"reference/config.html#files-databases-external-services","title":"Files, databases, external services","text":""},{"location":"reference/config.html#rabbitmq","title":"rabbitmq","text":"<p>Configures how NOMAD is connecting to RabbitMQ.</p> name type host <code>str</code> The name of the host that runs RabbitMQ.default: <code>localhost</code> user <code>str</code> The RabbitMQ user that is used to connect.default: <code>rabbitmq</code> password <code>str</code> The password that is used to connect.default: <code>rabbitmq</code>"},{"location":"reference/config.html#fs","title":"fs","text":"name type tmp <code>str</code> default: <code>.volumes/fs/tmp</code> staging <code>str</code> default: <code>.volumes/fs/staging</code> staging_external <code>str</code> public <code>str</code> default: <code>.volumes/fs/public</code> public_external <code>str</code> north_home <code>str</code> default: <code>.volumes/fs/north/users</code> north_home_external <code>str</code> local_tmp <code>str</code> default: <code>/tmp</code> prefix_size <code>int</code> default: <code>2</code> archive_version_suffix <code>str | list[str]</code> This allows to add an additional segment to the names of archive files and thereby allows different NOMAD installations to work with the same storage directories and raw files, but with separate archives.If this is a list, the first string is used. If the file with the first string does not exist on read, the system will look for the file with the next string, etc.default: <code>['v1.2', 'v1']</code> working_directory <code>str</code> default: <code>/home/runner/work/nomad-docs/nomad-docs</code> external_working_directory <code>str</code>"},{"location":"reference/config.html#elastic","title":"elastic","text":"name type username <code>str</code> default: \"\" password <code>str</code> default: \"\" host <code>str</code> default: <code>localhost</code> port <code>int</code> default: <code>9200</code> timeout <code>int</code> default: <code>60</code> bulk_timeout <code>int</code> default: <code>600</code> bulk_size <code>int</code> default: <code>1000</code> entries_per_material_cap <code>int</code> default: <code>1000</code> entries_index <code>str</code> default: <code>nomad_entries_v1</code> materials_index <code>str</code> default: <code>nomad_materials_v1</code>"},{"location":"reference/config.html#keycloak","title":"keycloak","text":"name type server_url <code>str</code> default: <code>https://nomad-lab.eu/fairdi/keycloak/auth/</code> public_server_url <code>str</code> realm_name <code>str</code> default: <code>fairdi_nomad_prod</code> username <code>str</code> default: <code>admin</code> password <code>str</code> default: <code>password</code> client_id <code>str</code> default: <code>nomad_public</code> client_secret <code>str</code>"},{"location":"reference/config.html#mongo","title":"mongo","text":"<p>Connection and usage settings for MongoDB.</p> name type host <code>str</code> The name of the host that runs mongodb.default: <code>localhost</code> port <code>int</code> The port to connect with mongodb.default: <code>27017</code> db_name <code>str</code> The used mongodb database name.default: <code>nomad_v1</code> username <code>str | None</code> password <code>str | None</code>"},{"location":"reference/config.html#logstash","title":"logstash","text":"name type enabled <code>bool</code> default: <code>False</code> host <code>str</code> default: <code>localhost</code> tcp_port <code>str</code> default: <code>5000</code> level <code>int | str</code> default: <code>10</code>"},{"location":"reference/config.html#mail","title":"mail","text":"name type enabled <code>bool</code> default: <code>False</code> with_login <code>bool</code> default: <code>False</code> host <code>str</code> default: \"\" port <code>int</code> default: <code>8995</code> user <code>str</code> default: \"\" password <code>str</code> default: \"\" from_address <code>str</code> default: <code>support@nomad-lab.eu</code> cc_address <code>str | None</code>"},{"location":"reference/config.html#datacite","title":"datacite","text":"name type mds_host <code>str</code> default: <code>https://mds.datacite.org</code> enabled <code>bool</code> default: <code>False</code> prefix <code>str</code> default: <code>10.17172</code> user <code>str</code> default: <code>*</code> password <code>str</code> default: <code>*</code>"},{"location":"reference/config.html#rfc3161_timestamp","title":"rfc3161_timestamp","text":"name type server <code>str</code> The rfc3161ng timestamping host.default: <code>http://zeitstempel.dfn.de</code> cert <code>str</code> Path to the optional rfc3161ng timestamping server certificate. hash_algorithm <code>str</code> Hash algorithm used by the rfc3161ng timestamping server.default: <code>sha256</code> username <code>str</code> password <code>str</code>"},{"location":"reference/config.html#processing","title":"Processing","text":""},{"location":"reference/config.html#celery","title":"celery","text":"name type max_memory <code>float</code> default: <code>64000000.0</code> timeout <code>int</code> default: <code>1800</code> acks_late <code>bool</code> default: <code>False</code> routing <code>str</code> default: <code>queue</code> priorities <code>dict[str, int]</code> default: Complex object, default value not displayed."},{"location":"reference/config.html#normalize","title":"normalize","text":"name type normalizers <code>Options</code> default: Complex object, default value not displayed. system_classification_with_clusters_threshold <code>float</code> The system size limit for running the dimensionality analysis. For very large systems the dimensionality analysis will get too expensive.default: <code>64</code> clustering_size_limit <code>float</code> The system size limit for running the system clustering. For very large systems the clustering will get too expensive.default: <code>600</code> symmetry_tolerance <code>float</code> Symmetry tolerance controls the precision used by spglib in order to find symmetries. The atoms are allowed to move this much from their symmetry positions in order for spglib to still detect symmetries. The unit is angstroms. The value of 0.1 is used e.g. by Materials Project according to https://pymatgen.org/pymatgen.symmetry.html#pymatgen.symmetry.analyzer.SpacegroupAnalyzerdefault: <code>0.1</code> prototype_symmetry_tolerance <code>float</code> The symmetry tolerance used in aflow prototype matching. Should only be changed before re-running the prototype detection.default: <code>0.1</code> max_2d_single_cell_size <code>float</code> Maximum number of atoms in the single cell of a 2D material for it to be considered valid.default: <code>7</code> cluster_threshold <code>float</code> The distance tolerance between atoms for grouping them into the same cluster. Used in detecting system type.default: <code>2.5</code> angle_rounding <code>float</code> Defines the \"bin size\" for rounding cell angles for the material hash in degree.default: <code>10.0</code> flat_dim_threshold <code>float</code> The threshold for a system to be considered \"flat\". Used e.g. when determining if a 2D structure is purely 2-dimensional to allow extra rigid transformations that are improper in 3D but proper in 2D.default: <code>0.1</code> k_space_precision <code>float</code> The threshold for point equality in k-space. Unit: 1/m.default: <code>150000000.0</code> band_structure_energy_tolerance <code>float</code> The energy threshold for how much a band can be on top or below the fermi level in order to still detect a gap. Unit: Joule.default: <code>8.01088e-21</code> springer_db_path <code>str | None</code> default: <code>/home/runner/work/nomad-docs/nomad-docs/.venv/lib/python3.12/site-packages/nomad/config/models/normalizing/data/springer.msg</code>"},{"location":"reference/config.html#options","title":"Options","text":"<p>Common configuration class used for enabling/disabling certain elements and defining the configuration of each element.</p> name type include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include. options <code>dict[str, Any] | None</code> Contains the available options.default: Complex object, default value not displayed."},{"location":"reference/config.html#process","title":"process","text":"name type store_package_definition_in_mongo <code>bool</code> default: <code>False</code> add_definition_id_to_reference <code>bool</code> default: <code>False</code> write_definition_id_to_archive <code>bool</code> default: <code>False</code> index_materials <code>bool</code> default: <code>True</code> reuse_parser <code>bool</code> default: <code>True</code> metadata_file_name <code>str</code> default: <code>nomad</code> metadata_file_extensions <code>tuple</code> default: <code>('json', 'yaml', 'yml')</code> auxfile_cutoff <code>int</code> default: <code>100</code> parser_matching_size <code>int</code> default: <code>12000</code> max_upload_size <code>int</code> default: <code>34359738368</code> use_empty_parsers <code>bool</code> default: <code>False</code> redirect_stdouts <code>bool</code> True will redirect lines to stdout (e.g. print output) that occur during processing (e.g. created by parsers or normalizers) as log entries.default: <code>False</code> rfc3161_skip_published <code>bool</code> default: <code>False</code>"},{"location":"reference/config.html#reprocess","title":"reprocess","text":"name type rematch_published <code>bool</code> default: <code>True</code> reprocess_existing_entries <code>bool</code> default: <code>True</code> use_original_parser <code>bool</code> default: <code>False</code> add_matched_entries_to_published <code>bool</code> default: <code>True</code> delete_unmatched_published_entries <code>bool</code> default: <code>False</code> index_individual_entries <code>bool</code> default: <code>False</code>"},{"location":"reference/config.html#bundle_export","title":"bundle_export","text":"<p>Controls behaviour related to exporting bundles.</p> name type default_cli_bundle_export_path <code>str</code> Default path used when exporting bundles using the CLI command.default: <code>./bundles</code> default_settings <code>BundleExportSettings</code> General default settings.default: Complex object, default value not displayed. default_settings_cli <code>BundleExportSettings</code> Additional default settings, applied when exporting using the CLI. This allows to override some of the settings specified in the general default settings above."},{"location":"reference/config.html#bundleexportsettings","title":"BundleExportSettings","text":"name type include_raw_files <code>bool</code> If the raw files should be included in the exportdefault: <code>True</code> include_archive_files <code>bool</code> If the parsed archive files should be included in the exportdefault: <code>True</code> include_datasets <code>bool</code> If the datasets should be included in the exportdefault: <code>True</code>"},{"location":"reference/config.html#bundle_import","title":"bundle_import","text":"<p>Controls behaviour related to importing bundles.</p> name type required_nomad_version <code>str</code> Minimum  NOMAD version of bundles required for import.default: <code>1.1.2</code> default_cli_bundle_import_path <code>str</code> Default path used when importing bundles using the CLI command.default: <code>./bundles</code> allow_bundles_from_oasis <code>bool</code> If oasis admins can \"push\" bundles to this NOMAD deployment.default: <code>False</code> allow_unpublished_bundles_from_oasis <code>bool</code> If oasis admins can \"push\" bundles of unpublished uploads.default: <code>False</code> default_settings <code>BundleImportSettings</code> General default settings.default: Complex object, default value not displayed. default_settings_cli <code>BundleImportSettings</code> Additional default settings, applied when importing using the CLI. This allows to override some of the settings specified in the general default settings above.default: Complex object, default value not displayed."},{"location":"reference/config.html#bundleimportsettings","title":"BundleImportSettings","text":"name type include_raw_files <code>bool</code> If the raw files should be included in the importdefault: <code>True</code> include_archive_files <code>bool</code> If the parsed archive files should be included in the importdefault: <code>True</code> include_datasets <code>bool</code> If the datasets should be included in the importdefault: <code>True</code> include_bundle_info <code>bool</code> If the bundle_info.json file should be kept (not necessary but may be nice to have.default: <code>True</code> keep_original_timestamps <code>bool</code> If all timestamps (create time, publish time etc) should be imported from the bundle.default: <code>False</code> set_from_oasis <code>bool</code> If the from_oasis flag and oasis_deployment_url should be set.default: <code>True</code> delete_upload_on_fail <code>bool</code> If False, it is just removed from the ES index on failure.default: <code>False</code> delete_bundle_on_fail <code>bool</code> Deletes the source bundle if the import fails.default: <code>True</code> delete_bundle_on_success <code>bool</code> Deletes the source bundle if the import succeeds.default: <code>True</code> delete_bundle_include_parent_folder <code>bool</code> When deleting the bundle, also include parent folder, if empty.default: <code>True</code> trigger_processing <code>bool</code> If the upload should be processed when the import is done (not recommended).default: <code>False</code> process_settings <code>Reprocess</code> When trigger_processing is set to True, these settings control the reprocessing behaviour (see the config for <code>reprocess</code> for more info). NOTE: reprocessing is no longer the recommended method to import bundles.default: Complex object, default value not displayed."},{"location":"reference/config.html#reprocess_1","title":"Reprocess","text":"name type rematch_published <code>bool</code> default: <code>True</code> reprocess_existing_entries <code>bool</code> default: <code>True</code> use_original_parser <code>bool</code> default: <code>False</code> add_matched_entries_to_published <code>bool</code> default: <code>True</code> delete_unmatched_published_entries <code>bool</code> default: <code>False</code> index_individual_entries <code>bool</code> default: <code>False</code>"},{"location":"reference/config.html#archive","title":"archive","text":"name type block_size <code>int</code> In case of using blocked TOC, this is the size of each block.default: <code>1048576</code> read_buffer_size <code>int</code> GPFS needs at least 256K to achieve decent performance.default: <code>1048576</code> copy_chunk_size <code>int</code> The chunk size of every read of binary data. It is used to copy data from one file to another. A small value will result in more syscalls, a large value will result in higher peak memory usage.default: <code>16777216</code> toc_depth <code>int</code> Depths of table of contents in the archive.default: <code>10</code> small_obj_optimization_threshold <code>int</code> For any child of lists/dicts whose encoded size is smaller than this value, no TOC will be generated.default: <code>1048576</code> fast_loading <code>bool</code> When enabled, this flag determines whether to read the whole dict/list at once when a certain mount of children has been visited. This reduces the number of syscalls although data may be repeatedly read. Otherwise, always read children one by one. This may slow down the loading as more syscalls are needed.default: <code>True</code> fast_loading_threshold <code>float</code> If the fraction of children that have been visited is less than this threshold, fast loading will be used.default: <code>0.6</code> trivial_size <code>int</code> To identify numerical lists.default: <code>20</code>"},{"location":"reference/config.html#user-interface","title":"User Interface","text":"<p>These settings affect the behaviour of the user interface. Note that the preferred way for creating custom apps is by using app plugin entry points.</p>"},{"location":"reference/config.html#ui","title":"ui","text":"<p>Used to customize the user interface.</p> name type app_base <code>str</code> This is automatically set. north_base <code>str</code> This is automatically set. theme <code>Theme</code> Controls the site theme and identity. unit_systems <code>UnitSystems</code> Controls the available unit systems. entry <code>Entry</code> Controls the entry visualization. apps <code>Apps</code> deprecated north <code>NORTHUI</code> NORTH (NOMAD Remote Tools Hub) UI configuration.default: Complex object, default value not displayed. example_uploads <code>ExampleUploads</code> Controls the available example uploads.default: Complex object, default value not displayed."},{"location":"reference/config.html#northui","title":"NORTHUI","text":"<p>NORTH (NOMAD Remote Tools Hub) UI configuration.</p> name type enabled <code>bool</code> Whether the NORTH tools are available in the UI. The default value is read from the root-level NORTH configuration.default: <code>True</code>"},{"location":"reference/config.html#apps","title":"Apps","text":"<p>Contains App definitions and controls their availability.</p> name type include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include. options <code>dict[str, App] | None</code> Contains the available app options."},{"location":"reference/config.html#exampleuploads","title":"ExampleUploads","text":"<p>Controls the availability of example uploads.</p> name type include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include."},{"location":"reference/config.html#theme","title":"Theme","text":"<p>Theme and identity settings.</p> name type title <code>str</code> Site name in the browser tab.default: <code>PydanticUndefined</code>"},{"location":"reference/config.html#unitsystems","title":"UnitSystems","text":"<p>Controls the available unit systems.</p> name type selected <code>str</code> Selected option.default: <code>PydanticUndefined</code> include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include. options <code>dict[str, UnitSystem] | None</code> Contains the available unit systems."},{"location":"reference/config.html#unitsystem","title":"UnitSystem","text":"name type label <code>str</code> Short, descriptive label used for this unit system.default: <code>PydanticUndefined</code> units <code>dict[str, UnitSystemUnit] | None</code> Contains a mapping from each dimension to a unit. If a unit is not        specified for a dimension, the SI equivalent will be used by default.        The following dimensions are available:         - dimensionless - length - mass - time - current - temperature - luminosity - luminous_flux - substance - angle - information - force - energy - power - pressure - charge - resistance - conductance - inductance - magnetic_flux - magnetic_field - frequency - luminance - illuminance - electric_potential - capacitance - activity"},{"location":"reference/config.html#unitsystemunit","title":"UnitSystemUnit","text":"name type definition <code>str</code> The unit definition. Can be a mathematical expression that combines several units, e.g. <code>(kg * m) / s^2</code>. You should only use units that are registered in the NOMAD unit registry (<code>nomad.units.ureg</code>).default: <code>PydanticUndefined</code> locked <code>bool | None</code> Whether the unit is locked in the unit system it is defined in.default: <code>False</code>"},{"location":"reference/config.html#entry","title":"Entry","text":"<p>Controls the entry visualization.</p> name type cards <code>Cards</code> Controls the cards that are displayed on the entry overview page.default: <code>PydanticUndefined</code>"},{"location":"reference/config.html#cards","title":"Cards","text":"<p>Contains the overview page card definitions and controls their visibility.</p> name type include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include. options <code>dict[str, Card] | None</code> Contains the available card options."},{"location":"reference/config.html#card","title":"Card","text":"<p>Definition for a card shown in the entry overview page.</p> name type error <code>str</code> The error message to show if an error is encountered within the card.default: <code>PydanticUndefined</code>"},{"location":"reference/config.html#others","title":"Others","text":""},{"location":"reference/config.html#logtransfer","title":"logtransfer","text":"<p>Configuration of logtransfer and statistics service.</p> <p>When enabled (enabled) an additional logger will write logs to a log file (log_file). At regular intervals (transfer_interval) a celery task is scheduled. It will log a set of statistics. It will copy the log file (transfer_log_files). Transfer the contents of the copy to the central NOMAD (oasis.central_nomad_deployment_url) and delete the copy. The transfer is only done if the the log file has a certain size (transfer_threshold). Only a maximum amount of logs are transferred (transfer_capacity). Only logs with a certain level (level) are considered. The files will be stored in fs.tmp.</p> name type enabled <code>bool</code> If enabled this starts process that frequently generates logs with statistics.default: <code>False</code> transfer_threshold <code>int</code> The minimum size in bytes of stored logs before logs are transferred. 0 means transfer at every transfer interval.default: <code>0</code> transfer_capacity <code>int</code> The maximum number of bytes of stored logs that are transferred. Excess is dropped.default: <code>1000000</code> transfer_interval <code>int</code> Time interval in seconds after which stored logs are potentially transferred.default: <code>600</code> level <code>int | str</code> The min log level for logs to be transferred.default: <code>20</code> log_file <code>str</code> The log file that is used to store logs for transfer.default: <code>nomad.log</code> transfer_log_file <code>str</code> The log file that is used to copy logs for transfer.default: <code>.transfer.log</code> file_rollover_wait_time <code>float</code> Time in seconds to wait after log file was \"rolled over\" for transfer.default: <code>1</code>"},{"location":"reference/config.html#tests","title":"tests","text":"name type default_timeout <code>int</code> default: <code>60</code> assume_auth_for_username <code>str</code> Will assume that all API calls with no authentication have authentication for the user with the given username."},{"location":"reference/config.html#resources","title":"resources","text":"name type enabled <code>bool</code> default: <code>False</code> db_name <code>str</code> default: <code>nomad_v1_resources</code> max_time_in_mongo <code>float</code> Maximum time a resource is stored in mongodb before being updated.default: <code>31536000.0</code> download_retries <code>int</code> Number of retries when downloading resources.default: <code>2</code> download_retry_delay <code>int</code> Delay between retries in seconds.default: <code>10</code> max_connections <code>int</code> Maximum simultaneous connections used to download resources.default: <code>10</code>"},{"location":"reference/config.html#client","title":"client","text":"name type user <code>str</code> password <code>str</code> access_token <code>str</code> url <code>str</code> default: <code>http://nomad-lab.eu/prod/v1/api</code>"},{"location":"reference/config.html#gitlab","title":"gitlab","text":"name type private_token <code>str</code> default: <code>not set</code>"},{"location":"reference/config.html#plugins","title":"plugins","text":"<p>Represent a PEP 604 union type</p> <p>E.g. for int | str</p> name type ### plugins name type ---- ---- - entry_points <code>EntryPoints</code> Used to control plugin entry points.default: <code>PydanticUndefined</code> plugin_packages <code>dict[str, PluginPackage]</code> Contains the installed installed plugin packages with the package name used as a key. This is autogenerated and should not be modified.default: <code>PydanticUndefined</code>"},{"location":"reference/config.html#pluginpackage","title":"PluginPackage","text":"name type name <code>str</code> Name of the plugin Python package, read from pyproject.toml.default: <code>PydanticUndefined</code> entry_points <code>list[str]</code> List of entry point ids contained in this package, read form pyproject.tomldefault: <code>PydanticUndefined</code> description <code>str | None</code> Package description, read from pyproject.toml. version <code>str | None</code> Plugin package version, read from pyproject.toml. homepage <code>str | None</code> Link to the plugin package homepage, read from pyproject.toml. documentation <code>str | None</code> Link to the plugin package documentation page, read from pyproject.toml. repository <code>str | None</code> Link to the plugin package source code repository, read from pyproject.toml."},{"location":"reference/config.html#entrypoints","title":"EntryPoints","text":"name type options <code>dict[str, Schema | Normalizer | Parser | SchemaPackageEntryPoint | ParserEntryPoint | NormalizerEntryPoint | AppEntryPoint | ExampleUploadEntryPoint | APIEntryPoint]</code> The available plugin entry points.default: Complex object, default value not displayed. include <code>list[str] | None</code> List of included options. If not explicitly defined, all of the options will be included by default. exclude <code>list[str] | None</code> List of excluded options. Has higher precedence than include."},{"location":"reference/config.html#parserentrypoint","title":"ParserEntryPoint","text":"<p>Base model for parser plugin entry points.</p> name type level <code>int</code> Integer that determines the execution order of this parser within an upload. Parser with lowest level will be executed first. Note that this only controls the order in which matched parsers are executed, but does not affect the order in which parsers are matched to files.default: <code>0</code> aliases <code>list[str]</code> List of alternative parser names.default: <code>[]</code> mainfile_name_re <code>str</code> A regular expression that is applied the name of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_mime_re <code>str</code> A regular expression that is applied the mime type of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_alternative <code>bool</code> If True, the parser only matches a file, if no other file in the same directory matches a parser.default: <code>False</code> supported_compressions <code>list[str]</code> Files compressed with the given formats (e.g. xz, gz) are uncompressed and matched like normal files.default: <code>[]</code> name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point. mainfile_contents_re <code>str | None</code> A regular expression that is applied the content of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches. mainfile_binary_header <code>bytes | None</code> Matches a binary file if the given bytes are included in the file. mainfile_binary_header_re <code>bytes | None</code> Matches a binary file if the given binary regular expression bytes matches the file contents. mainfile_contents_dict <code>dict | None</code> Is used to match structured data files like JSON or HDF5."},{"location":"reference/config.html#appentrypoint","title":"AppEntryPoint","text":"<p>Base model for app plugin entry points.</p> name type app <code>App</code> The app configuration.default: <code>PydanticUndefined</code> name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/config.html#schema","title":"Schema","text":"<p>A Schema describes a NOMAD Python schema that can be loaded as a plugin.</p> name type plugin_type <code>str</code> The type of the plugin. This has to be the string <code>schema</code> for schema plugins.default: <code>schema</code> name <code>str</code> A short descriptive human readable name for the plugin.default: <code>PydanticUndefined</code> python_package <code>str</code> Name of the python package that contains the plugin code and a plugin metadata file called <code>nomad_plugin.yaml</code>.default: <code>PydanticUndefined</code> id <code>str | None</code> The unique identifier for this plugin. description <code>str | None</code> A human readable description of the plugin. plugin_documentation_url <code>str | None</code> The URL to the plugins main documentation page. plugin_source_code_url <code>str | None</code> The URL of the plugins main source code repository. package_path <code>str | None</code> Path of the plugin package. Will be determined using python_package if not explicitly defined. key <code>str | None</code> Key used to identify this plugin."},{"location":"reference/config.html#apientrypoint","title":"APIEntryPoint","text":"<p>Base model for API plugin entry points.</p> name type prefix <code>str</code> The prefix for the API. The URL for the API will be the base URL of the NOMAD installation followed by this prefix. The prefix must not collide with any other API prefixes. There is no default, this field must be set. name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/config.html#normalizerentrypoint","title":"NormalizerEntryPoint","text":"<p>Base model for normalizer plugin entry points.</p> name type level <code>int</code> Integer that determines the execution order of this normalizer within the processing of an individual entry. Normalizers with the lowest level is run first.default: <code>0</code> name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/config.html#normalizer","title":"Normalizer","text":"<p>A Normalizer describes a NOMAD normalizer that can be loaded as a plugin.</p> name type plugin_type <code>str</code> The type of the plugin. This has to be the string <code>normalizer</code> for normalizer plugins.default: <code>normalizer</code> name <code>str</code> A short descriptive human readable name for the plugin.default: <code>PydanticUndefined</code> python_package <code>str</code> Name of the python package that contains the plugin code and a plugin metadata file called <code>nomad_plugin.yaml</code>.default: <code>PydanticUndefined</code> normalizer_class_name <code>str</code> The fully qualified name of the Python class that implements the normalizer. This class must have a function <code>def normalize(self, logger)</code>.default: <code>PydanticUndefined</code> id <code>str | None</code> The unique identifier for this plugin. description <code>str | None</code> A human readable description of the plugin. plugin_documentation_url <code>str | None</code> The URL to the plugins main documentation page. plugin_source_code_url <code>str | None</code> The URL of the plugins main source code repository."},{"location":"reference/config.html#parser","title":"Parser","text":"<p>A Parser describes a NOMAD parser that can be loaded as a plugin.</p> <p>The parser itself is referenced via <code>python_name</code>. For Parser instances <code>python_name</code> must refer to a Python class that has a <code>parse</code> function. The other properties are used to create a <code>MatchingParserInterface</code>. This comprises general metadata that allows users to understand what the parser is, and metadata used to decide if a given file \"matches\" the parser.</p> name type plugin_type <code>str</code> The type of the plugin. This has to be the string <code>parser</code> for parser plugins.default: <code>parser</code> name <code>str</code> A short descriptive human readable name for the plugin.default: <code>PydanticUndefined</code> python_package <code>str</code> Name of the python package that contains the plugin code and a plugin metadata file called <code>nomad_plugin.yaml</code>.default: <code>PydanticUndefined</code> parser_class_name <code>str</code> The fully qualified name of the Python class that implements the parser. This class must have a function <code>def parse(self, mainfile, archive, logger)</code>.default: <code>PydanticUndefined</code> parser_as_interface <code>bool</code> By default the parser metadata from this config (and the loaded nomad_plugin.yaml) is used to instantiate a parser interface that is lazy loading the actual parser and performs the mainfile matching. If the parser interface matching based on parser metadata is not sufficient and you implemented your own is_mainfile parser method, this setting can be used to use the given parser class directly for parsing and matching.default: <code>False</code> mainfile_name_re <code>str</code> A regular expression that is applied the name of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_mime_re <code>str</code> A regular expression that is applied the mime type of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>text/.*</code> mainfile_alternative <code>bool</code> If True, the parser only matches a file, if no other file in the same directory matches a parser.default: <code>False</code> supported_compressions <code>list[str]</code> Files compressed with the given formats (e.g. xz, gz) are uncompressed and matched like normal files.default: <code>[]</code> domain <code>str</code> The domain value <code>dft</code> will apply all normalizers for atomistic codes. Deprecated.default: <code>dft</code> level <code>int</code> The order by which the parser is executed with respect to other parsers.default: <code>0</code> id <code>str | None</code> The unique identifier for this plugin. description <code>str | None</code> A human readable description of the plugin. plugin_documentation_url <code>str | None</code> The URL to the plugins main documentation page. plugin_source_code_url <code>str | None</code> The URL of the plugins main source code repository. mainfile_contents_re <code>str | None</code> A regular expression that is applied the content of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches. mainfile_binary_header <code>bytes | None</code> Matches a binary file if the given bytes are included in the file. mainfile_binary_header_re <code>bytes | None</code> Matches a binary file if the given binary regular expression bytes matches the file contents. mainfile_contents_dict <code>dict | None</code> Is used to match structured data files like JSON, HDF5 or csv/excel files. In case of a csv/excel file for example, in order to check if certain columns exist in a given sheet, one can set this attribute to <code>'__has_all_keys': [&lt;column names&gt;]</code>. In case the csv/excel file contains comments that are supposed to be ignored, use this reserved key-value pair <code>'__has_comment': '&lt;symbol&gt;'</code> at the top level of the dictionary. Also in order to check if a certain sheet name with specific column names exist, one may set this attribute to: {'': {'__has_all_keys': []}}. Available options are: __has_key: str __has_all_keys: List[str] __has_only_keys: List[str] __has_comment: str (only for csv/xlsx files) code_name <code>str | None</code> code_homepage <code>str | None</code> code_category <code>str | None</code> metadata <code>dict | None</code> Metadata passed to the UI. Deprecated."},{"location":"reference/config.html#schemapackageentrypoint","title":"SchemaPackageEntryPoint","text":"<p>Base model for schema package plugin entry points.</p> name type name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/config.html#exampleuploadentrypoint","title":"ExampleUploadEntryPoint","text":"<p>Base model for example upload plugin entry points.</p> name type name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> Longer description of the example upload.default: <code>PydanticUndefined</code> category <code>str | None</code> Category for the example upload.default: <code>PydanticUndefined</code> title <code>str | None</code> Title of the example upload.default: <code>PydanticUndefined</code> resources <code>None | list[UploadResource | str] | UploadResource | str</code> List of data resources for this example upload. path <code>str | None</code> deprecated url <code>str | None</code> deprecated"},{"location":"reference/config.html#uploadresource","title":"UploadResource","text":"<p>Represents a request to include a certain resource into an example upload. Can point to a local folder/file, or alternatively to an online resource that can be downloaded.</p> name type path <code>str</code> Path to a file/folder within the python package (filepaths should start from the package root directory) or to an online URL.default: <code>PydanticUndefined</code> target <code>str</code> File path within the upload where the file should be stored.default: \"\""},{"location":"reference/glossary.html","title":"Glossary","text":"<p>This is a list of terms that have a specific meaning for NOMAD and are used through out the application and this documentation.</p>"},{"location":"reference/glossary.html#annotation","title":"Annotation","text":"<p>Annotations are part of data schemas and they describe aspects that are not directly defining the type or shape of data. They often allow to alter how certain data is managed, represented, or edited. See annotations in the schema documentation.</p>"},{"location":"reference/glossary.html#app","title":"App","text":"<p>Apps allow you to build customized user interfaces for specific research domains, making it easier to navigate and understand the data. This typically means that certain domain-specific properties are highlighted, different units may be used for physical properties, and specialized dashboards may be presented. This becomes crucial for NOMAD installations to be able to scale with data that contains a mixture of experiments and simulations, different techniques, and physical properties spanning different time and length scales.</p>"},{"location":"reference/glossary.html#archive","title":"Archive","text":"<p>NOMAD processes (parses and normalizes) all data. The entirety of all processed data is referred to as the Archive. Sometimes the term archive is used to refer to the processed data of a particular entry, e.g. \"the archive of that entry\".</p> <p>The term archive is an old synonym for processed data. Since the term has different meaning for different people and is very abstract, we are slowly deprecating its use in favor of processed data (e.g. NOMAD Repository and NOMAD Archive).</p>"},{"location":"reference/glossary.html#author","title":"Author","text":"<p>An author is typically a natural person that has uploaded a piece of data into NOMAD and has authorship over it. Often authors are users, but not always. Therefore, we have to distinguish between authors and users.</p>"},{"location":"reference/glossary.html#dataset","title":"Dataset","text":"<p>Users can organize entries into datasets. Datasets are not created automatically, don't confuse them with uploads. Datasets can be compared to albums, labels, or tags on other platforms. Datasets are used to reference a collection of data and users can get a DOI for their datasets.</p>"},{"location":"reference/glossary.html#deployment","title":"Deployment","text":"<p>NOMAD Deployment refers to a live instance of a NOMAD distribution running on a machine. This machine can be a cloud-based virtual machine or a local computer.</p>"},{"location":"reference/glossary.html#distribution-distro","title":"Distribution / distro","text":"<p>NOMAD Distribution is a Git repository containing the configuration for instantiating a customized NOMAD instance. Distributions define the plugins that should be installed, the configurations files (e.g. <code>nomad.yaml</code>) to use, CI pipeline steps for building final Docker images and a <code>docker-compose.yaml</code> file that can be used to launch the instance.</p>"},{"location":"reference/glossary.html#eln","title":"ELN","text":"<p>Electronic Lab Notebooks (ELNs) are a specific kind of entry in NOMAD. These entries can be edited in NOMAD, in contrast to entries that are created by uploading and processing data. ELNs offer form fields and other widgets to modify the contents of an entry. As all entries, ELNs are based on a schema; how quantities are edited (e.g. which type of widget) can be controlled through annotations.</p>"},{"location":"reference/glossary.html#entry","title":"Entry","text":"<p>Data in NOMAD is organized in entries (as in \"database entry\"). Entries have an entry id. Entries can be searched for and entries have individual pages on the NOMAD GUI. Entries are always associated with raw files, where one of these files is the mainfile. Raw files are processed to create the processed data (or the archive) for an entry.</p>"},{"location":"reference/glossary.html#example-upload","title":"Example upload","text":"<p>Example uploads are pre-prepared uploads containing data that typically showcases certain features of a plugin. The contents of example uploads can be fixed, created programmatically or fetched from online sources. Example uploads can be instantiated by using the \"Example uploads\" -button in the \"Uploads\" -page of the GUI. Example uploads can be defined by creating an example upload plugin entry point.</p>"},{"location":"reference/glossary.html#mainfile","title":"Mainfile","text":"<p>Each entry has one raw file that defines it. This is called the mainfile of that entry. Typically most, if not all, processed data of an entry is retrieved from that mainfile.</p>"},{"location":"reference/glossary.html#metadata","title":"Metadata","text":"<p>In NOMAD metadata refers to a specific technical sub-set of processed data. The metadata of an entry comprises ids, timestamps, hashes, authors, datasets, references, used schema, and other information.</p>"},{"location":"reference/glossary.html#metainfo","title":"Metainfo","text":"<p>The term metainfo refers to the sum of all schemas. In particular it is associated with all pre-defined schemas that are used to represent all processed data in a standardized way. Similar to an ontology, the metainfo provides additional meaning by associated in each piece of data with name, description, categories, type, shape, units, and more.</p>"},{"location":"reference/glossary.html#normalizer","title":"Normalizer","text":"<p>A normalizer is a small tool that can refine the processed data of an entry. Normalizers can read and modify processed data and thereby either normalize (change) some of the data or add normalized derived data. Normalizers are run after parsers and are often used to do processing steps that need to be applied to the outcome of many parsers and are therefore not part of the parsers themselves.</p> <p>There are normalizer classes and normalize functions. The normalizer classes are run after parsing in a particular order and if certain conditions are fulfilled. Normalize functions are part of schemas (i.e. section definitions). They are run at the end of processing on all the sections that instantiate the respective section definition.</p>"},{"location":"reference/glossary.html#parser","title":"Parser","text":"<p>A parser is a small program that takes a mainfile as input and produces processed data. Parsers transform information from a particular source format into NOMAD's structured schema-based format. Parsers start with a mainfile, but can open and read data from other files (e.g. those referenced in the mainfile). Typically, a parser is associated with a certain file-format and is only applied to files of that format.</p>"},{"location":"reference/glossary.html#plugin","title":"Plugin","text":"<p>NOMAD installations can be customized through plugins, which are Git repositories containing an installable python package that will add new features upon being installed. Plugins can contain one or many plugin entry points, which represent individual customizations.</p>"},{"location":"reference/glossary.html#plugin-entry-point","title":"Plugin entry point","text":"<p>Plugin entry points are used to configure and load different types of NOMAD customizations. There are several entry point types, including entry points for parsers, schema packages and apps. A single plugin may contain multiple entry points.</p>"},{"location":"reference/glossary.html#processed-data","title":"Processed data","text":"<p>NOMAD processes (parses and normalizes) all data. The processed data is the outcome of this process. Therefore, each NOMAD entry is associated with processed data that contains all the parsed and normalized information. Processed data always follows a schema. Processed data can be retrieved (via API) or downloaded as <code>.json</code> data.</p>"},{"location":"reference/glossary.html#processing","title":"Processing","text":"<p>NOMAD processes (parses and normalizes) all data. During processing, all provided files are considered. First, files are matched to parsers. Second, files that match with a parser, become mainfiles and an entry is created. Third, we run the parser to create processed data. Fourth, the processed data is further refined by running normalizers. Last, the processed data is saved and indexed. The exact processing time depends on the size of the uploaded data and users can track the processing state of each entry in the GUI.</p>"},{"location":"reference/glossary.html#quantity","title":"Quantity","text":"<p>All processed data is structured into sections and quantities. Sections provide hierarchy and organization, quantities refer to the actual pieces of data. In NOMAD, a quantity is the smallest referable unit of processed data. Quantities can have many types and shapes; examples are strings, numbers, lists, or matrices.</p> <p>In a schema, quantities are defined by their name, description, type, shape, and unit. Quantities in processed data are associated with a respective quantity definition from the respective schema.</p>"},{"location":"reference/glossary.html#raw-file","title":"Raw file","text":"<p>A raw file is any file that was provided by a NOMAD user. A raw-file might produce an entry, if it is of a supported file-format, but does not have to. Raw files always belong to an upload and might be associated with an entry (in this case, raw-files are also mainfiles).</p> <p>The sum of all raw files is also referred to as the Repository. This is an old term from when NOMAD was implemented as several services (e.g. NOMAD Repository and NOMAD Archive).</p>"},{"location":"reference/glossary.html#results-section-results","title":"Results (section <code>results</code>)","text":"<p>The results are a particular section of processed data. They comprise a summary of the most relevant data for an entry.</p> <p>While all processed data can be downloaded and is accessible via API, an entry's results (combined with its metadata) is also searchable and can be read quicker and in larger amounts.</p>"},{"location":"reference/glossary.html#schema","title":"Schema","text":"<p>Schemas define possible data structures for processed data. Like a book they organize data hierarchically in sections and subsections. Schemas are similar to ontologies as they define possible relationships between data organized within them.</p> <p>A schema is a collection of section and quantity definitions. Schemas are organized in schema packages, i.e. collections of definitions. All schemas combined form the metainfo.</p>"},{"location":"reference/glossary.html#schema-package","title":"Schema package","text":"<p>Schema packages contain a collection of schema definitions. Schema packages may be defined as YAML files or in Python as plugin entry points.</p>"},{"location":"reference/glossary.html#section-and-subsection","title":"Section and Subsection","text":"<p>All processed data is structured into sections and quantities. Sections provide hierarchy and organization, quantities refer to the actual pieces of data.</p> <p>In a schema, a section are defined by their name, description, all possible subsections, and quantities. Section definitions can also inherit all properties (subsections, quantities) from other section definitions using them as base sections.</p>"},{"location":"reference/glossary.html#upload","title":"Upload","text":"<p>NOMAD organizes raw-files (and all entries created from them) in uploads. Uploads consist of a directory structure of raw-files and a list of respective entries.</p> <p>Uploads are created by a single user, the owner. Uploads have two states. Initially, they are mutable and have limited visibility. Owners can invite other to collaborate and those users can add/remove/change data. The owner can publish an upload at some point, where the upload becomes immutable and visible to everyone. Uploads are the smallest unit of data that can be individually shared and published.</p>"},{"location":"reference/glossary.html#user","title":"User","text":"<p>A user is anyone with a NOMAD account. It is different from an author as all users can be authors, but not all authors have to be users. All data in NOMAD is always owned by a single user (others can be collaborators and co-authors).</p>"},{"location":"reference/parsers.html","title":"Supported parsers","text":"<p>This is a list of all available parsers and supported file formats:</p>"},{"location":"reference/plugins.html","title":"Plugins","text":"<p>Plugins allow one to add Python-based functionality to NOMAD without a custom NOMAD image or release. Plugins can be installed at NOMAD start-up time. Therefore, a NOMAD installation or Oasis can be configured with a different custom set of plugins or disable unnecessary plugins.</p> <p>Note</p> <p>You might also want to read the how-to guide on plugins</p>"},{"location":"reference/plugins.html#plugin-entry-point-reference","title":"Plugin entry point reference","text":"<p>This is a list of the available plugin entry point configuration models.</p>"},{"location":"reference/plugins.html#apientrypoint","title":"APIEntryPoint","text":"<p>Base model for API plugin entry points.</p> name type prefix <code>str</code> The prefix for the API. The URL for the API will be the base URL of the NOMAD installation followed by this prefix. The prefix must not collide with any other API prefixes. There is no default, this field must be set. name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/plugins.html#appentrypoint","title":"AppEntryPoint","text":"<p>Base model for app plugin entry points.</p> name type app <code>App</code> The app configuration.default: <code>PydanticUndefined</code> name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/plugins.html#exampleuploadentrypoint","title":"ExampleUploadEntryPoint","text":"<p>Base model for example upload plugin entry points.</p> name type name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> Longer description of the example upload.default: <code>PydanticUndefined</code> category <code>str | None</code> Category for the example upload.default: <code>PydanticUndefined</code> title <code>str | None</code> Title of the example upload.default: <code>PydanticUndefined</code> resources <code>None | list[UploadResource | str] | UploadResource | str</code> List of data resources for this example upload. path <code>str | None</code> deprecated url <code>str | None</code> deprecated"},{"location":"reference/plugins.html#normalizerentrypoint","title":"NormalizerEntryPoint","text":"<p>Base model for normalizer plugin entry points.</p> name type level <code>int</code> Integer that determines the execution order of this normalizer within the processing of an individual entry. Normalizers with the lowest level is run first.default: <code>0</code> name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/plugins.html#parserentrypoint","title":"ParserEntryPoint","text":"<p>Base model for parser plugin entry points.</p> name type level <code>int</code> Integer that determines the execution order of this parser within an upload. Parser with lowest level will be executed first. Note that this only controls the order in which matched parsers are executed, but does not affect the order in which parsers are matched to files.default: <code>0</code> aliases <code>list[str]</code> List of alternative parser names.default: <code>[]</code> mainfile_name_re <code>str</code> A regular expression that is applied the name of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_mime_re <code>str</code> A regular expression that is applied the mime type of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches.default: <code>.*</code> mainfile_alternative <code>bool</code> If True, the parser only matches a file, if no other file in the same directory matches a parser.default: <code>False</code> supported_compressions <code>list[str]</code> Files compressed with the given formats (e.g. xz, gz) are uncompressed and matched like normal files.default: <code>[]</code> name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point. mainfile_contents_re <code>str | None</code> A regular expression that is applied the content of a potential mainfile. If this expression is given, the parser is only considered for a file, if the expression matches. mainfile_binary_header <code>bytes | None</code> Matches a binary file if the given bytes are included in the file. mainfile_binary_header_re <code>bytes | None</code> Matches a binary file if the given binary regular expression bytes matches the file contents. mainfile_contents_dict <code>dict | None</code> Is used to match structured data files like JSON or HDF5."},{"location":"reference/plugins.html#schemapackageentrypoint","title":"SchemaPackageEntryPoint","text":"<p>Base model for schema package plugin entry points.</p> name type name <code>str | None</code> Name of the plugin entry point. description <code>str | None</code> A human readable description of the plugin entry point."},{"location":"reference/plugins.html#default-plugin-entry-points","title":"Default plugin entry points","text":"<p>This is a list of the plugin entry points that are activated by default:</p> <p>example_upload: example_uploads/1_basic_examples/1_theory, example_uploads/1_basic_examples/2_eln, example_uploads/1_basic_examples/3_tables, example_uploads/2_tutorials/1_rdm_tutorial, example_uploads/2_tutorials/2_cow_tutorial</p> <p>app: apps/1_all/1_entries, apps/2_theory/1_calculations, apps/2_theory/2_materials, apps/3_experiment/1_eln, apps/3_experiment/2_eels</p>"},{"location":"reference/tutorials.html","title":"List of NOMAD tutorials","text":"<p>Attention</p> <p>This page is still being updated with older NOMAD tutorials.</p> <ul> <li> <p>17.10.2024 FAIRmat Tutorial 15: Use of pynxtools with Examples from Optical Spectroscopy</p> <ul> <li>Demonstrates how to use the pynxtools package along with NOMAD to faciliate FAIR research data management for experimental data in materials science. Ellipsometry and Raman spectroscopy data are used as representative examples.</li> </ul> </li> <li> <p>10.07.2024 FAIRmat Tutorial 14: Developing schemas and parsers for FAIR computational data storage using NOMAD-Simulations</p> <ul> <li>Provides foundational knowledge for customizing NOMAD to fit the specific needs of your computational research project using the <code>nomad-simulations</code> schema plugin.</li> </ul> </li> <li> <p>15.05.2024 FAIRmat Tutorial 13: Getting started with NOMAD and NOMAD Oasis for research data management (RDM)</p> <ul> <li>Introduces NOMAD and NOMAD Oasis as essential tools for research data management (RDM), and demonstrates how to utilize these tools for managing experimental materials science data, with a particular focus on synthesis data.</li> </ul> </li> <li> <p>14.02.2024 FAIRmat Tutorial 12: Getting started with NOMAD and NOMAD Oasis for research data management (RDM)</p> <ul> <li>A practical session to go through a Jupyter notebook that demonstrates how to use NOMAD for managing custom data and file types. Based on a simple given dataset, we show how to model the data in a schema, do parsing and normalization, process data, access existing data with NOMAD's API for analysis, and how to add visualization to your data.</li> </ul> </li> <li> <p>25.10.2023 FAIRmat Tutorial 11: Research data management, from fundamentals to implementation</p> <ul> <li>Introduction to the FAIR data principles, Research data management practices, and data management plans.</li> </ul> </li> <li> <p>29.09.2023 CECAM workshop: An Introduction to the NOMAD repository for soft matter simulators</p> <ul> <li>Basic NOMAD usage with molecular dynamics simulations, custom workflows, python API</li> </ul> </li> <li> <p>14.06.2023 FAIRmat Tutorial 10: FAIR electronic-structure data in NOMAD</p> <ul> <li>Basic NOMAD usage for computational data, numerical precision filtering, custom workflows, knowledge-based XC functionals exploration</li> </ul> </li> <li> <p>26.04.2023 FAIRmat Tutorial 9: Plugins: Python schemas and parsers</p> <ul> <li>Introduction the new plugin mechanism in NOMAD: how to develop and integrate their own Python schemas and parsers to a NOMAD Oasis.</li> </ul> </li> <li> <p>15.03.2023 FAIRmat Tutorial 8: Using NOMAD as an Electronic lab notebook (ELN) for FAIR data</p> <ul> <li>Using NOMAD as an ELN which enables the users to generate data following the FAIR principles.</li> </ul> </li> <li> <p>15.02.2023 FAIRmat Tutorial 7: Molecular Dynamics Trajectories and Workflows in NOMAD</p> <ul> <li>Uploading MD data, examining metadata, overview page, workflow visualizer, extracting MD data for trajectory analysis</li> </ul> </li> <li> <p>06.11.2022 FAIRmat Tutorial 6: Experimental data management in NOMAD</p> <ul> <li>Follow research data lifecycle from planning and running an experiment to collecting and annotating data according to international community standards for searchability, and reuse.</li> </ul> </li> <li> <p>05.10.2022 FAIRmat Tutorial 5: NOMAD Encyclopedia</p> <ul> <li>Navigate the materials space using the Encyclopedia GUI, and using the Encyclopedia API.</li> </ul> </li> <li> <p>11.05.2022 FAIRmat Tutorial 4: NOMAD Oasis and FAIR Data Collaboration and Sharing</p> <ul> <li>How to get started with NOMAD Oasis and adapt it to your research.</li> </ul> </li> <li> <p>06.04.2022 FAIRmat Tutorial 3: Introduction to the Artificial Intelligence Toolkit</p> <ul> <li>NOMAD AI-toolkit: query over the NOMAD Archive via the NOMAD API and basic notebooks for learning AI methods.</li> </ul> </li> <li> <p>09.03.2022 FAIRmat Tutorial 2: Electronic Lab Notebooks and FAIR Data Management</p> <ul> <li>ELN in FAIR data management, NOMAD ELN, and integrating other ELN tools with NOMAD.</li> </ul> </li> <li> <p>09.02.2022 FAIRmat Tutorial 1: Publish and Explore Data with NOMAD</p> <ul> <li>Basic NOMAD guide to prepare, upload, publish data, and reference them with a DOI.</li> </ul> </li> </ul>"},{"location":"tutorial/NOMAD_ELN.html","title":"Using NOMAD as an Electronic Lab Notebook","text":"<p>In this tutorial, we will explore how to use NOMAD's Electronic Lab Notebook (ELN) functionality to record experiments effectively. You will learn how to create entries for substances and instruments, record samples along with their processing conditions, and the various measurements that make up your experiments. We will also cover NOMAD's built-in ELN templates, which help structure and interlink different aspects of an experiment, providing a clear, visual overview of the entire workflow.</p> <p>In doing this, we will apply an example of an experiment on preparing solution-processed polymer thin-films and measuring their optical absorption spectrum.</p> About the example experiment used for this exercise <p>In this exercise, we will work with an example experiment involving the preparation and characterization of Poly(3-hexylthiophene-2,5-diyl) (\"P3HT\") thin films. The experiment consists of three main activities: preparing solutions, depositing thin films, and measuring optical absorption.</p> <ol> <li> <p>Preparing solutions: The polymer powder is mixed with a solvent in predefined quantities to achieve the desired concentration. A scale is used to accurately weigh the polymer powder, ensuring precise solution concentration.</p> </li> <li> <p>Depositing thin-films: The prepared solution is used to create a thin film on a glass substrate through spin-coating. By carefully controlling the spin speed and duration, the desired film thickness is achieved.</p> </li> <li> <p>Measuring optical absorption: The optical absorption spectrum of the thin film is acquired using a UV-Vis-NIR spectrometer. The measurement results are saved as a .csv file for further analysis.</p> </li> </ol> <p>To effectively document this experiment, we will create and interlink electronic lab notebook (ELN) entries in NOMAD. These entries will include key entities such as substances, instruments, and samples, as well as activities like material processing and measurements. By structuring the data in this way, we ensure a comprehensive and FAIR-compliant record of the experiment.</p> <p></p>"},{"location":"tutorial/NOMAD_ELN.html#create-a-new-eln-upload","title":"Create a New ELN Upload","text":"<p>In NOMAD, an Electronic Lab Notebook (ELN) is created by initiating a NOMAD upload. This process allows you to structure and document your research data efficiently. For a step-by-step guide on how to create an upload, please refer to this page.</p>"},{"location":"tutorial/NOMAD_ELN.html#create-eln-entries","title":"Create ELN Entries","text":"<p>The next step is to create entries for your substances, instruments, processes, and measurements. In NOMAD, each ELN entry is structured using templates called built-in schema. These templates are specifically designed to capture relevant information for different types of entries, ensuring consistency and completeness in documentation.</p> <p>They include general fields tailored to the type of entry you are creating. The currently available ELN built-in schemas in NOMAD are illustrated in the figure below.</p> <p></p> <p>To create ELN entries using the templates provided by NOMAD, we will generate instances from the built-in schemas. This will automatically create entries with predefined fields, allowing us to efficiently fill in the relevant information of our experiment.</p> <p>Follow these steps to create ELN entries using the built-in schema: (click on the arrows to navigate between steps)</p> \u2190 \u2192"},{"location":"tutorial/NOMAD_ELN.html#create-a-substance-entry","title":"Create a Substance Entry","text":"<p>Now, let's create an entry using the built-in Substance ELN schema for P3HT powder. Follow the steps of creating an entry described above and select Substance ELN from the drop-down menu in step 4.</p> Input fields offered by the built-in schema Substance ELN <p>The built-in schema Substance ELN  provides the following fields for input:</p> <ul> <li>substance name: Automatically used as the entry name.</li> <li>tags: User selected tags to improve searchability.</li> <li>datetime: Allows entry of a date/time stamp.</li> <li>substance ID: A unique, human-readable ID for the substance.</li> <li>detailed substance description: A free text field for additional information.</li> </ul> <p>Additional subsections available in the data subsection include:</p> <ul> <li>elemental composition: Define the chemical composition with atomic and mass fractions.</li> <li>pure substance: Specify if the material is a pure substance purchased from an external vendor, with fields like:<ul> <li>Substance name</li> <li>IUPAC name</li> <li>Molecular formula</li> <li>CAS number</li> <li>Inchi Key, SMILES, and more.</li> </ul> </li> <li>substance identifier: Add identifiers for specific substances.</li> </ul> <p>Once the entry is created, we can fill in the relevant fields with detailed and accurate information. Fields can also be updated as needed to keep the entry accurate and useful.</p> <p>See the example below. (click the arrows to navigate through the steps)</p> \u2190 \u2192 Task: Create an ELN entry for substances <p>Create an ELN entry in NOMAD for the following substances:</p> <ul> <li>Chloroform</li> <li>Glass substrate</li> </ul> <p>Use the Substance ELN schema and include as many details as you like (e.g., Substance Name, Datetime, Substance ID, Description).</p>"},{"location":"tutorial/NOMAD_ELN.html#create-a-sample-entry","title":"Create a Sample Entry","text":"<p>Now, let's create an entry using the built-in Generic Sample ELN schema for P3HT Thin Film. Follow the steps of creating an entry described above and select Generic Sample ELN from the drop-down menu in step 4.</p> Input fields offered by the built-in schema Generic Sample ELN <p>The built-in schema Generic Sample ELN provides the following fields for input:</p> <ul> <li>name: Automatically used as the entry name.</li> <li>tags: User selected tags to improve searchability.</li> <li>datetime: Allows entry of a date/time stamp.</li> <li>ID: A unique, human-readable ID for the sample.</li> <li>description: A free text field for additional information.</li> </ul> <p>Additional subsections available in the data subsection include:</p> <ul> <li>elemental composition: Define the chemical composition with atomic and mass fractions.</li> <li>components: Specify the components used to create the sample, including raw materials or system components.</li> <li>sample identifier: Add unique identifiers for the sample.</li> </ul> <p>Once the entry is created, we can fill in the relevant fields with detailed and accurate information. Fields can also be updated as needed to keep the entry accurate and useful.</p> <p>See the example below. (click the arrows to navigate through the steps)</p> \u2190 \u2192 Task: Create an ELN entry for a sample <p>Create an ELN entry in NOMAD for P3HT solution in chloroform. Reference the sample to its components (P3HT powder and chloroform).</p> <p>Use the Generic Sample ELN schema and include as many details as you like (e.g., Short Name, Datetime, ID, Description).</p>"},{"location":"tutorial/NOMAD_ELN.html#create-an-instrument-entry","title":"Create an Instrument Entry","text":"<p>Now, let's create an entry using the built-in Instrument ELN schema for scale. Follow the steps of creating an entry described above and select Instrument ELN from the drop-down menu in step 4.</p> Input fields offered by the built-in schema Instrument ELN <p>The built-in schema Instrument ELN provides the following fields for input:</p> <ul> <li>name: Automatically used as the entry name.</li> <li>tags: User selected tags to improve searchability.</li> <li>datetime: Allows entry of a date/time stamp.</li> <li>ID: A unique, human-readable ID for the.</li> <li>description: A free text field for additional information.</li> </ul> <p>Additional subsections available in the data subsection include:</p> <ul> <li>instrument identifiers: Specify the type of instrument and additional metadata, if applicable.</li> </ul> <p>Once the entry is created, we can fill in the relevant fields with detailed and accurate information. Fields can also be updated as needed to keep the entry accurate and useful.</p> <p>See the example below. (click the arrows to navigate through the steps)</p> \u2190 \u2192 Task: Create an ELN entry for an instrument <p>Create an ELN entry in NOMAD for one of the following instruments:</p> <ul> <li>Optical Spectrometer</li> <li>Spin Coater</li> </ul> <p>Use the Instrument ELN schema and include as many details as you like (e.g., name, datetime, ID, description).</p>"},{"location":"tutorial/NOMAD_ELN.html#create-a-process-entry","title":"Create a Process Entry","text":"<p>Now, let's create an entry using the built-in Material Processing ELN schema for Preparation of P3HT solution. Follow the steps of creating an entry described above and select Materials Processing ELN from the drop-down menu in step 4.</p> Input fields offered by the built-in schema Material Processing ELN <p>The Material Processing ELN schema provides the following fields for input:</p> <ul> <li>name: Automatically used as the entry name.</li> <li>starting time and ending time: Allows entry of a date/time stamp for the process duration.</li> <li>tags: User selected tags to improve searchability.</li> <li>ID: A unique, human-readable ID for the process.</li> <li>location: A text field specifying the location where the process took place.</li> <li>description: A free text field for additional information about the process.</li> </ul> <p>Additional subsections available in the data subsection include:</p> <ul> <li>steps: Define the step-by-step procedure for the material processing.</li> <li>process identifier: Add unique identifiers for the process.</li> <li>instruments: List the instruments used in the process.</li> <li>samples: Specify the samples that are created or used in the process.</li> </ul> <p>Once the entry is created, we can fill in the relevant fields with detailed and accurate information. Fields can also be updated as needed to keep the entry accurate and useful.</p> <p>See the example below (click the arrows to navigate through the steps).</p> \u2190 \u2192 Task: Reference a sample to your process ELN <p>For the Process ELN entry created above, make a reference to a sample entry called P3HT_solution_in_CF.</p> <ul> <li>If the P3HT_solution_in_CF sample entry already exists, simply link to it within the samples subsection of your Process ELN entry.</li> <li>If the sample entry does not exist, first create a Sample ELN entry named P3HT_solution_in_CF, then add the reference in the Process ELN entry.</li> </ul> <p>Defining the steps of a process</p> <p>The steps subsection in the Materials Processing ELN allows us to document each stage of the process and visualize them in an interactive workflow graph.</p> <p>For the example process entry Preparation of P3HT solution, we will define the following three steps:</p> <ol> <li>weighing the powder</li> <li>filling the solvent</li> <li>mixing the solution</li> </ol> <p>Follow these steps to define the process stages in your material processing entry: (click on the arrows to navigate between steps)</p> \u2190 \u2192 <p>Note that the added information in the subsections will be used to automatically fill in the Workflow graph as tasks, as well as the References section. You can find the Workflow Graph the in OVERVIEW tab of the entry.</p> <p>The workflow graph can be modified and enriched by adding additional information such as inputs, additional tasks, and outputs for each step. You can do this in the workflow2 section.</p> <p>The workflow2 section of the Preparation of P3HT solution example can be found under the DATA tab, in the left panel under workflow2. We can now add inputs, by referencing existing substance entries.</p> <p>See the example below. click the arrows to navigate through the steps.</p> \u2190 \u2192 Task: Reference P3HT powder as input for the process <p>For the Process ELN entry created above, make reference to the substance ELN entry P3HT Powder as an input of the process.</p> <p>Tip: Use the workflow2 section of the entry.</p> <p>We can now see the changes in the workflow graph based on our modifications in the workflow section.</p>"},{"location":"tutorial/NOMAD_ELN.html#create-a-measurement-entry","title":"Create a Measurement Entry","text":"<p>Now, let's create an entry using the built-in Measurement ELN schema for Optical absorption measurement. Follow the steps of creating an entry described above and select Measurement ELN from the drop-down menu in step 4.</p> Input fields offered by the built-in schema Measurement ELN <ul> <li>name: Automatically used as the entry name.</li> <li>starting time Allows entry of a date/time stamp for the measurement.</li> <li>tags: User selected tags to improve searchability.</li> <li>ID: A unique, human-readable ID for the process.</li> <li>location: A text field specifying the location where the process took place.</li> <li>description: A free text field for additional information about the process.</li> </ul> <p>Additional subsections available in the data subsection include:</p> <ul> <li>steps: Define the step-by-step procedure for the material processing.</li> <li>samples: Specify the samples that are being measured.</li> <li>measurement identifier: Add unique identifiers for the measurement.</li> <li>instruments: List the instruments used in the measurement.</li> <li>results: Provide information about the results of the measurements (text and images).</li> </ul> <p>Once the entry is created, we can fill in the relevant fields with detailed and accurate information. Fields can also be updated as needed to keep the entry accurate and useful.</p> <p>See the example below. (click the arrows to navigate through the steps)</p> \u2190 \u2192"},{"location":"tutorial/NOMAD_ELN.html#integrate-your-experiment","title":"Integrate Your Experiment","text":"<p>Once all substances, samples, processes, and measurements are defined, you can integrate them into a structured workflow using the Experiment ELN schema. The Experiment ELN schema allows linking processes and measurements into a single entry for a comprehensive overview of your experimental workflow.</p> <p>To create an entry using the built-in Experiment ELN schema for Characterization of P3HT. Follow the steps of creating an entry described above and select Experiment ELN from the drop-down menu in step 4.</p> Input fields offered by the built-in schema Experiment ELN <ul> <li>name: Automatically used as the entry name.</li> <li>starting time Allows entry of a date/time stamp for the measurement.</li> <li>tags: User selected tags to improve searchability.</li> <li>ID: A unique, human-readable ID for the process.</li> <li>location: A text field specifying the location where the process took place.</li> <li>description: A free text field for additional information about the process.</li> </ul> <p>Additional subsections available in the data subsection include:</p> <ul> <li>steps: Define the step-by-step procedure for the material processing.</li> <li>experiment identifiers: Specify the additional metadata for the experiment.</li> </ul> <p>The steps subsection allows us to reference the various processes and measurements that were part of the experiments. By organizing these elements into a structured and interactive workflow, we can provide a clearer overview of the experimental sequence, enabling better visualization and understanding of how different steps are interconnected.</p>"},{"location":"tutorial/NOMAD_ELN.html#exploring-and-searching-your-eln","title":"Exploring and Searching Your ELN","text":"Download the example file for this exercise <p>We have prepared a compressed file for this task, which can be downloaded from this link.</p> <p>The file contains multiple NOMAD ELN entries in <code>.json</code> format.</p> <p>These entries have been created using the NOMAD ELN built-in schema, organized into folders, and categorized with custom tags.</p> <p>You can drag and drop this file into a new upload in NOMAD to view its contents.</p> <p>Imagine you have created multiple entries of substances, samples, instruments, processes, and measurements, and you need to quickly find a specific experiment or material. Instead of manually searching through files, NOMAD\u2019s ELN allows you to search, filter, and organize your entries\u2014saving you time and effort.</p> Organizing your ELN upload <p>NOMAD is a file-based system. You can access, organize, and download your files within each upload. You can also create folders to categorize entries into materials, samples, instruments, processes, and results, as well as upload additional documents, such as relevant PDFs.</p> <p>If you plan to organize your entries into separate folders, do so before you reference them to each other. Moving them afterward may break the reference links.</p> <p>You can follow these steps to organize your ELN entries:</p> <ol> <li> <p>Navigate to the FILES tab in your upload. This view functions like a file explorer, allowing you to view and manage files.  </p> </li> <li> <p>Add new folders and organize them according to your needs.  </p> </li> <li> <p>Drag and drop files into the desired folder. A prompt will appear, asking if you want to copy or move the files\u2014choose according to your needs.  </p> </li> <li> <p>Once all files are sorted, take a moment to review the structure. Here\u2019s an example of an organized ELN  </p> </li> </ol> <p>Searching your ELN entries</p> <p>To search for entries in your ELN, follow these steps:</p> <ol> <li> <p>on the top of the ELN upload page, click on the  icon.</p> <p></p> </li> <li> <p>From the drop-down menu, select Entries.</p> <p></p> <p>This will open NOMAD's EXPLORE page with a filter applied to display only the entries from your upload.</p> <p></p> </li> </ol> <p>On the EXPLORE page, you can use the filter options in the sidebar to refine your search, enter specific keywords in the search bar to find relevant entries, or create custom widgets to visualise your ELN data.</p> Filtering entries in NOMAD <p>NOMAD provides various filters that can be used to efficiently find your ELN entries, but the following two filters are particularlly effective:</p> <ul> <li> <p>Filter by built-in schema used to create the entry.</p> <p>For example, ELNInstrument, ELNSubstances, ELNSample, etc.</p> </li> <li> <p>Filter by custom tags, where you assign common tags to related entries for easy grouping.</p> <p>For example, tag all solvents as \"my_solvent\" or all samples as \"my_samples\".</p> </li> </ul> <p>Using these filters helps you quickly locate specific entries in your ELN.</p> <p>Customize your search interface with widgets</p> <p>Widgets allow you to customize your search interface to better suit your data exploration needs. By adding and rearranging widgets, you can create a personalized view that highlights the most relevant filters, metadata, or visualizations most relevant to your research.</p> Create a custom widget for ELN sections and custom tags <p>To create a custom widget for filtering your ELN, follow these steps:</p> <ol> <li>Click on the <code>+ TERMS</code> button to open the Edit terms widget menu.</li> </ol> <p> </p> <ol> <li> <p>In the Search quantity field, type eln. A list of available filters will appear.</p> </li> <li> <p>Select <code>results.eln.sections</code> from the list. This will enable filtering based on the built-in ELN sections available in your ELN upload.</p> </li> </ol> <p> </p> <ol> <li> <p>Write a descriptive title for the custom widget in Title field.</p> </li> <li> <p>Click DONE!</p> </li> </ol> <p> </p> <p>The new ELN sections widget now appears at the top of your EXPLORE page and displays ELN entry types along with their corresponding counts.</p> <p> </p> <p>You can now follow the same steps to create a custom widget for filtering by custom tags.</p> <p>In Step 3, instead of selecting <code>results.eln.sections</code>, choose <code>results.eln.tags</code>. This will create a widget that filters your ELN entries based on the custom tags you have assigned.</p> <p>This widget will then appear on your EXPLORE page, allowing you to quickly view and filter entries by their associated tags.</p> <p> </p>"},{"location":"tutorial/access_api.html","title":"Accessing data via API","text":"<p>Attention</p> <p>We are currently working to update this content.</p> <p>This video tutorial explains the basics of API and shows how to do simple requests against the NOMAD API.</p> <p>Note</p> <p>The NOMAD seen in the tutorials is an older version with a different color theme, but all the demonstrated functionality is still available on the current version. You'll find the NOMAD test installation mentioned in the first video here.</p>"},{"location":"tutorial/custom.html","title":"Creating custom schemas","text":""},{"location":"tutorial/custom.html#what-is-a-custom-schema","title":"What is a custom schema","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p> <p>An example of custom schema written in YAML language.</p> <pre><code>definitions:\n  name: 'My test ELN'\n  sections:\n    MySection:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n      m_annotations:\n        eln:\n      quantities:\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n</code></pre>"},{"location":"tutorial/custom.html#the-base-sections","title":"The base sections","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"tutorial/custom.html#use-of-yaml-files","title":"Use of YAML files","text":"<p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"tutorial/custom.html#the-built-in-tabular-parser","title":"The built-in tabular parser","text":"<p>NOMAD provides a standard parser to import your data from a spreadsheet file (<code>Excel</code> file with .xlsx extension) or from a CSV file (a Comma-Separated Values file with .csv extension). There are several ways to parse a tabular data file into a structured data file, depending on which structure we want to give our data. Therefore, the tabular parser can be set very flexibly, directly from the schema file through annotations. In this tutorial we will focus on most common modes of the tabular parser. A complete description of all modes is given in the Reference section. You can also follow the dedicated How To to see practical examples of the NOMAD tabular parser, in each section you can find a commented sample schema with a step-by-step guide on how to set it to obtain the desired final structure of your parsed data. We will make use of the tabular parser in a custom yaml schema. To obtain some structured data in NOMAD with this parser:</p> <p>1) the schema files should follow the NOMAD archive files naming convention (i.e. <code>.archive.json</code> or <code>.archive.yaml</code> extension) 2) a data file must be instantiated from the schema file</p> <p>3) a tabular data file must be dragged in the annotated quantity in order for NOMAD to parse it (the quantity is called <code>data_file</code> in the following examples)</p>"},{"location":"tutorial/custom.html#to-be-an-entry-or-not-to-be-an-entry","title":"To be an Entry or not to be an Entry","text":"<p>To use this parser, three kinds of annotation must be included in the schema: <code>tabular</code>, <code>tabular_parser</code>, <code>label_quantity</code>. Refer to the dedicated Reference section for the full list of options.</p> <p>important</p> <p>The ranges of the three <code>mapping_options</code>, namely <code>file_mode</code>, <code>mapping_mode</code>, and <code>sections</code> can give rise to twelve different combinations (see table in Reference). It is worth to analyze each of them to understand which is the best choice to pursue from case to case. Some of them give rise to \"not possible\" data structures but are still listed for completeness, a brief explanation of why it is not possible to implement them is also provided. The main bring-home message is that a tabular data file can be parsed in one or more entries in NOMAD, giving rise to diverse and arbitrarily complex structures.</p> <p>In the following sections, two examples will be illustrated. A tabular data file is parsed into one or more data archive files, their structure is based on a schema archive file. NOMAD archive files are denoted as Entries.</p> <p>Note</p> <p>From the NOMAD point of view, a schema file and a data file are the same kind of file where different sections have been filled (see archive files description). Specifically, a schema file has its <code>definitions</code> section filled while a data file will have its <code>data</code> section filled. See How to write a schema for a more complete description of an archive file.</p>"},{"location":"tutorial/custom.html#example-1","title":"Example 1","text":"<p>We want instantiate an object created from the schema already shown in the first Tutorial section and populate it with the data contained in the following excel file.</p> <p> </p> <p>The two columns in the file will be stored in a NOMAD Entry archive within two array quantities, as shown in the image below. In the case where the section to be filled is not in the root level of our schema but nested inside, it is useful to check the dedicated How-to.</p> <p> </p> <p>The schema will be decorated by the annotations mentioned at the beginning of this section  and will look like this:</p> <pre><code>definitions:\n  name: 'My test ELN'\n  sections:\n    MySection:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: column\n                  file_mode: current_entry\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_array_quantity_1:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_array_quantity_2:\n          type: str\n          shape: ['*']\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre> <p>Here the tabular data file is parsed by columns, directly within the Entry where the <code>TableData</code> is inherited and filling the quantities in the root level of the schema (see dedicated how-to to learn how to inherit tabular parser in your schema).</p> <p>Note</p> <p>In yaml files a dash character indicates a list element. <code>mapping_options</code> is a list because it is possible to parse multiple tabular sheets from the same schema with different parsing options. <code>sections</code> in turn is a list because multiple sections of the schema can be parsed with same parsing options.</p>"},{"location":"tutorial/custom.html#example-2","title":"Example 2","text":"<p>In this example, each row of the tabular data file will be placed in a new Entry that is an instance of a class defined in the schema. This would make sense for, say, an inventory spreadsheet where each row can be a separate entity such as a sample, a substrate, etc. In this case, a manyfold of Entries will be generated based on the only class available in the schema. These Entries will not be bundled together by a parent Entry but just live in our NOMAD Upload as a spare list, to bundle them together it is useful to check the dedicated How-to. They might still be referenced manually inside an overarching Entry, such as an experiment Entry, from the ELN with <code>ReferenceEditQuantity</code>.</p> <pre><code>definitions:\n  name: 'My test ELN'\n  sections:\n    MySection:\n      base_sections:\n       - nomad.datamodel.data.EntryData\n       - nomad.parsing.tabular.TableData\n      m_annotations:\n        eln:\n      more:\n        label_quantity: my_quantity_1\n      quantities:\n        data_file:\n          type: str\n          default: test.xlsx\n          m_annotations:\n            tabular_parser:\n              parsing_options:\n                comment: '#'\n              mapping_options:\n                - mapping_mode: row\n                  file_mode: multiple_new_entries\n                  sections:\n                    - '#root'\n            browser:\n              adaptor: RawFileAdaptor\n            eln:\n              component: FileEditQuantity\n        my_quantity_1:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 1\"\n        my_quantity_2:\n          type: str\n          m_annotations:\n            tabular:\n              name: \"My header 2\"\n</code></pre> <p>Attention</p> <p>This part of the documentation is still work in progress.</p>"},{"location":"tutorial/custom.html#custom-normalizers","title":"Custom normalizers","text":"<p>For custom schemas, you might want to add custom normalizers. All files are parsed and normalized when they are uploaded or changed. The NOMAD metainfo Python interface allows you to add functions that are called when your data is normalized.</p> <p>Here is an example:</p> <pre><code>from nomad.datamodel import Schema, ArchiveSection\nfrom nomad.metainfo.metainfo import Quantity, Datetime, SubSection\n\n\nclass Sample(ArchiveSection):\n    added_date = Quantity(type=Datetime)\n    formula = Quantity(type=str)\n\n    sample_id = Quantity(type=str)\n\n    def normalize(self, archive, logger):\n        super(Sample, self).normalize(archive, logger)\n\n        if self.sample_id is None:\n            self.sample_id = f'{self.added_date}--{self.formula}'\n\n\nclass SampleDatabase(Schema):\n    samples = SubSection(section=Sample, repeats=True)\n</code></pre> <p>To add a <code>normalize</code> function, your section has to inherit from <code>ArchiveSection</code> which provides the base for this functionality. Now you can overwrite the <code>normalize</code> function and add you own behavior. Make sure to call the <code>super</code> implementation properly to support schemas with multiple inheritance.</p> <p>If we parse an archive like this:</p> <pre><code>data:\n  m_def: 'examples.archive.custom_schema.SampleDatabase'\n  samples:\n    - formula: NaCl\n      added_date: '2022-06-18'\n</code></pre> <p>we will get a final normalized archive that contains our data like this:</p> <pre><code>{\n  \"data\": {\n    \"m_def\": \"examples.archive.custom_schema.SampleDatabase\",\n    \"samples\": [\n      {\n        \"added_date\": \"2022-06-18T00:00:00+00:00\",\n        \"formula\": \"NaCl\",\n        \"sample_id\": \"2022-06-18 00:00:00+00:00--NaCl\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"tutorial/develop_plugin.html","title":"Developing a NOMAD Plugin","text":"<p>In this tutorial you will learn how to create and develop a NOMAD plugin. As an example we will create a plugin to log data for a simple sintering process.</p>"},{"location":"tutorial/develop_plugin.html#prerequisites","title":"Prerequisites","text":"<ul> <li>A GitHub account. This can be created for free on github.com.</li> <li>Basic understanding of Python.</li> <li>Basic understanding of NOMAD metainfo, see for example tutorial 8.</li> </ul> <p>Note</p> <p>Several software development concepts are being used during this tutorial. Here is a list with some further information on each of them:</p> <ul> <li>what is Git</li> <li>what is VSCode, i. e., an Integrated Development Environment (IDE)</li> <li>what is Pip</li> <li>what is a Python virtual environment</li> <li>creating a Python package</li> <li>uploading a package to PyPI</li> <li>what is cruft</li> </ul>"},{"location":"tutorial/develop_plugin.html#create-a-github-repository","title":"Create a Git(Hub) repository","text":"<p>Firstly, we recommend to use git to version control your NOMAD plugin. There is a GitHub template repository that can be used for this at github.com/FAIRmat-NFDI/nomad-plugin-template.</p> <p>To use the template you should choose the \"Create an new repository\" option after pressing the green \"Use this template\" button in the upper right corner. Please note that you have to be logged into to GitHub to see this option.</p> <p> </p> <p>Enter a name (I will use \"nomad-sintering\" for mine) for your repository and click \"Create Repository\".</p>"},{"location":"tutorial/develop_plugin.html#generate-the-plugin-structure","title":"Generate the plugin structure","text":"<p>Next, we will use a cookiecutter template to create the basic structure of our NOMAD plugin.</p> <p>There are now two options for how to proceed.</p> <ol> <li>You can use the GitHub codespaces environment to develop your plugin, or</li> <li>If you have access to a Linux computer you can also run the same steps locally.</li> </ol>"},{"location":"tutorial/develop_plugin.html#1-using-github-codespaces","title":"1. Using GitHub codespaces","text":"<p>To use a GitHub codespace for the plugin development you should choose the \"Create codespace on main\" option after pressing the green \"&lt;&gt; Code\" button in the upper right corner.</p> <p> </p>"},{"location":"tutorial/develop_plugin.html#2-developing-locally","title":"2. Developing locally","text":"<p>If you have a Linux machine and prefer to develop locally you should instead click the \"Local\" tab after pressing the green \"&lt;&gt; Code\" button, copy the path, and clone your repository by running:</p> <p><pre><code>git clone PATH/COPIED/FROM/REPOSITORY\n</code></pre> and move inside the top directory <pre><code>cd REPOSITORY_NAME\n</code></pre> You will also need to install cruft, preferably using <code>pipx</code>: <pre><code># pipx is strongly recommended.\npipx install cruft\n\n# If pipx is not an option,\n# you can install cruft in your Python user directory.\npython -m pip install --user cruft\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#run-cruft","title":"Run cruft","text":"<p>The next step is to run cruft to use our cookiecutter template: <pre><code>cruft create https://github.com/FAIRmat-NFDI/cookiecutter-nomad-plugin\n</code></pre> Cookiecutter prompts you for information regarding your plugin and I will enter the following for my example:</p> <pre><code>  [1/12] full_name (John Doe): Hampus N\u00e4sstr\u00f6m\n  [2/12] email (john.doe@physik.hu-berlin.de): hampus.naesstroem@physik.hu-berlin.de\n  [3/12] github_username (foo): hampusnasstrom\n  [4/12] plugin_name (foobar): sintering\n  [5/12] module_name (sintering):\n  [6/12] short_description (Nomad example template): A schema package plugin for sintering.\n  [7/12] version (0.1.0):\n  [8/12] Select license\n    1 - MIT\n    2 - BSD-3\n    3 - GNU GPL v3.0+\n    4 - Apache Software License 2.0\n    Choose from [1/2/3/4] (1):\n  [9/12] include_schema_package [y/n] (y): y\n  [10/12] include_normalizer [y/n] (y): n\n  [11/12] include_parser [y/n] (y): n\n  [12/12] include_app [y/n] (y): n\n</code></pre> <p>There you go - you just created a minimal NOMAD plugin:</p> <p>Note</p> <p>In the above prompt, we pressed <code>y</code> for schema_package, this creates a python package</p> <p>with a plugin entry point for a schema package.</p> <pre><code>nomad-sintering/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 move_template_files.sh\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 nomad_sintering\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 schema_packages\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u2514\u2500\u2500 mypackage.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 data\n    \u2502   \u2514\u2500\u2500 test.archive.yaml\n    \u2514\u2500\u2500 schema_packages\n        \u2514\u2500\u2500 test_schema.py\n</code></pre> <p>Note</p> <p>The project <code>nomad-sintering</code> is created in a new directory, we have included a helper script to move all the files to the parent level of the repository.</p> <pre><code>sh CHANGE_TO_PLUGIN_NAME/move_template_files.sh\n</code></pre> <p>Attention</p> <p>The <code>CHANGE_TO_PLUGIN_NAME</code> should be substituted by the name of the plugin you've created. In the above case it'll be <code>sh nomad-sintering/move_template_files.sh</code>.</p> <p>Finally, we should add the files we created to git and commit the changes we have made: <pre><code>git add -A\ngit commit -m \"Generated plugin from cookiecutter template\"\ngit push\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#enable-cruft-updates","title":"Enable Cruft updates","text":"<p>In order to receive updates from our cookiecutter template we have included a GitHub action that automatically checks for updates once a week (or by triggering it manually). In order for this action to run we need to give the action permission to write and create pull requests. To do this we should go back to the plugin repo and head to the settings tab and navigate to the Actions/General options on the left:</p> <p> </p> <p>At the very bottom of this place you should mark the \"Read and write permissions\" and the \"Allow GitHub Actions to create and approve pull requests\" options and click save.</p> <p> </p>"},{"location":"tutorial/develop_plugin.html#setting-up-the-python-environment","title":"Setting up the python environment","text":""},{"location":"tutorial/develop_plugin.html#creating-a-virtual-environment","title":"Creating a virtual environment","text":"<p>Before we can start developing we recommend to create a virtual environment using Python 3.12</p> <pre><code>python3.12 -m venv .pyenv\nsource .pyenv/bin/activate\n</code></pre>"},{"location":"tutorial/develop_plugin.html#installing-the-plugin","title":"Installing the plugin","text":"<p>Next we should install our plugin package in editable mode and using the nomad package index</p> <pre><code>pip install --upgrade pip\npip install -e '.[dev]' --index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre> <p>Note</p> <p>Until we have an official PyPI NOMAD release with the latest NOMAD version, make sure to include NOMAD's internal package registry (e.g. via --index-url). The latest PyPI package available today is version 1.2.2 and it misses some updates functional to this tutorial. In the future, when a newer release of <code>nomad-lab</code> will be available (    1.2.2) you can omit the <code>--index-url</code>.</p>"},{"location":"tutorial/develop_plugin.html#importing-a-yaml-schema","title":"Importing a yaml schema","text":""},{"location":"tutorial/develop_plugin.html#the-schema","title":"The schema","text":"<p>We will now convert the yaml schema package from part 2 where we described a sintering step:</p> <pre><code>definitions:\n  name: 'Tutorial 13 sintering schema'\n  sections:\n    TemperatureRamp:\n      m_annotations:\n        eln:\n          properties:\n            order:\n              - \"name\"\n              - \"start_time\"\n              - \"initial_temperature\"\n              - \"final_temperature\"\n              - \"duration\"\n              - \"comment\"\n      base_sections:\n        - nomad.datamodel.metainfo.basesections.ProcessStep\n      quantities:\n        initial_temperature:\n          type: np.float64\n          unit: celsius\n          description: \"initial temperature set for ramp\"\n          m_annotations:\n            eln:\n              component: NumberEditQuantity\n              defaultDisplayUnit: celsius\n        final_temperature:\n          type: np.float64\n          unit: celsius\n          description: \"final temperature set for ramp\"\n          m_annotations:\n            eln:\n              component: NumberEditQuantity\n              defaultDisplayUnit: celsius\n    Sintering:\n      base_sections:\n        - nomad.datamodel.metainfo.basesections.Process\n        - nomad.datamodel.data.EntryData\n      sub_sections:\n        steps:\n          repeats: True\n          section: '#/TemperatureRamp'\n</code></pre> <p>We can grab this file from the tutorial repository using curl <pre><code>curl -L -o sintering.archive.yaml \"https://raw.githubusercontent.com/FAIRmat-NFDI/AreaA-Examples/main/tutorial13/part3/files/sintering.archive.yaml\"\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#metainfo-yaml2py","title":"<code>metainfo-yaml2py</code>","text":"<p>We will now use an external package <code>metainfo-yaml2py</code> to convert the yaml schema package into python class definitions. First we install the package with <code>pip</code>: <pre><code>pip install metainfoyaml2py\n</code></pre></p> <p>Then we can run the <code>metainfo-yaml2py</code> command on the <code>sintering.archive.yaml</code> file with the <code>-n</code> flag for adding <code>normalize()</code> functions (will be explained later) and specify the output directory, with the <code>-o</code> flag, to be our <code>schema_packages</code> directory: <pre><code>metainfo-yaml2py sintering.archive.yaml -o src/nomad_sintering/schema_packages -n\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#updating-__init__py-and-pyprojecttoml","title":"Updating <code>__init__.py</code> and <code>pyproject.toml</code>","text":"<p>The metadata of our package is defined in the <code>__init__.py</code> file and here we now need to add the sintering package that we just created. If we take a look in that file we can see an example created by the cookiecutter template. We can go ahead and copy the <code>MySchemaPackageEntryPoint</code> class and the <code>mypackage</code> instance and paste them below. We then need to change: 1. the name of the class, 2. the import in the load function to import our sintering schema package, 3. the name of the instance and the class it uses, 4. ideally we should also update the description and the name.</p> <p>The changes could look something like this:</p> <pre><code>class SinteringEntryPoint(SchemaPackageEntryPoint):\n\n    def load(self):\n        from nomad_sintering.schema_packages.sintering import m_package\n\n        return m_package\n\n\nsintering = SinteringEntryPoint(\n    name='Sintering',\n    description='Schema package for describing a sintering process.',\n)\n</code></pre> <p>Finally, we also need to add our new entry point to the <code>pyproject.toml</code>. At the bottom of the toml you will see how this was done for the example and we just need to replicate that with whatever we called our instance: <pre><code>sintering = \"nomad_sintering.schema_packages:sintering\"\n</code></pre></p> <p>After adding the entry point to <code>pyproject.toml</code> file, re-install the package to make sure the new entry point is available: <pre><code>pip install -e '.[dev]' --index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre></p> <p>Before we continue, we should commit our changes to git: <pre><code>git add -A\ngit commit -m \"Added sintering classes from yaml schema\"\ngit push\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#ruff-autoformatting","title":"Ruff autoformatting","text":"<p>If we check the actions tab of the GitHub repository we might see that the last commit caused an error in the Ruff format checking. We can either disable this workflow (not recommended) or we can check and format our code with Ruff.</p> <p>To check what Ruff thinks about our code we run:</p> <pre><code>ruff check .\n</code></pre> <p>To fix any issues we can run:</p> <pre><code>ruff check . --fix\n</code></pre> <p>And commit the changes: <pre><code>git add -A\ngit commit -m \"Ruff linting\"\ngit push\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#adding-a-normalize-function","title":"Adding a normalize function","text":"<p>Next we will add some functionality to our use case through a so called \"normalize\" function. This allows us to add functionality to our schemas via Python code.</p>"},{"location":"tutorial/develop_plugin.html#the-use-case","title":"The use case","text":"<p>For this tutorial we will assume that we have a recipe file for our hot plate that we will parse: <pre><code>step name,duration [min],initial temperature [C],final temperature [C]\nheating, 30, 25, 300\nhold, 60, 300, 300\ncooling, 30, 300, 25\n</code></pre></p> <p>We can grab this file from the tutorial repository and place it in the tests/data directory using curl <pre><code>curl -L -o tests/data/sintering_example.csv \"https://raw.githubusercontent.com/FAIRmat-NFDI/AreaA-Examples/main/tutorial13/part3/files/sintering_example.csv\"\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#adding-the-code","title":"Adding the code","text":"<p>The first thing we need to add is a new <code>Quantity</code> in our <code>Sintering</code> class to hold the recipe file: <pre><code>data_file = Quantity(\n    type=str,\n    description='The recipe file for the sintering process.',\n    a_eln={\n        \"component\": \"FileEditQuantity\",\n    },\n)\n</code></pre> Here we have used the <code>a_eln</code> component annotation to add a <code>FileEditQuantity</code>. You will see in part 4 how this looks in the GUI.</p> <p>Secondly we need to update the normalize method to read the data file and update the corresponding data.</p> <p>First we will check if the <code>self.data_file</code> is present and, if so, use the <code>archive.m_context.raw_file()</code> method to open the file and read it with the pandas function <code>read_csv()</code>:</p> <pre><code>if self.data_file:\n  with archive.m_context.raw_file(self.data_file) as file:\n    df = pd.read_csv(file)\n</code></pre> <p>We will then create a list to hold the steps, iterate over our data frame, create an instance of a <code>TemperatureRamp</code>, and fill them. <pre><code>    steps = []\n    for i, row in df.iterrows():\n      step = TemperatureRamp()\n      step.name = row['step name']\n      step.duration = ureg.Quantity(float(row['duration [min]']), 'min')\n      step.initial_temperature = ureg.Quantity(row['initial temperature [C]'], 'celsius')\n      step.final_temperature = ureg.Quantity(row['final temperature [C]'], 'celsius')\n      steps.append(step)\n</code></pre> Here we have used the NOMAD unit registry to handle all the units.</p> <p>Finally, we will assign the <code>self.steps</code> with our new list of steps. <pre><code>  self.steps = steps\n</code></pre></p> <p>We also need to add the import of pandas and the NOMAD unit registry to the top of our <code>sintering.py</code> file: <pre><code>from nomad.units import ureg\nimport pandas as pd\n</code></pre></p> <p>Here are all the changes combined: <pre><code>from nomad.units import ureg\nimport pandas as pd\n\n\nclass Sintering(Process, EntryData, ArchiveSection):\n    '''\n    Class autogenerated from yaml schema.\n    '''\n    m_def = Section()\n    steps = SubSection(\n        section_def=TemperatureRamp,\n        repeats=True,\n    )\n    data_file = Quantity(\n        type=str,\n        description='The recipe file for the sintering process.',\n        a_eln={\n            \"component\": \"FileEditQuantity\",\n        },\n    )\n\n    def normalize(self, archive, logger: 'BoundLogger') -&gt; None:\n        '''\n        The normalizer for the `Sintering` class.\n\n        Args:\n            archive (EntryArchive): The archive containing the section that is being\n            normalized.\n            logger (BoundLogger): A structlog logger.\n        '''\n        super().normalize(archive, logger)\n        if self.data_file:\n            with archive.m_context.raw_file(self.data_file) as file:\n                df = pd.read_csv(file)\n            steps = []\n            for i, row in df.iterrows():\n                step = TemperatureRamp()\n                step.name = row['step name']\n                step.duration = ureg.Quantity(float(row['duration [min]']), 'min')\n                step.initial_temperature = ureg.Quantity(row['initial temperature [C]'], 'celsius')\n                step.final_temperature = ureg.Quantity(row['final temperature [C]'], 'celsius')\n                steps.append(step)\n            self.steps = steps\n</code></pre></p>"},{"location":"tutorial/develop_plugin.html#running-the-normalize-function","title":"Running the normalize function","text":"<p>We will now run the NOMAD processing on a test file to see the normalize function in action.</p>"},{"location":"tutorial/develop_plugin.html#create-an-archivejson-file","title":"Create an archive.json file","text":"<p>The first step is to create the test file. We should add a file with the ending <code>.archive.yaml</code> or <code>archive.json</code> and which contains a <code>data</code> section and an <code>m_def</code> key with the value being our sintering section. Finally, we should also add the <code>data_file</code> key with the value being our <code>.csv</code> file from before. <pre><code>data:\n  m_def: nomad_sintering.schema_packages.sintering.Sintering\n  data_file: sintering_example.csv\n</code></pre></p> <p>We can once again grab this file from the tutorial repository and place it in the tests/data directory using curl <pre><code>curl -L -o tests/data/test_sintering.archive.yaml \"https://raw.githubusercontent.com/FAIRmat-NFDI/AreaA-Examples/main/tutorial13/part3/files/test_sintering.archive.yaml\"\n</code></pre></p> <p>Attention</p> <p>You might need to modify the package name for the <code>m_def</code> if you called your python module something other than <code>nomad_sintering</code></p>"},{"location":"tutorial/develop_plugin.html#run-the-nomad-cli","title":"Run the NOMAD CLI","text":"<p>To run the processing we us the NOMAD CLI method <code>parse</code>and save the output in a json file</p> <pre><code>nomad parse tests/data/test_sintering.archive.yaml &gt; normalized.archive.json\n</code></pre> <p>However, when we run this we will get an error from NOMAD!</p> <pre><code>could not normalize section (normalizer=MetainfoNormalizer, section=Sintering, exc_info=Cannot convert from 'milliinch' ([length]) to 'second' ([time]))\n</code></pre> <p>What is happening here is that it has treated our <code>'min'</code> unit for duration as <code>'milliinch'</code> and not the intended minutes. To fix this we can directly edit the normalize function of the <code>Sintering</code> class in the <code>sintering.py</code> file by replacing <code>'min'</code> with <code>'minutes'</code>.</p> <pre><code>def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n    \"\"\"\n    The normalizer for the `Sintering` class.\n\n    Args:\n        archive (EntryArchive): The archive containing the section that is being\n        normalized.\n        logger (BoundLogger): A structlog logger.\n    \"\"\"\n    super().normalize(archive, logger)\n    if self.data_file:\n        with archive.m_context.raw_file(self.data_file) as file:\n            df = pd.read_csv(file)\n        steps = []\n        for i, row in df.iterrows():\n            step = TemperatureRamp()\n            step.name = row['step name']\n            # Changed 'min' to 'minutes' here:\n            step.duration = ureg.Quantity(float(row['duration [min]']), 'minutes')\n            step.initial_temperature = ureg.Quantity(row['initial temperature [C]'], 'celsius')\n            step.final_temperature = ureg.Quantity(row['final temperature [C]'], 'celsius')\n            steps.append(step)\n        self.steps = steps\n</code></pre> <p>Since we installed our package in editable mode the changes will take effect as soon as we save and rerunning the nomad parse command above should now work.</p> <p>To view the output you can open and inspect the <code>normalized.archive.json</code> file. The beginning of that file should look something like:</p> <pre><code>{\n  \"data\": {\n    \"m_def\": \"nomad_sintering.schema_packages.sintering.Sintering\",\n    \"name\": \"test sintering\",\n    \"datetime\": \"2024-06-04T16:52:23.998519+00:00\",\n    \"data_file\": \"sintering_example.csv\",\n    \"steps\": [\n      {\n        \"name\": \"heating\",\n        \"duration\": 1800.0,\n        \"initial_temperature\": 25.0,\n        \"final_temperature\": 300.0\n      },\n      {\n        \"name\": \"hold\",\n        \"duration\": 3600.0,\n        \"initial_temperature\": 300.0,\n        \"final_temperature\": 300.0\n      },\n      {\n        \"name\": \"cooling\",\n        \"duration\": 1800.0,\n        \"initial_temperature\": 300.0,\n        \"final_temperature\": 25.0\n      }\n    ]\n  },\n...\n</code></pre>"},{"location":"tutorial/develop_plugin.html#next-steps","title":"Next steps","text":"<p>The next step is to include your new schema in a custom NOMAD Oasis. For more information on how to configure a NOMAD Oasis you can have a look at How-to guides/NOMAD Oasis/Configuration.</p> <p>Before we move one we should make sure that we have committed our changes to git: <pre><code>git add -A\ngit commit -m \"Added a normalize function to the Sintering schema\"\ngit push\n</code></pre></p>"},{"location":"tutorial/explore.html","title":"Exploring Data in NOMAD","text":"<p>In this tutorial, we will learn how to efficiently search and filter entries in NOMAD to find data that matches your criteria.</p> <p>We will work with the EXPLORE page in the NOMAD GUI, starting with an overview of the search and filter options available. We will then use these tools to refine our results step by step. Finally, we will explore how to create interactive widgets based on selected properties, allowing for a more dynamic and customized filtering experience.</p> <p>By the end of this tutorial, you will be able to navigate NOMAD with confidence, quickly locate relevant data, and tailor your searches using interactive filters and widgets.</p>"},{"location":"tutorial/explore.html#navigate-to-nomads-explore-entries-page","title":"Navigate to NOMAD's Explore Entries Page","text":"<p>The EXPLORE menu allows you to navigate and search through a vast amount of materials-science data. It provides several options, each focuses on a specific application or method. They include different sets of filters and/or search widgets that allow users to efficiently filter and narrow down results, making it easier to find relevant entries in specific domains.</p> <p>In this tutorial, we will focus on searching data in NOMAD using the Entries option within the EXPLORE menu. You will see an overview of SOLAR CELLS search dashboard.</p> <p>To start with exploring data across all domains in NOMAD, go to EXPLORE \u2192 Entries.</p> <p>The Entries page shows all the uploaded data that are published on NOMAD. Published entries are accessible without login, while logging in grants access to your private data and those that are shared with you.</p> Contents of NOMAD's Explore Menu <p>The following explore pages are currently available:</p> <ul> <li>Entries: Search entries across all domains.</li> <li>Theory: Focus on calculations and materials data derived from theoretical models.</li> <li>Experiment: Explore data from experimental sources, such as ELNs (Electronic Lab Notebooks) or characterization techniques e.g., EELS (Electron Energy Loss Spectroscopy).</li> <li>Tools: Explore among several AI toolkit notebooks.</li> <li>Use Cases: Search data tailored to specific use cases, such Metal-Organic Frameworks (MOFs).</li> </ul> <p> </p>"},{"location":"tutorial/explore.html#search-interface-filters","title":"Search Interface &amp; Filters","text":"<p>In the Entries page, you will find a list of possible filters on the left panel. NOMAD supports advanced searches based on:</p> <ul> <li>Material \u2013 Elements, formula, or structure.</li> <li>Method \u2013 Scientific techniques (e.g., DFT).</li> <li>Properties \u2013 Band structure, conductivity, etc.</li> <li>Use Cases \u2013 Application-specific searches (e.g., Solar Cells).</li> <li>Origin \u2013 Filter by uploader, date, dataset, or tags.</li> </ul> <p>Filters refine searches dynamically. You can apply the following example to experience this.</p> <ul> <li>Material Filter: Select B and N to find hexagonal boron nitride.</li> <li>Method Filter: Find BN simulations using VASP.</li> <li>Properties Filter: Search for entries with band structure data.</li> </ul> <p>You can pin frequently used filters using (+) for a customized search interface.</p>"},{"location":"tutorial/explore.html#search-bar-a-quick-way-to-explore-data","title":"Search Bar: A Quick Way to Explore Data","text":"<p>You can use the NOMAD search bar to find indexed quantities. As you begin typing, all available and searchable sets (with their paths in the NOMAD metainfo) appear in the advanced menu below the search bar. Continue typing to refine the results and select the desired set.</p> <p>For the example presented above (searching for Boron Nitride):</p> <ul> <li>Type \"Boron\" and select results.material.material_name = Boron.</li> <li>Type \"Nitrogen\" and select results.material.material_name = Nitrogen.</li> </ul> <p>Alternatively, you can directly enter the element paths in the search field:</p> <ul> <li>results.material.elements = B</li> <li>results.material.elements = N</li> </ul> <p>Does NOMAD have a bandgap filter?</p> <p>Can you find a filter for bandgap? Does it provide the bandgap value or indicate direct/indirect nature?</p> <ul> <li>Try typing variations like \"bandgap\", \"band gap\", or \"band_gap\" into the search bar.</li> <li>Search for \"direct\" or \"indirect\" to explore bandgap characteristics.</li> </ul> <p>In addition, you can also perform range-based searches for values using the search bar. This allows you to find materials with specific properties that fall within a defined numerical range.</p> <p>For example, if you want to find materials with a band gap of 2 eV or larger, enter the following in the search bar:</p> <ul> <li>results.properties.electronic.band_gap.value &gt;= 2 eV</li> </ul> <p>Similarly, you can define a bounded range for the values. For example, to search for materials with a band gap between 2 eV and 4 eV, enter the following line in the search bar:</p> <ul> <li>2 &lt;= results.properties.electronic.band_gap.value &lt;= 4 eV</li> </ul>"},{"location":"tutorial/explore.html#custom-widgets-for-advanced-searches","title":"Custom Widgets for Advanced Searches","text":"<p>NOMAD enables searching entries using rich metadata. Some metadata is extracted automatically, while others are user-provided via schemas. Only metadata stored according to schemas is searchable.</p> <p>NOMAD also offers custom widgets to create advanced dashboards. These widgets are accessible below the search bar on any EXPLORE page.</p> <p>Here are the main four widgets:</p> <ul> <li>TERMS: Visualize and explore categorical data based on user-defined terms and keywords.</li> <li>HISTOGRAM: Display the distribution of a specific numerical quantity within data.</li> <li>SCATTER PLOT: Generate scatter plots to visualize relationships between different quantities.</li> <li>PERIODIC TABLE: Filter data by selecting elements directly from an interactive periodic table.</li> </ul>"},{"location":"tutorial/explore.html#example-1-finding-alternative-etl-materials-for-perovskite-solar-cells","title":"Example 1: Finding Alternative ETL Materials for Perovskite Solar Cells","text":"<p>In the following, we'll walk through an example to help you better understand how to use these widgets. Imagine we are working on solar cell research and have fabricated solar cell devices using the absorber material CsPbBr2I (Cesium Lead Bromine Iodide), a mixed halide perovskite.</p> <p>Device Structure:</p> Component Material Top Contact Au HTL (Hole Transport Layer) Spiro-OMeTAD (C81\u200bH68\u200bN4\u200bO8\u200b) Perovskite Absorber CsPbBr2I ETL (Electron Transport Layer) TiO2-c (compact Titanium Dioxide) Bottom Contact FTO (Fluorine-doped Tin Oxide) Substrate SLG (Soda Lime Glass) <p>Now, let us answer the following question:</p> <p>What ETL materials can replace TiO2-c to improve Voc (open circuit voltage) in perovskite solar cells?</p> <p>To gain insights into this question, we can utilize NOMAD's widgets to explore relevant data:</p> <ol> <li> <p>Start with the Periodic Table:</p> <ul> <li>Click on the PERIODIC TABLE widget button and use the (+) button to pin it to the dashboard.</li> <li>Select the elements of the absorber from the periodic table: Cs, Pb, Br, and I.</li> <li>After selecting these elements, you should see approximately 7,500 entries matching your search filters.</li> </ul> </li> <li> <p>Use the TERMS Widget:</p> <ul> <li>To find out what ETL and HTL materials are used in the available data, click on the TERMS widget button.</li> <li>For the X-axis, type 'electron transport layer'. As you type, suggestions will appear. Choose <code>results.properties.optoelectronic.solar_cell.electron_transport_layer</code>.</li> <li>Set the statistics scaling to linear, give the widget a descriptive title like \"ETL\", and pin it to the dashboard.</li> <li>Repeat the process for the HTL materials.</li> </ul> </li> <li> <p>Create a Scatter Plot:</p> <ul> <li>Click on the SCATTER PLOT widget button to visualize the relationship between open circuit voltage (Voc), short circuit current density (Jsc), and efficiency.</li> <li>Set the X-axis to \"Open Circuit Voltage (Voc)\", the Y-axis to \"Efficiency\", and use the marker color to represent \"Short Circuit Current Density\".</li> <li>The scatter plot will allow you to explore the data interactively.</li> </ul> </li> </ol> <p> </p> <p>4- Interpreting Results</p> <ul> <li>Interactive scatter plots reveal relationships between ETLs, HTLs, and performance.</li> <li>Hover over data points for details.</li> <li>Click entries for full metadata, dataset links, and publication info.</li> </ul> <p>Custom widgets provide a powerful way to explore NOMAD data and answer research questions efficiently.</p>"},{"location":"tutorial/explore.html#example-2-exploring-sn-based-solar-cells","title":"Example 2: Exploring Sn-Based Solar Cells","text":"<p>Let\u2019s explore how hole transport layer (HTL) materials affect efficiency in Sn-based solar cells with C60 as the electron transport layer (ETL).</p> <p>In this example, we will utilize the Solar Cell Explore page, which offers filters and preset widgets that makes it easier to search for solar cell entries.</p> <p>Step 1: Navigate to the Solar Cell Explore Page</p> <ul> <li>Navigate to EXPLORE \u2192 Solar Cells.</li> <li>This dashboard provides predefined filters and plots optimized for solar cell research.</li> </ul> <p>The dashboard includes the following preset widgets:</p> <ul> <li>Periodic Table: Filter materials by elements.</li> <li>Scatter Plots: Explore efficiency vs. Voc, Jsc, and device architecture.</li> <li>Histograms: Analyze bandgap and illumination intensity.</li> <li>TERMS Plots: Categorize fabrication method, device stack, ETL, and HTL materials.</li> </ul> <p>Step 2: Apply Filters</p> <ul> <li>Select Sn in the Periodic Table to filter Sn-based absorbers.</li> <li>Set ETL = C60 in the TERMS plot. (~400 entries remain)</li> <li>Narrow results further using:<ul> <li>Bandgap slider (e.g., &gt;1.3 eV).</li> <li>Device architecture scatter plot (e.g., pin).</li> </ul> </li> </ul> <p>Step 3: Customize Widgets</p> <p>Click the pen icon on any widget to modify its plotted quantities, color mapping, or units. Each widget offers a customizable set of filters or visualizations, depending on the data type.</p> <p>Step 4: Inspect the Results Matching the Criteria</p> <ul> <li>Hover over scatter plots to inspect data points.</li> <li>Click entries for full metadata, dataset links, and further analysis.</li> </ul>"},{"location":"tutorial/nomad_repo.html","title":"Navigating to the NOMAD repository","text":"<p>There are several access points to the NOMAD repository. The general landing page will give you a quick rundown of NOMAD's usage and features, and provides several links to documentation, tutorials, and the history behind the project.</p> <p>From this page, we can navigate to the NOMAD repository, where we can upload, manage, and explore data. There are 2 public versions available:</p> <ol> <li>stable, which is accessed by clicking the \"Open NOMAD\" button at the top of the landing page (highlighted orange in images below).</li> <li>beta /staging, which has the latest release and updates much more frequently, but may also harbor unstable or untested features. You can navigate to this version via two distinct links: 1. at the bottom-right corner of the landing page and 2. under \"SOLUTIONS\" &gt; \"NOMAD\" &gt; \"Try and Test\" in the top navigation menu (highlighted red in images below).</li> </ol> <p> </p>"},{"location":"tutorial/overview.html","title":"NOMAD Tutorials","text":"<p>This section of the documentation provides practical, hands-on instructions for working with NOMAD. Each tutorial is designed to guide you through specific tasks while focusing on key learning outcomes.</p> <p>By following these tutorials, you will be able to:</p> <ul> <li>Interact directly with NOMAD to perform essential tasks.</li> <li>Develop a deeper understanding of its features and capabilities.</li> <li>Gain confidence in using NOMAD efficiently for your research.</li> </ul> <p>These tutorials provide a structured learning experience that helps you apply concepts to real-world scenarios while reinforcing your knowledge.</p> <p>As you go through the tutorial pages, you will encounter embedded boxes with different colors and icons, each providing distinct types of information or instructions. Below is a list of the various box types and guidance on how to use them:</p> <p>Offers additional information to enhance your understanding of NOMAD.</p> <p>Presents hands-on tasks or questions with instructions to complete them.</p> <p>Shares example files for you to try out, along with explanations of their contents and scientific use cases.</p> <p>Displays important warnings to consider before proceeding further in the tutorial.</p>"},{"location":"tutorial/overview.html#scope-of-the-tutorials","title":"Scope of the Tutorials","text":"<p>NOMAD is available in two flavours to address the diverse needs of scientists: Central NOMAD and NOMAD Oasis. To learn more about these solutions, refer to our web page.</p> <p>Our tutorial documentation covers basic usage that can be performed in both Central NOMAD and NOMAD Oasis. These tasks range from basic operations using the graphical user interface (GUI) to a certain level of customization (level 1) using YAML files, which do not require local installations.</p> <p>Additionally, the tutorials include in-depth customization (level 2) of NOMAD Oasis using NOMAD Plugins. However, more advanced topics such as installing an Oasis, setting up an internal Keycloak instance, managing a local file system, or adding tools to NORTH are not currently covered in the tutorials. Users interested in these topics should refer to other sections of the documentation, such as:</p> <ul> <li>How-to guides for step-by-step instructions,</li> <li>Explanations for deeper conceptual understanding, and</li> <li>Reference documentation for detailed technical information.</li> </ul> <p>An overview of the skills required to use NOMAD and NOMAD Oasis, categorized into basic use, customization, and configuration, is summarized in the figure below.</p> <p></p>"},{"location":"tutorial/overview.html#ways-to-access-nomad","title":"Ways to Access NOMAD","text":"<p>There are multiple access points to NOMAD. The general landing page provides an overview of NOMAD\u2019s features, along with links to documentation, tutorials, and project history.</p> <p>From this page, we can navigate to the NOMAD to upload, manage, and explore data.</p> <p>Three public versions are available:</p> <ol> <li> <p>Official \u2013 The latest stable version of NOMAD.</p> <p>Access it by clicking the \"Open NOMAD\" button at the top of the landing page.</p> </li> <li> <p>Beta/staging \u2013 Features the most recent updates but may include untested or unstable features. It uses the official NOMAD data.</p> <p>Access it from the Installations menu at the bottom of the landing page by selecting \"Beta/staging.\"</p> </li> <li> <p>Test \u2013 Runs the latest released version of NOMAD but on temporary test data that is routinely wiped.</p> <p>Access it from the Installations menu at the bottom of the landing page by selecting \"Test.\"</p> </li> </ol> <p>These tutorials are based on the official version of NOMAD. All instructions are based on it unless stated otherwise.</p>"},{"location":"tutorial/overview.html#create-a-nomad-user-account","title":"Create a NOMAD User Account","text":"<p>A NOMAD user account is required if you want to upload, share, publish, or analyze your data. However, exploring data in NOMAD does not require an account.</p> <p>Creating a NOMAD user account is quick and free. Follow these steps to set up your account (click on the arrows to navigate between steps):</p> \u2190 \u2192"},{"location":"tutorial/upload_publish.html","title":"Uploading and Publishing Data in NOMAD","text":"<p>In this tutorial, we will explore how to upload, share, and research data in NOMAD, using examples from both computational and experimental research. We will start by introducing the key components of NOMAD that facilitate the process, from uploading raw data files to publishing datasets with a Digital Object Identifier (DOI). Step by step, we will guide you through creating and managing uploads, adding files to generate entries that NOMAD processes, and organizing these entries into datasets for publication. By the end of this tutorial, you will have a clear understanding of how to handle different types of research data efficiently within NOMAD.</p> <p></p>"},{"location":"tutorial/upload_publish.html#the-key-elements-in-nomad","title":"The Key Elements in NOMAD","text":"<p>To begin the journey from uploading raw files to publishing datasets with DOIs in NOMAD, you need to understand the key elements of this process in NOMAD, which are illustrated in the figure below.</p> <p></p> <ol> <li> <p>In order to upload data into NOMAD, a user account is required. You can create a user account by following these steps on the overview page.</p> </li> <li> <p>All files and data created by users are organized in NOMAD as Uploads, which function as directories within the user\u2019s account. You can create an upload for each project and structure it into nested folder directories for better organization.</p> Features of NOMAD Uploads <ul> <li>Creation: Uploads are created and managed by users.</li> <li>Structure: Uploads are associated to a user account and are listed in the my uploads page in the GUI. They serve as containers for multiple files and entries, preserving their directory structure and metadata.</li> <li>Organization: Each upload can contain multiple files organized into directories.</li> <li>Sharing &amp; Publishing: Entries and files can only be shared, transferred, or published as complete uploads.</li> </ul> </li> <li> <p>Research data in the form of raw files are added to uploads. If NOMAD has a built-in parser for the file formats, it automatically processes the files and creates Entries. These entries represent structured data extracted from the raw files.</p> Features of NOMAD Entries <ul> <li>Creation: Entries are automatically generated by NOMAD from uploaded raw files or instantiated schemas.</li> <li>Structure: Entries are distinct data entities that can be searched and viewed through dedicated pages in the NOMAD GUI.</li> <li>Organization: Entries belong to specific uploads and are linked to a raw file that NOMAD has recognized and parsed.</li> <li>Sharing &amp; Publishing: Entries can be shared as part of an upload, and individual entries can be added to datasets, which can be published with a DOI.</li> </ul> </li> <li> <p>Entries can be grouped into Datasets, which allow for better organization of related data. Once the dataset is complete, it can be published with a DOI, making the research data findable, accessible, interoperable, and reusable (FAIR).</p> Features of NOMAD Datasets <ul> <li>Creation: Datasets are created and managed by users.</li> <li>Structure: Datasets are associated to a user account and are listed in the Your datasets page in the GUI. They serve as containers for curated and related data entires.</li> <li>Organization: Datasets contains several entries from various uploads. One entry can be contained in many dataset.</li> <li>Sharing &amp; Publishing: Datasets are published independent of uploads and you can get a DOI for your datasets.</li> </ul> </li> </ol> <p>For more detailed explanation, you can refer to this page.</p>"},{"location":"tutorial/upload_publish.html#create-new-upload","title":"Create New Upload","text":"<p>The uploads exist in the Your uploads page. Here you can view a list of all your uploads with their relevant information. You can also create new uploads or add an example upload prepared by others.</p> <p>Follow these steps to create your first upload (click on the arrows to navigate between steps):</p> \u2190 \u2192 Two different views of the upload page <p>At the very top of the upload page, you can toggle between two different views for this page:</p> <ul> <li>Overview: This view includes all the tools needed to manage your upload, along with guided steps to start uploading your files to NOMAD. It will also show all the processed files (entries) that you will upload. </li> <li>Files: This view shows all the files included in upload, whether they are raw files or processed files. You can also organize these files into folders as needed. </li> </ul> Icons on the upload overview <p>At the top of the <code>OVERVIEW</code> tab, you will find several icons that help you to manage your upload:</p> <p></p> <p>The name of the upload can be modified by clicking on the pen icon . The other icons are used as follows:</p> <p> Manage members: Allows users to invite collaborators by assigning co-authors and reviewers roles.</p> <p> Download files: Downloads all files present in the upload.</p> <p> Reload: Reloads the uploads page.</p> <p> Reprocess: Triggers the uploaded data to be processed again.</p> <p> API: Displays a GET request url and corresponding JSON response demonstrating how to access the entries of the upload via the NOMAD API and the expected result, respectively.  </p> <p> Delete the upload: Deletes the upload permanently.</p> Components of the upload overview <p>The remainder of the uploads page is divided into five segments, each presenting a step in the uploading and publishing process:</p> <p><code>1. Prepare and upload your files:</code> displays the files and folder structure of the upload. You can add a <code>README.md</code> file to the root directory and its contents will be shown above this section</p> <p><code>2. Process data:</code> shows the processed data and the generated entries in NOMAD.</p> <p><code>3. Edit visibility and access:</code> allows users to make the upload public or share it with specific users before publishing.</p> <p><code>4. Edit author metadata:</code> allows users to edit certain metadata fields from all entries recognized in the upload. This includes comments, where you can add as much extra information as you want, references, where you can add a URL to your upload (e.g., an article DOI), and datasets, where you can link the uploaded data to other uploads to define a larger-scale organizational structure (see Group entries into a dataset below.)</p> <p><code>5. Publish:</code> lets users publish data with or without an embargo (i.e., a waiting period before the data are publicly visible).</p>"},{"location":"tutorial/upload_publish.html#sharing-and-publishing-uploads","title":"Sharing and Publishing Uploads","text":"<p>Uploads in NOMAD can be shared or published. When an upload is shared or published, all entries and files contained within it are also shared or published.</p> What is the difference between sharing and publishing an Upload? <p>Sharing an upload allows you to grant access to colleagues or collaborators while working on it.</p> <ul> <li>This facilitates collaboration on projects or enables progress reviews during the project.</li> <li>You can invite individuals with either read/write access (coauthors) or read-only access (reviewers).</li> </ul> <p>Publishing an upload makes it searchable, findable, and accessible on NOMAD for everyone.</p> <ul> <li>Once published, the upload becomes immutable, meaning its contents can no longer be modified.</li> <li>You also have the option to publish with an embargo period, temporarily restricting public access until the embargo expires.</li> </ul> <p>A NOMAD upload can have four states based on sharing and publishing:</p> Status Icon Description Private The upload is private and is visible to the uploader only. Shared The upload is accessible to selected users but not publicly available. Published The upload is publicly available to everyone. Visible The upload is unpublished but accessible to everyone. <p>You can manage upload sharing in the Edit upload members menu. To access it, click on  available on the top of the upload page. at the top of the upload page.</p> <p>Alternatively, you can click the <code>EDIT UPLOAD MEMBERS</code> button below the list of entries on your upload page.</p> Share your upload <p></p> <p>Step 1: Open the Edit upload members window, but clicking on the EDIT UPLOAD MEMBERS button.</p> <p>Step 2: Start typing the name of the NOMAD user you want to share the upload with. A list of matching users will appear\u2014select the correct name from the list.</p> <p>Step 3: Assign a role to the user by selecting either Coauthor or Reviewer from the dropdown menu.     - Coauthors can edit and add files to the upload.     - Reviewers can only view the contents of the upload (no editing rights).</p> <p>Step 4: Click on submit.</p> <p></p> Make your upload visible to everyone <p></p> <p>To make your upload visible to everyone on NOMAD, simply check the box in the Edit visibility and access section, located under the list of your entries.</p> <p>This setting allows all users, including guests without an account, to view the upload even before it is published.</p> <p>You can still modify the upload\u2019s access settings and edit its contents while it remains visible.</p> <p></p> Publish your upload <p></p> <p>Once an upload is published, it cannot be deleted, and the files and entries cannot be changed</p> <p>Step 1: Select an embargo period (if needed) from the dropdown menu, located in the publish section located at the bottom of the upload page.</p> <p>If you would like to publish immediately, select No embargo.</p> <p>Step 2: Click on the PUBLISH button.</p> <p>Step 3: A prompt for confirmation appears on the screen. Click on PUBLISH.</p> <p></p> <p>Having an embargo on your data means:</p> <ul> <li>Raw files and entries remain hidden, except to you and users you share the data with.</li> <li>Some metadata (e.g., chemical formula, system type, spacegroup) will be public.</li> <li>Embargoed entries can be added to datasets and be assigned DOIs.</li> <li>The embargo can be lifted earlier than the assigned duration by the user.</li> </ul> <p>The following image shows an example of an embargoed upload and the option to lift the embargo by clicking the LIFT EMBARGO button.</p> <p></p>"},{"location":"tutorial/upload_publish.html#add-files-to-an-upload","title":"Add Files to an Upload","text":"<p>Let's start adding files to your NOMAD upload. We will explore three different examples:</p> <ol> <li>Miscellaneous Files.</li> <li>Files of DFT calculation on Iron(III) Oxide.</li> <li>Files of x-ray photoelectron spectroscopy (XPS) measurement on polymers.</li> </ol> <p>Files can be added to an upload individually, or you can group them into a compressed file in <code>.zip</code> or <code>.tar</code> formats.</p> Different ways and scenarios to upload files to NOMAD <p>In NOMAD, files can be uploaded in different ways, but whether they are processed into Entries or not depends on their format and NOMAD\u2019s built-in parsers. Here\u2019s how it works:</p> <p>Files that are processed by NOMAD</p> <ul> <li> <p>These are files that are recognized by NOMAD, meaning that a built-in parser exists for the file you uploaded, the file will be processed according to a data schemas.</p> </li> <li> <p>It means that NOMAD reads the file, extracts data and organizes them based on the data schema which allows for generating visualizations, and analysis automations.</p> </li> <li> <p>Raw files that are processed in NOMAD become Entries</p> </li> <li> <p>This is common for simulation files since they follow standardized structures. Also a variety of experimental data organized in NeXus format <code>.nxs</code> are recognized by NOMAD.</p> </li> </ul> <p>Files that are not processed by NOMAD</p> <ul> <li> <p>These are files that do not have a built-in parser, meaning NOMAD cannot automatically extract and structure their data.</p> </li> <li> <p>They are still stored in NOMAD as part of your upload, and can be downloaded or shared, but they do not become entries in the database.</p> </li> <li> <p>NOMAD allows you to preview common file formats that are not automatically parsed. This means you can view the contents of the file directly within the platform without having to download it. Examples include: <code>.txt</code>, <code>.csv</code>, <code>.pdf</code>, <code>.png</code>, <code>.jpg</code>.</p> </li> </ul>"},{"location":"tutorial/upload_publish.html#uploading-miscellaneous-files","title":"Uploading Miscellaneous Files","text":"Download the example files for this exercise <p>We have prepared a set of files for this task that can be downloaded from this link.</p> <p>Download the folder, then uncompress it on your local machine at you preferred directory.</p> <p>The folder contains files of the following formats: <code>.zip</code>, <code>.pdf</code>, <code>.jpg</code>, <code>.txt</code>, and <code>.csv</code>.</p> file name format description FAIRmat_graphics .zip Compressed file that contains several FAIRmat logos in <code>.png</code> format JOSS_2023 .pdf A publication of NOMAD in the Journal of Open Source Software Nature_2022 .pdf A publication of FAIRmat in Nature journal P3HT_optical_absorption .csv Results of absorption measurement on P3HT from PerkinElmer spectrometer note_properties_of_good_dopants .txt Notes recorded during a conference talk experiment_polymer_doping .jpg A photograph of an experiment of preparing doped polymer solutions <p>Note that these files will not create entries in NOMAD, because a built-in parser for them doesn't exist.</p> <p>They will be stored in your upload and can be accessed and shared with your colleagues, however, they will not be searchable within NOMAD. In this case, NOMAD functions as a storage system, similar to a cloud drive.</p> <p>You can add these files to your NOMAD upload. Do so by simply drag and drop the file or by opening the dialog to browse the files in your device.</p> Uploading images, pdf files, text files, and tabular data <p>Drag and drop</p> <p>Start with uploading the file <code>FAIRmat_graphics.zip</code>. Let's use the drag and drop method as shown in the animation below.</p> <p></p> <p>When a compressed file is uploaded to NOMAD, it will be extracted automatically and the included files will be added to your upload.</p> <p>Open the file browser dialog</p> <p>Upload the remaining files using the file browser dialog as shown in the animation below.</p> <p></p>"},{"location":"tutorial/upload_publish.html#uploading-computations-data","title":"Uploading Computations Data","text":"Download the example files for this exercise <p>We have prepared a set of files for this task that can be downloaded from this link.</p> <p>Download the folder to your local machine at you preferred directory.</p> <p>This folder contains the input and output files of a DFT calculation for Iron(III) Oxide using the FHI-aims code.</p> <p>FHI-aims* an all-electron density-functional-theory package that employs numeric atom-centered basis functions. It is designed for accurate and efficient simulations of molecules, clusters, surfaces, and bulk materials across the periodic table. Its advanced treatment of electronic structure allows for precise calculations of material properties, including band structures, total energies, and magnetic properties. More information in this link</p> <p>The calculations in this example were preformed using a code that is supported by NOMAD, i.e. the FHI-aims code.</p> <p>NOMAD has a parser for the FHI-aims code. This means it will create an entry for these data.</p> <p>In other words, NOMAD will read the input and output files and provide all information in NOMAD's unified metainfo data schema.</p> Uploading input and output files <code>.zip</code> of a DFT calculation <p>Uploading the files</p> <p>Start with uploading the file <code>FHI-aims.zip</code>. Let's use the drag and drop method as shown in the animation below.</p> <p></p> <p>After uploading files, processing is automatically triggered. This involves identifying supported file formats in NOMAD and extracting relevant (meta)data. The exact processing steps depend on the use case. For example, you can learn more about the processing of computational data on this link.</p> <p>Once processing is complete, NOMAD generates an entry page that presents the data in a structured, hierarchical format based on the NOMAD metainfo schema.</p> <p>Opening and exploring your entry</p> <p>To go to the entry page, click on the  icon next to the entry. Here you can view the metadata and useful visualization of your data, check the uploaded files, and explore your entry in details.</p> <ul> <li> <p>OVERVIEW tab:</p> <p>1- On the left, core metadata are displayed.</p> <p>2- On the right, various cards present the available information.</p> <p></p> <p>The cards you see depend on the properties or data available for that entry.</p> <p>For FHI-aims files, three main cards appear in the overview page:</p> <ul> <li> <p>Materials card: Shows key information and visulization of the composition and conventional cell. </p> </li> <li> <p>Electronic properties card: Shows the calculated band structure, density of states, and Brillouin zone. </p> </li> <li> <p>Workflow Graph card: Illustrates the various steps in the calculation, including their inputs and outputs. </p> </li> </ul> </li> <li> <p>FILES tab:</p> <p>This tab shows the uploaded files. NOMAD lists all files in the same directory, as they usually belong together.</p> <p></p> </li> <li> <p>DATA tab:</p> <p>Also known as the \"processed data\" tab, this shows the results of the parsing and normalization process done by NOMAD.</p> <p>NOMAD puts all the data in a unified, hierarchical, and machine-processable format, following the NOMAD metainfo.</p> <p></p> </li> </ul>"},{"location":"tutorial/upload_publish.html#uploading-experimental-data","title":"Uploading Experimental Data","text":"Download the example files for this exercise <p>We have prepared a set of files for this task that can be downloaded from this link.</p> <p>Download the folder, then uncompress it to your local machine at you preferred directory.</p> <p>This folder contains files related to an x-ray photoelectron spectroscopy (XPS) on the polymer PBTTT using SPECS spectrometer.</p> <p>It includes the data in two formats (<code>.nxs</code> and <code>.xml</code>), in addtion to an electronic lab notebook (ELN) file (<code>.yaml</code>) documenting additional details of the experiment.</p> file name format description PBTTT_XPS_SPECS .nxs XPS data file in the standard NeXuS file format PBTTT_XPS_SPECS_raw .xml XPS data in a raw file format as produced by the spectrometer ELN_data_xml .yaml An ELN file used to record additional metadata of the experiment <p>NOMAD supports experimental data files in the <code>.nxs</code> format.</p> <p>These files can be uploaded directly to NOMAD, where they are processed, and structured Entries are created.</p> Uploading experimental data in the <code>.nxs</code> format <p>Uploading the file</p> <p>Start with uploading the file <code>PBTTT_XPS_SPECS.nxs</code>. Let's use the drag and drop method as shown in the animation below.</p> <p></p> <p>After uploading files, processing is automatically triggered. This involves identifying supported file formats in NOMAD and extracting relevant (meta)data. The exact processing steps depend on the use case.</p> <p>Once processing is complete, NOMAD generates an entry page that presents the data in a structured, hierarchical format based on the NOMAD metainfo schema.</p> <p>Opening and exploring the entry</p> <p>Click on the  icon next to an entry navigates you to the respective entry page.</p> <p>Here you can view the metadata and useful visualization of your data, check the uploaded files, and explore your entry in details.</p> <ul> <li> <p>OVERVIEW Tab:</p> <p>1- On the left, core metadata are displayed.</p> <p>2- On the right, various cards present the available information.</p> <p>The cards you see depend on the properties or data available for that entry.</p> <p>For <code>.nxs</code> files, two main cards appear in the overview page: the data viewer and the materials card.</p> <p></p> </li> <li> <p>FILES Tab:</p> <p>This tab shows the uploaded files. NOMAD lists all files in the same directory, as they usually belong together.</p> </li> <li> <p>DATA Tab:</p> <p>Also known as the \"processed data\" tab, this shows the results of the parsing and normalization process done by NOMAD.</p> <p>NOMAD puts all the data in a unified, hierarchical, and machine-processable format, following the NOMAD metainfo.</p> <p></p> </li> </ul> <p>Most scientific instruments generate experimental results in formats other than <code>.nxs</code>. NOMAD still supports these files by providing a conversion process using the NexusDataConverter built-in schema, which transforms raw data into the <code>.nxs</code> format.</p> NexusDataConverter readers and the NeXuS application definitions <p>A Reader is a program designed to interpret and extract data from a specific experimental technique or file format.</p> <p>The reader understands the structure and encoding of the particular data format and provides methods for accessing its contents in a programmatically friendly way. It acts as a bridge between raw experimental data and NOMAD by converting the data into the structured file format according to domain-specific application definitions.</p> <p>A list of available readers can be found here</p> <p>A NeXus application definition provides a structured specification of the terms and metadata required in an <code>.nxs</code> data file for a particular scientific application. These definitions outline the minimum set of terms that must be included in the data file for it to be considered valid according to the NeXus format.</p> <p>A list of NeXuS application definitions developed by FAIRmat can be found here</p> <p>NexusDataConverter uses readers to interpret the raw data files, and then structures them according to the outlines of the application definitions</p> <p>In the following examples, you will learn how to upload a raw file from a SPECS instrument in <code>.xml</code> format by using the NexusDataConverter. You will do this in two ways:</p> <ol> <li>Uploading only the raw file.</li> <li>Uploading both the raw file and an ELN file, enriching your data with metadata and ensuring compliance with community standards.</li> </ol> Uploading experimental data in the <code>.xml</code> format <p>Step 1: Click on the CREATE FROM SCHEMA button in your upload page. </p> <p>Step 2: In the create new entry from schema window, click on the drop-down menu of the built-in schema, and select NexusDataConverter</p> <p>Step 3: Give a descriptive name for the entry.</p> <p>Step 4: Click on CREATE. This will take you the NexusDataConverter entry page.</p> <p></p> <p>Step 5: From the reader drop-down menu, choose the appropriate reader for your files. For this exercise select xps.</p> <p>Step 6: From the nxdl drop-down menu, choose the appropriate application definition for your experiment. For this exercise select NXxps</p> <p>Step 7: Upload the raw data file <code>PBTTT_XPS_SPECS_raw.xml</code>.</p> <p>Step 8: Give a descriptive name for the generated <code>.nxs</code> file.</p> <p>Step 9: Click on the save icon to start the conversion process.</p> <p></p> <p>Check the overview page of your upload. There you will find two newly created entries; one for the NexusDataConverter and one for the generated <code>.nxs</code> file from your raw file.</p> <p>NOMAD still stores your <code>.xml</code> raw file in the upload directory.</p> <p></p> Uploading experimental data in the <code>.xml</code> format with additional ELN data <p>Often, raw files generated by instruments from various vendors lack complete metadata and essential details about the experiment.</p> <p>To address this, scientists document all experimental details in an Electronic Lab Notebook (ELN). Combining data from raw files with information from ELNs ensures that experiments are comprehensively described with rich metadata and conform to community standards.</p> <p>NOMAD\u2019s NexusDataConverter allows you to merge experimental raw data files with ELN entries in <code>.yaml</code> format, producing <code>.nxs</code> files that meet community standards and ensure your data are richly-described.</p> <p>The ELN <code>.yaml</code> file can be generated by various ELN software tools or created manually using a text editor.</p> <p>For each supported experimental raw file format, FAIRmat provides a template ELN <code>.yaml</code> file containing all necessary attributes and parameters to complement the raw file\u2019s data. These templates can be found here</p> <p>While these files can be edited with any text editor, we recommend using VS Code for an optimized editing experience.</p> <p>Open the <code>eln_data_xml.yaml</code> file and edit its contents</p> <ul> <li>Modify the start_time and end_time of your experiment.</li> <li>Write your information in the <code>users</code> section.</li> <li>Explore the other fields available in the ELN file.</li> <li>Save the file.</li> </ul> <p></p> <p>Upload your data file <code>.xml</code> and your ELN data <code>.yaml</code> using NexusDataConverter</p> <ul> <li>Step 1: Click on the CREATE FROM SCHEMA button in your upload page.</li> </ul> <p></p> <ul> <li> <p>Step 2: In the create new entry from schema window, click on the drop-down menu of the built-in schema, and select NexusDataConverter</p> </li> <li> <p>Step 3: Give a descriptive name for the entry.</p> </li> <li> <p>Step 4: Click on CREATE. This will take you the NexusDataConverter entry page.</p> </li> </ul> <p></p> <ul> <li> <p>Step 5: From the reader drop-down menu, choose the approperiate reader for your files. For this exercise select xps.</p> </li> <li> <p>Step 6: From the nxdl drop-down menu, choose the approperiate application definition for your experiment. For this exercise select NXxps</p> </li> <li> <p>Step 7: Upload the raw data file <code>PBTTT_XPS_SPECS_raw.xml</code> as well as the ELN data file <code>eln_data_xml.yaml</code>.</p> </li> <li> <p>Step 8: Give a descriptive name for the generated <code>.nxs</code> file.</p> </li> <li> <p>Step 9: Click on the save icon to start the conversion process.</p> </li> </ul> <p></p>"},{"location":"tutorial/upload_publish.html#create-datasets-and-get-a-doi","title":"Create Datasets and Get a DOI","text":"<p>You can organize several entries  by grouping them into common datasets, making it easier to manage related data. Datasets are for organizing and referencing curated data. They do not affect how data is processed. Users can get a DOI for their datasets.</p>"},{"location":"tutorial/upload_publish.html#group-entries-into-a-dataset","title":"Group Entries into a Dataset","text":"<p>Step 1: In the uploads page, click on EDIT AUTHOR METADATA OF ALL ENTRIES button. This will add all the Entries in the upload to the dataset.</p> <p></p> <p>Alternatively, if you would like to add selected entries from your Upload, click on the check box next to the entries you would like to add, then click on  to open the EDIT AUTHOR METADATA window for the selected entries only.</p> <p></p> <p>Step 2: In the Datasets section, you have two options:</p> <ul> <li>Create a new dataset: If the dataset you want to group your entries under doesn't exist yet, you can create one here.</li> <li>Search for an existing dataset: If the dataset already exists, you can search for it by name or other attributes.</li> </ul> <p>Step 3: Once you've selected or created the appropriate dataset, click ADD ENTRY TO NEW DATASET</p> <p>Step 4: Click on SUBMIT to to group your entries under that dataset.</p> <p></p>"},{"location":"tutorial/upload_publish.html#manage-a-dataset-and-assign-it-a-doi","title":"Manage a Dataset and Assign it a DOI","text":"<p>User created datasets exist in the \u201cYour datasets\u201d page page. You can reach there by clicking on Datasets in the PUBLISH menu. Here you can view a list of all your created datasets with their relevant information and assign them a DOI.</p> <p>Assigning a DOI makes the dataset permanent\u2014it cannot be modified or deleted.</p> <p>If you want to test the process without actually publishing the data, you can do so in the NOMAD test installation.</p> <p>Follow these steps to explore and manage your datasets (click on the arrows to navigate between steps):</p> \u2190 \u2192 <p>To publish your datasets and assign them a DOI, follow these steps (click on the arrows to navigate between steps):</p> \u2190 \u2192"},{"location":"tutorial/workflows_projects.html","title":"Managing workflows and projects","text":"<p>NOMAD Tutorial Workflow provides a stand-alone tutorial on workflow and project management with NOMAD. It utilizes the <code>nomad-utility-workflows</code> Python module to lower the entry barrier for advanced NOMAD usage.</p>"},{"location":"tutorial/workflows_projects.html#what-you-will-learn","title":"\ud83e\udded What You Will Learn","text":"<ul> <li>Organize and manage complex research workflows using NOMAD</li> <li>Integrate diverse data sources into a single, reproducible project</li> <li>Track data provenance and metadata</li> <li>Interface with the NOMAD repository programmatically for automation and high-throughput use</li> </ul>"},{"location":"tutorial/workflows_projects.html#recommended-tools-and-background-for-efficiency-and-automation","title":"Recommended tools and background (for efficiency and automation)","text":"<ul> <li> <p>\ud83d\udcbb Terminal environment:   Install the workflow utility module via Bash (Linux/macOS) or PowerShell (Windows)</p> </li> <li> <p>\ud83d\udc0d Basic Python knowledge:   Utilize workflow utility tools using provided Jupyter notebooks</p> </li> </ul>"}]}